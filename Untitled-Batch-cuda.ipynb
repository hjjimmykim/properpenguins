{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "All the logarithms used are base 2. <br>\n",
    "Assumes 2 self-interested agents alternating turns. <br>\n",
    "Baseline (1 for each agent) gets updated after each episode ends (see corpses). <br>\n",
    "Rewards only possible at the end of each game. <br>\n",
    "Uses same (numerical) encoder for both item context and proposal. Reference code uses 3 distinct ones. It also has max_utility = num_types instead of 10 for us.<br>\n",
    "Check how message policy works again; paper seemed to imply that each output of the lstm is a letter. (we take the hidden output and make a probability over letters out of it).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Network\n",
    "import torch\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "# cuda\n",
    "use_cuda = True\n",
    "\n",
    "# Utility functions\n",
    "from utility import truncated_poisson_sampling, create_item_pool, create_agent_utility, rewards_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Game setup\n",
    "num_agents = 2         # Number of agents playing the game\n",
    "num_types = 3          # Number of item types\n",
    "max_item = 5           # Maximum number of each item in a pool\n",
    "max_utility = 10       # Maximum utility value for agents\n",
    "\n",
    "# Turn sampling\n",
    "lam = 7                # Poisson parameter\n",
    "max_N = 10             # Maximum number of turns\n",
    "min_N = 4              # Minimum number of turns\n",
    "\n",
    "# Linguistic channel\n",
    "num_vocab = 10         # Symbol vocabulary size for linguistic channel\n",
    "len_message = 6        # Linguistic message length\n",
    "\n",
    "# Training\n",
    "alpha = 0.001          # learning rate\n",
    "N_ep = 200              # Number of episodes\n",
    "num_games = 128        # Number of games per episode (batch size)\n",
    "\n",
    "# Appendix\n",
    "lambda1 = 0.05         # Entropy regularizer for pi_term, pi_prop\n",
    "lambda2 = 0.001        # Entropy regularizer for pi_utt\n",
    "smoothing_const = 0.7  # Smoothing constant for the exponential moving average baseline\n",
    "\n",
    "# Miscellaneous\n",
    "ep_time = 10         # Print time every ep_time episodes\n",
    "ep_record = 10        # Record training curve every ep_record episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class combined_policy(nn.Module):\n",
    "    def __init__(self, embedding_dim = 100, batch_size = 128, num_layers = 1, bias = True, batch_first = False, dropout = 0, bidirectional = False):\n",
    "        super(combined_policy, self).__init__()\n",
    "        # Save variables\n",
    "        self.embedding_dim = embedding_dim # Hidden layer dimensions\n",
    "        self.batch_size = batch_size       # Batch size (updated every forward pass)\n",
    "        self.log_p = torch.zeros([batch_size,1])                     # Store policy log likelihood for REINFORCE\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            self.log_p = self.log_p.cuda()\n",
    "        \n",
    "        # Encoding -------------------------------------------------------------\n",
    "        \n",
    "        # Numerical encoder\n",
    "        self.encoder1 = nn.Embedding(max_utility+1, embedding_dim)\n",
    "        # Linguistic encoder\n",
    "        self.encoder2 = nn.Embedding(num_vocab+1, embedding_dim)\n",
    "        \n",
    "        # Item context LSTM\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        # Linguistic LSTM\n",
    "        self.lstm2 = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        # Proposal LSTM\n",
    "        self.lstm3 = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        \n",
    "        # Outputs of the 3 LSTMS get concatenated together\n",
    "        \n",
    "        # Feed-forward\n",
    "        self.ff = nn.Linear(3*embedding_dim, embedding_dim)\n",
    "        \n",
    "        # Output of feed-forward is the input for the policy networks\n",
    "        \n",
    "        # Policy ---------------------------------------------------------------\n",
    "        \n",
    "        # Termination policy\n",
    "        self.policy_term = nn.Linear(embedding_dim, 1)\n",
    "        # Linguistic policy\n",
    "        self.policy_ling = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        self.ff_ling = nn.Linear(embedding_dim, num_vocab)\n",
    "        # Proposal policies\n",
    "        self.policy_prop = nn.ModuleList([nn.Linear(embedding_dim, max_item+1) for i in range(num_types)])\n",
    "        \n",
    "    def forward(self, x, test, batch_size=128):\n",
    "        # Inputs --------------------------------------------------------------------\n",
    "        # x = list of three elements consisting of:\n",
    "        #   1. item context (longtensor of shape batch_size x (2*num_types))\n",
    "        #   2. previous linguistic message (longtensor of shape batch_size x len_message)\n",
    "        #   3. previous proposal (longtensor of shape batch_size x num_types)\n",
    "        # test = whether training or testing (testing selects actions greedily)\n",
    "        # batch_size = batch size\n",
    "        # Outputs -------------------------------------------------------------------\n",
    "        # term = binary variable where 1 indicates proposal accepted => game finished (longtensor of shape batch_size x 1)\n",
    "        # message = crafted linguistic message (longtensor of shape batch_size x len_message)\n",
    "        # prop = crafted proposal (longtensor of shape batch_size x num_types)\n",
    "        # entropy_loss = Number containing the sum of policy entropies (should be total entropy by additivity)\n",
    "        \n",
    "        # Update batch_size variable (changes throughout training due to sieving (see survivors below))\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Extract inputs ------------------------------------------------------------\n",
    "        \n",
    "        # Item context\n",
    "        x1 = x[0]\n",
    "        # Previous linguistic message\n",
    "        x2 = x[1]\n",
    "        # Previous proposal\n",
    "        x3 = x[2]  \n",
    "\n",
    "        # Encoding ------------------------------------------------------------------\n",
    "\n",
    "        # Initial embedding\n",
    "        x1 = self.encoder1(x1).transpose(0,1)\n",
    "        x2 = self.encoder2(x2).transpose(0,1)\n",
    "        x3 = self.encoder1(x3).transpose(0,1) # Same encoder as item context\n",
    "            \n",
    "        # LSTM for item context\n",
    "        h1 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial hidden\n",
    "        c1 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial cell\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            h1 = h1.cuda()\n",
    "            c1 = c1.cuda()\n",
    "\n",
    "        for i in range(x1.size()[0]):\n",
    "            _, (h1,c1) = self.lstm1(x1[i].view(1,self.batch_size,self.embedding_dim),(h1,c1))\n",
    "        x1_encoded = h1\n",
    "        \n",
    "        # LSTM for linguistic\n",
    "        h2 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial hidden\n",
    "        c2 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial cell\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            h2 = h2.cuda()\n",
    "            c2 = c2.cuda()\n",
    "\n",
    "        for i in range(x2.size()[0]):\n",
    "            _, (h2,c2) = self.lstm2(x2[i].view(1,self.batch_size,self.embedding_dim),(h2,c2))\n",
    "        x2_encoded = h2\n",
    "        \n",
    "        # LSTM for proposal\n",
    "        h3 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial hidden\n",
    "        c3 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial cell\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            h3 = h3.cuda()\n",
    "            c3 = c3.cuda()\n",
    "\n",
    "        for i in range(x3.size()[0]):\n",
    "            _, (h3,c3) = self.lstm2(x3[i].view(1,self.batch_size,self.embedding_dim),(h3,c3))\n",
    "        x3_encoded = h3\n",
    "\n",
    "        # Concatenate side-by-side\n",
    "        h = torch.cat([x1_encoded,x2_encoded,x3_encoded],2)\n",
    "\n",
    "        # Feedforward\n",
    "        h = self.ff(h)\n",
    "        h = F.relu(h) # Hidden layer input for policy networks\n",
    "        \n",
    "        # Policy ------------------------------------------------------------------\n",
    "\n",
    "        # Termination -----------------------------------------------\n",
    "        p_term = F.sigmoid(self.policy_term(h)).view(self.batch_size,1)\n",
    "        print(h.sum())\n",
    "\n",
    "        # Entropy\n",
    "        one_tensor = torch.ones(self.batch_size,1)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            one_tensor = one_tensor.cuda()\n",
    "        entropy_term = -(p_term * p_term.log()) - (one_tensor-p_term) * (one_tensor-p_term.log())\n",
    "        entropy_term = torch.sum(entropy_term)\n",
    "        \n",
    "        if test:\n",
    "            # Greedy\n",
    "            term = torch.round(p_term).long()\n",
    "        else:\n",
    "            # Sample\n",
    "            term = torch.bernoulli(p_term).long()\n",
    "            \n",
    "        # log p for REINFORCE\n",
    "        log_p_term = torch.zeros(self.batch_size,1)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            log_p_term = log_p_term.cuda()\n",
    "\n",
    "        log_p_term = (term.float() * (p_term+sys.float_info.min).log()) + (one_tensor-term.float()) * (one_tensor-p_term+sys.float_info.min).log()\n",
    "        \n",
    "        # Linguistic construction ----------------------------------\n",
    "        h_ling = h.clone() # Initial hidden state\n",
    "        c_ling = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial cell state\n",
    "        letter = torch.zeros(self.batch_size,1).long() # Initial letter (dummy)\n",
    "        entropy_letter = torch.zeros([self.batch_size,len_message])\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            c_ling = c_ling.cuda()\n",
    "            letter = letter.cuda()\n",
    "            entropy_letter = entropy_letter.cuda()\n",
    "        \n",
    "        # log p for REINFORCE \n",
    "        log_p_letter = torch.zeros([self.batch_size,1])\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            log_p_letter = log_p_letter.cuda()\n",
    "\n",
    "        message = torch.zeros(self.batch_size,len_message) # Message\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            message = message.cuda()\n",
    "        for i in range(len_message):\n",
    "            embedded_letter = self.encoder2(letter)\n",
    "\n",
    "            _, (h_ling,c_ling) = self.policy_ling(embedded_letter.view(1,self.batch_size,self.embedding_dim),(h_ling,c_ling))\n",
    "            logit = self.ff_ling(h_ling)\n",
    "            p_letter = F.softmax(logit,dim=2).view(self.batch_size,num_vocab)\n",
    "\n",
    "            entropy_letter[:,i] = -torch.sum(p_letter*p_letter.log(),1)\n",
    "\n",
    "            if test:\n",
    "                # Greedy\n",
    "                letter = p_letter.argmax(dim=1).view(self.batch_size,1).long()\n",
    "            else:\n",
    "                # Sample\n",
    "                letter = torch.multinomial(p_letter,1).long()\n",
    "                \n",
    "            # Gather the probabilities for the letters we've picked\n",
    "            probs = torch.gather(p_letter, 1, letter)\n",
    "            log_p_letter = log_p_letter + probs.log()\n",
    "                \n",
    "            message[:,i] = letter.squeeze()\n",
    "            \n",
    "        message = message.long()\n",
    "        entropy_letter = torch.sum(entropy_letter)     \n",
    "   \n",
    "        # Proposal ----------------------------------------------\n",
    "        p_prop = []\n",
    "        prop = []\n",
    "        \n",
    "        #prop = torch.zeros([self.batch_size,num_types]).long()\n",
    "        entropy_prop_list = [0,0,0]\n",
    "        \n",
    "        # log p for REINFORCE \n",
    "        log_p_prop = torch.zeros([self.batch_size,1])\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            log_p_prop = log_p_prop.cuda()\n",
    "\n",
    "        for i in range(num_types):\n",
    "            p_prop.append(F.sigmoid(self.policy_prop[i](h)))\n",
    "            \n",
    "            entropy_prop_list[i] = -torch.sum(p_prop[i]*p_prop[i].log())\n",
    "            \n",
    "            p_prop[i] = p_prop[i].view(self.batch_size,max_item+1)\n",
    "\n",
    "            if test:\n",
    "                # Greedy\n",
    "                #prop[:,i] = p_prop[i].argmax(dim=1)\n",
    "                prop.append(p_prop[i].argmax(dim=1))\n",
    "            else:\n",
    "                # Sample\n",
    "                #prop[:,i] = torch.multinomial(p_prop,1)\n",
    "                prop.append(torch.multinomial(p_prop,1))\n",
    "                \n",
    "            # Gather the probabilities for the letters we've picked\n",
    "            probs = torch.gather(p_prop[i], 1, prop[i].view(self.batch_size,1))\n",
    "            log_p_prop = log_p_prop + probs.log()\n",
    "              \n",
    "        prop = torch.stack(prop).transpose(0,1)\n",
    "        entropy_prop = sum(entropy_prop_list) # Entropy for exploration\n",
    "\n",
    "        # Combine -----------------------------------------------------------------\n",
    "        entropy_loss = torch.sum(lambda1*entropy_term + lambda1*entropy_prop + lambda2*entropy_letter)\n",
    "        self.log_p = log_p_term + log_p_letter + log_p_prop\n",
    "\n",
    "        return (term,message,prop, entropy_loss, log_p_term,log_p_letter,log_p_prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(476.8357, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "net = combined_policy()\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    net = net.cuda()\n",
    "\n",
    "# Dummy inputs\n",
    "x = torch.randint(0,max_item,[128,6]).long()\n",
    "y = torch.randint(0,num_vocab,[128,6]).long()\n",
    "z = torch.randint(0,max_item,[128,3]).long()\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    z = z.cuda()\n",
    "\n",
    "blah = net([x,y,z],True)\n",
    "\n",
    "# Initialize agents\n",
    "Agents = []\n",
    "for i in range(num_agents):\n",
    "    Agents.append(combined_policy())\n",
    "    if torch.cuda.is_available() and use_cuda:\n",
    "        Agents[i] = Agents[i].cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start ----------------\n",
      "tensor(553.2516, device='cuda:0')\n",
      "tensor(477.9747, device='cuda:0')\n",
      "tensor(293.2145, device='cuda:0')\n",
      "tensor(295.4312, device='cuda:0')\n",
      "tensor(310.0394, device='cuda:0')\n",
      "tensor(313.8653, device='cuda:0')\n",
      "tensor(283.9988, device='cuda:0')\n",
      "tensor(237.9163, device='cuda:0')\n",
      "tensor(165.3361, device='cuda:0')\n",
      "tensor(193.7599, device='cuda:0')\n",
      "tensor(1702.0530, device='cuda:0')\n",
      "tensor(2439.4226, device='cuda:0')\n",
      "tensor(2680.5474, device='cuda:0')\n",
      "tensor(3222.7632, device='cuda:0')\n",
      "tensor(3123.9238, device='cuda:0')\n",
      "tensor(3143.0081, device='cuda:0')\n",
      "tensor(2784.2690, device='cuda:0')\n",
      "tensor(2303.7251, device='cuda:0')\n",
      "tensor(1772.9055, device='cuda:0')\n",
      "tensor(1324.5183, device='cuda:0')\n",
      "tensor(5586.4419, device='cuda:0')\n",
      "tensor(8400.6738, device='cuda:0')\n",
      "tensor(9182.8701, device='cuda:0')\n",
      "tensor(10772.6963, device='cuda:0')\n",
      "tensor(9081.2178, device='cuda:0')\n",
      "tensor(8355.5332, device='cuda:0')\n",
      "tensor(7079.5942, device='cuda:0')\n",
      "tensor(5029.6021, device='cuda:0')\n",
      "tensor(2989.1328, device='cuda:0')\n",
      "tensor(2585.0933, device='cuda:0')\n",
      "tensor(16103.1885, device='cuda:0')\n",
      "tensor(20088.2500, device='cuda:0')\n",
      "tensor(21931.9219, device='cuda:0')\n",
      "tensor(24010.6133, device='cuda:0')\n",
      "tensor(20501.7012, device='cuda:0')\n",
      "tensor(16927.5430, device='cuda:0')\n",
      "tensor(14529.7500, device='cuda:0')\n",
      "tensor(11750.4385, device='cuda:0')\n",
      "tensor(6621.6807, device='cuda:0')\n",
      "tensor(4072.0928, device='cuda:0')\n",
      "tensor(30761.9844, device='cuda:0')\n",
      "tensor(35307.2617, device='cuda:0')\n",
      "tensor(37310.7422, device='cuda:0')\n",
      "tensor(38374.5703, device='cuda:0')\n",
      "tensor(35530.7891, device='cuda:0')\n",
      "tensor(29962.7812, device='cuda:0')\n",
      "tensor(23471.4688, device='cuda:0')\n",
      "tensor(18998.1504, device='cuda:0')\n",
      "tensor(13658.3662, device='cuda:0')\n",
      "tensor(8440.3652, device='cuda:0')\n",
      "tensor(46605.7812, device='cuda:0')\n",
      "tensor(51045.6875, device='cuda:0')\n",
      "tensor(52439.6602, device='cuda:0')\n",
      "tensor(49243.7578, device='cuda:0')\n",
      "tensor(42826.7227, device='cuda:0')\n",
      "tensor(33512.5312, device='cuda:0')\n",
      "tensor(30474.8164, device='cuda:0')\n",
      "tensor(19982.7109, device='cuda:0')\n",
      "tensor(15505.2139, device='cuda:0')\n",
      "tensor(10854.1699, device='cuda:0')\n",
      "tensor(63849.4922, device='cuda:0')\n",
      "tensor(62373.3594, device='cuda:0')\n",
      "tensor(68800.6719, device='cuda:0')\n",
      "tensor(63329.2969, device='cuda:0')\n",
      "tensor(63303.7344, device='cuda:0')\n",
      "tensor(42357.2852, device='cuda:0')\n",
      "tensor(40516.1797, device='cuda:0')\n",
      "tensor(23621.3320, device='cuda:0')\n",
      "tensor(17247.4766, device='cuda:0')\n",
      "tensor(10819.9033, device='cuda:0')\n",
      "tensor(83889.7578, device='cuda:0')\n",
      "tensor(77280.1250, device='cuda:0')\n",
      "tensor(89527.8906, device='cuda:0')\n",
      "tensor(81761.0938, device='cuda:0')\n",
      "tensor(77065.7031, device='cuda:0')\n",
      "tensor(57189.5078, device='cuda:0')\n",
      "tensor(54590.6797, device='cuda:0')\n",
      "tensor(36616.7773, device='cuda:0')\n",
      "tensor(26289.2305, device='cuda:0')\n",
      "tensor(19035.7949, device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "Runtime for episodes 0-10: 2.758333444595337s\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "Runtime for episodes 10-20: 0.4005429744720459s\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "Runtime for episodes 20-30: 0.39002299308776855s\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "Runtime for episodes 30-40: 0.35587644577026367s\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "Runtime for episodes 40-50: 0.3494527339935303s\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "Runtime for episodes 50-60: 0.35650110244750977s\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "Runtime for episodes 60-70: 0.3495750427246094s\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "Runtime for episodes 70-80: 0.37275123596191406s\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "Runtime for episodes 80-90: 0.4030418395996094s\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "Runtime for episodes 90-100: 0.4032778739929199s\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "Runtime for episodes 100-110: 0.4118039608001709s\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "Runtime for episodes 110-120: 0.3985781669616699s\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "Runtime for episodes 120-130: 0.4045257568359375s\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "Runtime for episodes 130-140: 0.3989696502685547s\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "Runtime for episodes 140-150: 0.40114426612854004s\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "Runtime for episodes 150-160: 0.36596202850341797s\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "Runtime for episodes 160-170: 0.3551962375640869s\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "Runtime for episodes 170-180: 0.3922250270843506s\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "Runtime for episodes 180-190: 0.40116238594055176s\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "End ------------------\n",
      "Total runtime: 10.032737016677856s\n"
     ]
    }
   ],
   "source": [
    "# Random seeds for testing\n",
    "torch.manual_seed(0)\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    torch.cuda.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "    \n",
    "baselines = [0 for _ in range(num_agents)] # Baselines for reward calculation\n",
    "\n",
    "# Initialize optimizers for learning\n",
    "optimizers = []\n",
    "for i in range(num_agents):\n",
    "    optimizers.append(optim.Adam(Agents[i].parameters()))\n",
    "    \n",
    "# Train rewards\n",
    "r_list = []\n",
    "for i in range(num_agents):\n",
    "    r_list.append([])\n",
    "\n",
    "print('Start ----------------')\n",
    "time_start = time.time()\n",
    "time_p1 = time.time()\n",
    "# Loop over episodes\n",
    "for i_ep in range(N_ep):\n",
    "    # Setting up games -----------------------------------------------------------------------\n",
    "    # Game setup\n",
    "    # Truncated Poisson sampling for number of turns in each game\n",
    "    N = truncated_poisson_sampling(lam, min_N, max_N, num_games)\n",
    "    # Item pools for each game\n",
    "    pool = create_item_pool(num_types, max_item, num_games)\n",
    "    if torch.cuda.is_available() and use_cuda:\n",
    "        N = N.cuda()\n",
    "        pool = pool.cuda()\n",
    "    # Item contexts for each game\n",
    "    item_contexts = [] # Each agent has different utilities (but same pool)\n",
    "    for i in range(num_agents):\n",
    "        utility = create_agent_utility(num_types, max_utility, num_games)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            utility = utility.cuda()\n",
    "        item_contexts.append(torch.cat([pool, utility],1))\n",
    "    \n",
    "    # Initializations\n",
    "    survivors = torch.ones(num_games).nonzero()               # Keeps track of ongoing games; everyone alive initially\n",
    "    num_alive = len(survivors)                                # Actual batch size for each turn (initially num_games)\n",
    "    prev_messages = torch.zeros(num_games, len_message).long() # Previous linguistic message for each game\n",
    "    prev_proposals = torch.zeros(num_games, num_types).long()  # Previous proposal for each game\n",
    "    if torch.cuda.is_available() and use_cuda:\n",
    "        survivors = survivors.cuda()\n",
    "        prev_messages = prev_messages.cuda()\n",
    "        prev_proposals = prev_proposals.cuda()\n",
    "    \n",
    "    rewards = [torch.zeros(num_games), torch.zeros(num_games)]       # Rewards for each game for each agent\n",
    "    # Keep track of sum of all rewards (from all games in a batch) for baseline updates (see corpses below)\n",
    "    reward_sums = []\n",
    "    for i in range(num_agents):\n",
    "        reward_sums.append(torch.zeros(1)) # Just a number\n",
    "        Agents[i].log_p = torch.zeros([num_games,1])\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            rewards[i] = rewards[i].cuda()\n",
    "            reward_sums[i] = reward_sums[i].cuda()\n",
    "            Agents[i].log_p = Agents[i].log_p.cuda()\n",
    "\n",
    "    # Play the games -------------------------------------------------------------------------\n",
    "    for i_turn in range(max_N): # Loop through maximum possible number of turns for all games\n",
    "        \n",
    "        # Losses for each agent\n",
    "        reward_losses = [torch.zeros([],requires_grad=True), torch.zeros([],requires_grad=True)]  \n",
    "        entropy_losses = [torch.zeros([],requires_grad=True), torch.zeros([],requires_grad=True)] # Exploration\n",
    "        for j in range(num_agents):\n",
    "            Agents[j].log_p = torch.zeros([num_alive,1])\n",
    "            \n",
    "            if torch.cuda.is_available() and use_cuda:\n",
    "                reward_losses[j] = reward_losses[j].cuda()\n",
    "                entropy_losses[j] = entropy_losses[j].cuda()\n",
    "                Agents[j].log_p = Agents[j].log_p.cuda()\n",
    "        \n",
    "        # Agent IDs\n",
    "        id_1 = i_turn % 2    # Current player\n",
    "        id_2 = int(not id_1) # Other player\n",
    "        \n",
    "        # Remove finished games (batch size decreases)\n",
    "        N = N[survivors].view(num_alive, 1)\n",
    "        pool = pool[survivors].view(num_alive, num_types)\n",
    "        prev_messages = prev_messages[survivors].view(num_alive, len_message)\n",
    "        prev_proposals = prev_proposals[survivors].view(num_alive, num_types)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            N = N.cuda()\n",
    "            pool = pool.cuda()\n",
    "            prev_messages = prev_messages.cuda()\n",
    "            prev_proposals = prev_proposals.cuda()\n",
    "        # Quantities different for each agent\n",
    "        for j in range(num_agents):\n",
    "            item_contexts[j] = item_contexts[j][survivors].view(num_alive,num_types*2)\n",
    "            #rewards[j] = rewards[j][survivors].view(num_alive)\n",
    "            #reward_losses[j] = reward_losses[j][survivors].view(num_alive)\n",
    "            if torch.cuda.is_available() and use_cuda:\n",
    "                item_contexts[j] = item_contexts[j].cuda()\n",
    "        \n",
    "        # Agent currently playing\n",
    "        Agent = Agents[id_1]             \n",
    "        item_context = item_contexts[id_1]\n",
    "        \n",
    "        # Play the game -------------------------------------------------------------\n",
    "        term, prev_messages, proposals, entropy_loss, lt,ll,lp = Agent([item_context, prev_messages, prev_proposals], True, num_alive)\n",
    "        entropy_losses[id_1] = entropy_loss\n",
    "        \n",
    "        # Compute reward loss (assumes 2 agents) ------------------------------------\n",
    "        # Games terminated by the current agent (previous proposal accepted)\n",
    "        \n",
    "        finishers = term.squeeze().nonzero()          # squeeze is for getting rid of extra useless dimension that pops up for some reason\n",
    "        num_finishers = len(finishers)\n",
    "        \n",
    "        if len(finishers) != 0:\n",
    "            pool_12 = pool[finishers].view(num_finishers,num_types)\n",
    "            \n",
    "            share_2 = prev_proposals[finishers].view(num_finishers,num_types) # Share of other (previous proposal) \n",
    "            share_1 = pool_12 - share_2 # Share of this agent (remainder)\n",
    "            \n",
    "            # Zero reward if proposal exceeds pool\n",
    "            invalid_batches = torch.sum(share_2>pool_12,1)>0\n",
    "            share_2[invalid_batches] = 0\n",
    "            share_1[invalid_batches] = 0\n",
    "            \n",
    "            utility_1 = item_contexts[id_1][:,num_types:] # Recall that item context is a concatenation of pool and utility\n",
    "            utility_1 = utility_1[finishers].view(num_finishers,num_types)\n",
    "            utility_2 = item_contexts[id_2][:,num_types:]\n",
    "            utility_2 = utility_2[finishers].view(num_finishers,num_types)\n",
    "\n",
    "            log_p_1 = Agents[id_1].log_p[finishers].view(num_finishers,1)\n",
    "            log_p_2 = Agents[id_2].log_p[finishers].view(num_finishers,1)\n",
    "\n",
    "            # Calculate reward and reward losses\n",
    "            r1, rl1 = rewards_func(share_1, utility_1, pool_12, log_p_1, baselines[id_1])\n",
    "            r2, rl2 = rewards_func(share_2, utility_2, pool_12, log_p_2, baselines[id_2])\n",
    "            '''\n",
    "            if (torch.sum(r1) > 128):\n",
    "                for i in range(128):\n",
    "                    if r1[i] > 1:\n",
    "                        print(r1[i])\n",
    "                        print(share_2[i])\n",
    "                        print(share_1[i])\n",
    "                        print(utility_1[i])\n",
    "                        print(utility_2[i])\n",
    "                        print(pool_12[i])\n",
    "                        print(lt[i])\n",
    "                        print(lp[i])\n",
    "                        print(ll[i])\n",
    "                        print(baselines)\n",
    "                        sys.exit(0);\n",
    "            '''\n",
    "            \n",
    "            # Add rewards and reward losses\n",
    "            rewards[id_1] = r1.squeeze()\n",
    "            rewards[id_2] = r2.squeeze()\n",
    "            reward_losses[id_1] = rl1\n",
    "            reward_losses[id_2] = rl2\n",
    "            reward_sums[id_1] = reward_sums[id_1] + rewards[id_1].sum()\n",
    "            reward_sums[id_2] = reward_sums[id_2] + rewards[id_2].sum()\n",
    "\n",
    "        prev_proposals = proposals # Don't need previous proposals anymore so update it\n",
    "        \n",
    "        # Gradient descent -----------------------------------------------------------\n",
    "        \n",
    "        for i in range(num_agents):\n",
    "            # optimize\n",
    "            loss = reward_losses[i] + entropy_losses[i]\n",
    "            optimizers[i].zero_grad()\n",
    "            loss.backward()\n",
    "            optimizers[i].step()\n",
    "        \n",
    "        # Wrapping up the end of turn ------------------------------------------------\n",
    "        # Remove finished games\n",
    "        # In term and term_N, element = 1 means die\n",
    "        term_N = (N <= (i_turn+1)).view(num_alive,1).long() # Last turn reached; i_turn + 1 since i_turn starts counting from 0\n",
    "        # In survivors, element = 1 means live\n",
    "        survivors = (term+term_N) == 0\n",
    "\n",
    "        # Check if everyone's dead\n",
    "        if survivors.sum() == 0: # If all games over, break episode\n",
    "            # Baseline updates\n",
    "            for i in range(num_agents):\n",
    "                # Update with batch-averaged rewards\n",
    "                baselines[i] = smoothing_const * baselines[i] + (1-smoothing_const)*reward_sums[i]/num_games\n",
    "            break;\n",
    "            \n",
    "        # Reshape\n",
    "        survivors = ((term+term_N) == 0).nonzero()[:,0].view(-1,1)\n",
    "        num_alive = len(survivors) # Number of survivors\n",
    "\n",
    "        #print('i_turn = ' + str(i_turn))\n",
    "        \n",
    "    #print('i_ep = ' + str(i_ep))\n",
    "    if (i_ep % ep_time == 0) and (i_ep != 0):\n",
    "        time_p2 = time.time()\n",
    "        print('Runtime for episodes ' + str(i_ep-ep_time) + '-' + str(i_ep) + ': ' + str(time_p2 - time_p1) + 's')\n",
    "        time_p1 = time_p2\n",
    "        \n",
    "    #if (i_ep % ep_record == 0):\n",
    "    #    for j in range(num_agents):\n",
    "    #        r_list[j].append(reward_sums[j]/num_games)\n",
    "    \n",
    "    #print('----------------')\n",
    "    \n",
    "print('End ------------------')\n",
    "time_finish = time.time()\n",
    "print('Total runtime: ' + str(time_finish-time_start) + 's')\n",
    "\n",
    "#for i in range(num_agents):\n",
    "#    torch.save(Agents[0].state_dict(),'saved_model_agent_' + str(i) + '.pt')\n",
    "    \n",
    "#Agents[0].load_state_dict(torch.load('saved_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         ...,\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "         [-1.4294,  0.7757,  2.4056,  ..., -0.9166, -1.0313, -0.3089],\n",
       "         [-1.0103,  0.1748,  0.0584,  ..., -3.1818,  1.4392,  0.2379],\n",
       "         ...,\n",
       "         [ 0.6998, -0.3224, -2.4143,  ...,  1.0166, -1.5080, -0.2623],\n",
       "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "         [ 1.1913, -0.7837,  1.0883,  ...,  0.5980, -0.1940, -1.3195]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         ...,\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         ...,\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         ...,\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         ...,\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[-2.0385e-02,  5.9105e-02, -4.0800e-02,  ..., -5.8295e-02,\n",
       "          -6.6405e-02,  2.5446e-02],\n",
       "         [-7.5911e-02, -8.2912e-02, -4.6900e-02,  ..., -5.0662e-03,\n",
       "           1.7122e-02, -1.7935e-02],\n",
       "         [ 8.4523e-02, -1.2577e-02, -6.7375e-02,  ..., -3.1100e-02,\n",
       "          -1.7061e-02,  2.1024e-02],\n",
       "         ...,\n",
       "         [-7.2562e-02, -5.0637e-02, -6.7012e-02,  ...,  9.3706e-02,\n",
       "          -7.3641e-02, -1.3545e-02],\n",
       "         [-3.8184e-02,  6.0776e-02,  8.4458e-02,  ..., -2.5728e-02,\n",
       "          -2.6913e-02, -5.9943e-02],\n",
       "         [-3.5755e-02,  7.6826e-02,  5.3910e-02,  ...,  4.1468e-02,\n",
       "          -2.8385e-02,  2.4472e-02]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [[-2.9987, -2.4510, -0.2551,  ...,  1.9497,  2.2422,  5.9182],\n",
       "         [ 1.5585, -9.2872,  0.2864,  ..., -1.2705, -4.4719,  8.6252],\n",
       "         [ 4.9851, -3.8547,  8.9771,  ..., -2.1956, -5.7931, -3.8036],\n",
       "         ...,\n",
       "         [-2.2670,  3.7260,  6.9512,  ...,  3.4356, -5.2289, -9.5431],\n",
       "         [ 4.4117,  0.7767, -5.4590,  ..., -4.1730, -7.0445, -3.6045],\n",
       "         [ 0.4776, -9.0534, -3.6145,  ...,  1.1273,  4.7698,  8.3228]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [ 4.5602,  8.4276, -7.7909,  0.6497, -4.1867, -7.6199,  5.7811,\n",
       "          4.4979,  0.2292,  5.6851, -7.0686,  6.8350,  9.3544, -1.5705,\n",
       "         -0.9651, -2.1289,  8.9946,  9.2454, -2.6875, -6.9363,  3.1487,\n",
       "         -5.4708, -0.6135, -7.5112, -4.9501,  9.6454,  1.9923,  0.4688,\n",
       "         -7.4642,  1.1138, -8.0091, -9.7860, -2.3155,  8.4050,  5.8230,\n",
       "         -3.5823,  1.8589,  1.1372, -7.2328,  0.6231, -3.2016, -6.3690,\n",
       "          0.3281,  3.5915, -3.1030, -9.6397, -9.7383,  7.6926,  2.7622,\n",
       "         -5.0497, -1.8519, -3.5936, -7.5695, -3.8998,  8.6992,  6.0105,\n",
       "          1.1392, -5.3645, -8.5180,  1.9541,  8.9345,  3.4967,  8.0239,\n",
       "          4.0688, -3.9861,  5.5758, -2.2574, -1.5410, -3.8803,  2.6643,\n",
       "         -6.5612,  8.9081,  5.0179,  9.6786,  4.2254, -4.6000,  4.3239,\n",
       "          4.0855,  8.8771,  8.5361, -8.5907, -1.6676, -5.9586,  4.8283,\n",
       "          2.5862, -1.6077,  7.3766,  2.5372, -1.3578,  9.3150,  2.8175,\n",
       "         -2.5881,  5.3730,  1.9986, -0.1714,  6.3203, -7.4331,  8.5608,\n",
       "          0.0592, -3.7483, -4.1510, -3.9068,  5.7891,  7.1988,  8.8047,\n",
       "          8.8751, -1.0022, -9.9813,  1.6849,  1.1098, -3.2071,  2.6709,\n",
       "          7.3623,  4.2469,  3.5004,  0.8392,  8.3226,  6.6430, -7.4826,\n",
       "          7.2915,  2.1713,  7.1827, -5.7106,  7.1875, -2.1859,  3.3329,\n",
       "         -3.4924, -8.8814, -2.5006,  1.9814,  8.8462,  1.3159,  1.7273,\n",
       "         -0.7345,  4.0685, -4.1346,  5.3966,  8.3395,  6.5074, -8.5444,\n",
       "         -9.1848, -1.0780, -5.7512, -0.3246, -2.3566,  2.7193,  5.4559,\n",
       "         -9.0164, -1.7533,  2.6522,  6.4140, -3.1032, -5.7432,  9.0667,\n",
       "          1.1178, -1.9111,  4.8920, -1.4217,  6.4840,  7.9265,  8.1909,\n",
       "         -2.8884, -9.5902,  4.4321, -9.2482, -1.8660,  0.2481, -6.8630,\n",
       "          8.4340, -7.3496,  6.5666, -9.2003, -3.1870,  0.5115,  1.4984,\n",
       "         -7.6119,  1.0434,  6.7493,  7.4456,  9.8714, -5.9544, -9.2886,\n",
       "         -6.1262,  6.5299,  0.8088, -8.9973, -8.3259,  6.2962,  3.4072,\n",
       "         -4.0627, -2.0638, -8.7514, -4.2806,  8.4281, -3.2238,  4.4160,\n",
       "          0.3155,  3.7212, -8.9446,  3.1041,  1.1592,  8.9349, -4.6975,\n",
       "          3.5966, -3.8415,  3.0693, -5.4815, -7.8325,  9.1566, -4.6241,\n",
       "         -7.8731, -9.0505, -0.3743, -5.9185,  7.5598,  4.5021,  7.5102,\n",
       "          5.1342,  1.6639, -3.1051, -1.3002, -4.6029, -7.1254,  0.4594,\n",
       "         -9.2617,  1.2613, -2.9448, -2.9275, -3.5021,  1.0325,  0.2778,\n",
       "         -2.6378, -1.1309,  6.0528,  9.9041,  4.4405, -6.1634,  3.4324,\n",
       "         -5.9052,  7.0434, -4.8305, -3.7323, -1.1226, -2.4795,  1.9089,\n",
       "         -4.0342,  9.5458,  9.2005, -0.8126, -5.1247, -6.5531,  1.6006,\n",
       "         -8.9615, -8.7588,  4.3598,  3.9267,  2.9836, -7.0914,  8.9148,\n",
       "          4.2635,  3.6669,  2.2346, -7.1894,  2.3269,  4.6981,  0.3932,\n",
       "         -8.7199, -6.4190, -6.4989,  9.5656,  7.4826,  7.3702,  1.7918,\n",
       "         -3.7571,  2.9125, -8.8660,  6.8616,  2.8003,  1.5338, -8.1347,\n",
       "          0.1256,  2.4197,  9.3866, -8.8991, -2.6980, -3.8156,  2.7502,\n",
       "         -5.0198,  1.5604, -7.1069,  9.0816, -8.4904, -7.8286,  3.8353,\n",
       "          6.9904, -1.9194,  1.4937, -9.0922, -8.5658,  7.3838, -9.7765,\n",
       "         -1.0032,  8.8229,  6.8609, -1.3381,  7.8653, -4.8281,  4.1828,\n",
       "         -3.4101,  4.1824, -0.3581,  6.3542, -2.9488, -4.4346,  2.2308,\n",
       "         -4.1888,  5.4472,  6.8774, -3.8826, -1.6918, -6.6719, -6.5156,\n",
       "         -9.0144, -7.2810,  5.6243,  9.7777,  5.3883,  7.7808,  2.9268,\n",
       "         -7.1801,  9.5768,  5.6866,  1.6450, -2.5327, -0.9539,  9.6148,\n",
       "          9.5292, -5.5085,  2.0308, -6.4113, -5.6369, -1.2477,  6.0421,\n",
       "          5.6741,  0.1476,  7.2940, -3.0160,  8.1788,  9.4173,  6.4342,\n",
       "         -6.9939, -7.6464,  2.1890,  3.5472, -8.5101,  9.2288, -0.5185,\n",
       "          7.6132,  8.7802,  4.3239,  1.1762, -6.0058, -6.7617,  4.0507,\n",
       "          1.3609,  9.8888,  4.7893, -3.9694,  1.1941,  8.7962,  1.8837,\n",
       "         -4.3753, -5.4788, -5.8071,  6.3685, -0.3272, -8.6041,  7.4772,\n",
       "         -4.2927, -9.9221, -1.9214,  7.2354, -8.1847,  3.1812,  1.9440,\n",
       "         -1.1792,  6.8769,  8.6710, -1.7487,  9.9756,  7.1855,  0.3358,\n",
       "          0.6447, -6.5233,  6.1590, -4.1607,  7.0567, -3.9055,  7.8913,\n",
       "         -9.2510], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [ 8.6093,  7.9249, -2.6238, -7.4697,  1.7004,  7.6724, -5.6245,\n",
       "         -1.2076,  6.4745,  7.5386, -2.2608, -2.6321,  8.8427,  2.5783,\n",
       "          5.6728, -0.5103,  0.6675, -4.6643,  7.8520,  8.9754,  7.1740,\n",
       "          5.5123,  4.6015,  9.5329,  7.7119, -5.3867, -8.7405,  2.7608,\n",
       "         -9.2412, -0.0154, -1.3081,  4.2081,  8.2111, -3.0367,  3.0650,\n",
       "         -3.5743,  0.7407, -4.5912, -0.1674, -9.0627,  1.9540,  8.7734,\n",
       "         -1.5199,  6.1081,  2.1524, -7.7540, -2.5697,  8.5250, -0.7794,\n",
       "         -5.6535,  6.2602,  3.6750, -0.2452,  7.4301,  6.7701,  6.8268,\n",
       "         -2.4829,  2.8518, -5.5976, -9.5338, -2.6352,  2.4516,  4.6884,\n",
       "         -6.6094, -5.5014,  1.3779, -9.8524,  0.1152, -1.1732,  3.5162,\n",
       "          2.3306, -6.4100, -3.1243,  2.3786,  2.1824, -3.4423, -0.4799,\n",
       "         -3.2209,  5.9302,  1.3751,  4.4902, -4.0293,  2.7580,  1.7043,\n",
       "         -7.1244,  7.6009,  8.9475, -8.5427, -9.2548,  7.7310, -5.7590,\n",
       "         -4.6497,  9.1709,  6.2044,  9.8350,  3.4311,  0.4204, -9.7485,\n",
       "          4.9884, -0.7067, -2.2853,  3.9251, -0.8607, -3.0856,  2.3143,\n",
       "         -0.1986, -9.4301, -4.2745, -4.5817,  7.5930,  9.5974,  5.2658,\n",
       "         -6.9720, -0.0685,  9.4503,  7.2476, -1.7498,  7.7673, -5.2722,\n",
       "          4.7832,  7.1466,  8.7699,  7.0099,  3.3910, -4.5425, -6.7722,\n",
       "          6.1984,  3.9150,  7.8571,  6.9235,  5.9091, -8.5587,  4.2861,\n",
       "          8.5221,  8.8839, -1.0433,  1.9728, -6.7227,  9.2181, -1.3743,\n",
       "          3.9880,  8.4908, -2.4381, -0.5847,  8.7620,  1.8971, -8.3093,\n",
       "         -8.4041,  5.6178,  7.9963,  9.6309, -1.3803,  1.6774, -2.8174,\n",
       "          4.6212, -7.9032, -6.4190, -8.2973,  2.8788,  5.2811, -9.2181,\n",
       "         -2.8113,  7.6828,  0.3684,  4.4785,  3.0839,  0.5088,  4.3471,\n",
       "         -3.6734, -7.0512,  2.4846,  6.6039,  6.5898,  6.6698,  2.0886,\n",
       "         -0.2159,  3.4241,  3.1522, -8.4196, -7.6137,  8.4421, -3.2790,\n",
       "          0.8655,  5.2722, -4.1039,  7.9100, -6.2298, -7.4439,  7.6290,\n",
       "          3.9682,  5.4419, -2.2340,  6.7122, -6.5516, -6.7243,  2.7601,\n",
       "          0.5776,  8.9289, -3.2829, -7.9196,  8.3690,  8.0749, -3.3644,\n",
       "         -3.2839,  9.9191,  6.7077,  5.1692,  5.4394,  4.2286,  8.1786,\n",
       "          0.4893, -6.5410,  8.8089,  1.0700, -2.6632,  1.9899, -0.8065,\n",
       "          0.5057,  4.5919, -8.3879, -8.1295,  6.5673, -0.4716, -4.0962,\n",
       "          4.6076, -2.1744,  8.1490,  9.4022, -2.9795,  3.3879, -1.8926,\n",
       "         -2.7115, -1.3902,  7.9002, -4.2857, -1.9368, -5.1891, -7.8552,\n",
       "         -1.3930,  7.4219,  2.7126, -2.3980, -6.8396, -4.8442,  6.4543,\n",
       "         -3.2245, -2.0930, -1.8877, -9.9679,  8.6664,  6.9120,  5.0351,\n",
       "          1.2341,  2.7545,  8.3054, -1.5619, -7.9803, -8.6783, -2.7907,\n",
       "          9.4858, -1.7694,  9.2526,  2.1858, -0.8581, -6.6045,  4.7020,\n",
       "         -8.9665,  8.6066,  3.4742, -1.6364, -5.8147,  6.4043, -3.8437,\n",
       "         -4.2131,  9.4655, -7.9908,  9.4627,  3.5411,  6.3809,  7.6327,\n",
       "          1.3529,  3.8080,  5.1328, -1.0512, -5.4552, -3.6770,  0.9812,\n",
       "          6.7629, -0.9817,  9.4317, -3.4066, -8.2039,  5.1867, -4.1083,\n",
       "         -0.2346, -9.6467, -5.1572, -6.5779, -9.0402,  5.3852,  3.3116,\n",
       "         -3.3075,  4.4192, -3.6273, -9.5012, -1.5449, -4.9120,  6.2522,\n",
       "         -7.3937,  8.1356, -0.3746, -3.3773,  7.5230, -5.0260,  8.0413,\n",
       "         -4.5745,  5.7690, -0.2887, -6.5930, -5.8479, -2.5951, -2.8268,\n",
       "          4.1441,  8.1550, -0.3075, -5.1608, -7.5101, -2.7484,  3.2514,\n",
       "         -3.1479, -0.1239, -6.4753, -4.3837, -2.7374, -0.5417, -3.0556,\n",
       "          8.2535,  3.6472, -9.8110, -8.4767,  2.1748, -4.3320,  6.3837,\n",
       "          3.5092,  8.9926, -6.1411, -5.2558,  6.5278, -4.0631,  2.2817,\n",
       "          9.0558,  2.2853,  7.0329, -5.5678,  4.9869,  7.1248,  8.8367,\n",
       "          2.1115,  8.1558, -6.2088,  4.5642, -3.9198,  9.2794,  0.4006,\n",
       "          3.6110, -8.7961,  7.4842,  0.0342, -8.0299,  9.3418, -6.6298,\n",
       "         -3.6258,  1.3923, -1.4284, -6.8325, -0.9517,  8.3369, -6.3323,\n",
       "          0.9996, -6.4269,  9.4364,  1.4305, -4.4766,  6.4148,  1.3767,\n",
       "          9.9987, -1.3979,  7.5379, -7.3594, -7.6923,  9.8068, -9.8653,\n",
       "          7.8104, -0.3753,  6.7657, -3.6490,  1.3182,  6.4000,  2.6357,\n",
       "          8.4078], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         ...,\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [-3.9740,  4.3761,     nan,  1.6289,     nan,     nan,     nan,\n",
       "         -0.4215,     nan,     nan,     nan, -4.5319, -1.4972,     nan,\n",
       "             nan,  1.8492, -5.7099, -3.4198,     nan,     nan, -1.8147,\n",
       "          1.3849,  1.1474,  3.8232,     nan,     nan, -3.3866,  1.2411,\n",
       "         -0.5337,     nan,     nan,     nan,     nan,  1.7980, -2.5090,\n",
       "          1.5849, -4.3256,     nan,     nan,     nan,  4.4508,     nan,\n",
       "          0.3574,  0.5550, -2.0330, -1.4196, -3.4098, -5.6433,  4.1418,\n",
       "         -5.7363,  1.1808,     nan, -4.3284, -4.6784,     nan,  0.0651,\n",
       "          2.0704,     nan,     nan, -4.9868,  3.9166,  1.4603, -2.3610,\n",
       "             nan,     nan,     nan,     nan,  0.6601, -4.1960,     nan,\n",
       "         -5.7088,     nan,     nan,     nan,     nan, -6.5966,     nan,\n",
       "         -2.8701,  2.9979,     nan,     nan,  0.1752,     nan, -4.6721,\n",
       "             nan,     nan,     nan,  1.4000,     nan,     nan,  4.1776,\n",
       "             nan,     nan,     nan,     nan, -6.3315,     nan,     nan,\n",
       "         -0.3343, -2.8058], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([nan.], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         ...,\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         ...,\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.],\n",
       "         [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.],\n",
       "         [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.],\n",
       "         [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.],\n",
       "         [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.],\n",
       "         [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.],\n",
       "         [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.],\n",
       "         [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.],\n",
       "         [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.],\n",
       "         [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[-0.0478, -0.0070, -0.0988,  0.1015, -0.0896, -0.0256, -0.0831,\n",
       "          -0.0913,  0.0291, -0.0736, -0.0902,  0.0234, -0.0555,  0.0618,\n",
       "           0.0611,  0.0400,  0.0366, -0.0472,  0.0558, -0.0906,  0.0713,\n",
       "           0.0249,  0.0505, -0.0652, -0.0716, -0.0725, -0.0592,  0.0730,\n",
       "           0.0133, -0.0176,  0.0530, -0.0519, -0.0406, -0.0091,  0.1057,\n",
       "           0.0750, -0.0211, -0.0932, -0.0421,  0.0260, -0.0108, -0.0944,\n",
       "          -0.0435,  0.0539, -0.0402,  0.0874,  0.0782,  0.0299,  0.0723,\n",
       "          -0.0517, -0.0560,  0.0171, -0.0038,  0.0864, -0.0355, -0.0613,\n",
       "           0.0601, -0.0941, -0.1101,  0.0082,  0.0929, -0.0335, -0.0689,\n",
       "          -0.0530, -0.1077, -0.0179,  0.0551, -0.0452, -0.0266, -0.0293,\n",
       "          -0.0315, -0.0113,  0.0622,  0.0762, -0.0045, -0.0321,  0.0756,\n",
       "           0.0629, -0.0556, -0.0869, -0.0291, -0.0732, -0.0822, -0.0135,\n",
       "          -0.0555, -0.0742, -0.0821, -0.0729, -0.0499, -0.0654,  0.0664,\n",
       "           0.0865,  0.0271,  0.0835, -0.0094, -0.0344,  0.0119, -0.0395,\n",
       "           0.0764, -0.0563],\n",
       "         [-0.0058,  0.0803,  0.0695,  0.0816,  0.0467,  0.1252,  0.0818,\n",
       "           0.0940,  0.0306,  0.0605,  0.0937,  0.0359, -0.0703,  0.0769,\n",
       "           0.0176, -0.0580,  0.0634,  0.0639, -0.0211,  0.0584,  0.0180,\n",
       "           0.0052, -0.0537,  0.0724,  0.0120,  0.0947,  0.0730, -0.0348,\n",
       "           0.0875, -0.0289,  0.0337,  0.0375,  0.0208,  0.0834, -0.0291,\n",
       "          -0.0086,  0.0904, -0.0152,  0.0265,  0.0899,  0.0295, -0.0056,\n",
       "          -0.0830,  0.0689,  0.0634,  0.0782,  0.0687,  0.0616, -0.0770,\n",
       "          -0.0865,  0.0946, -0.0193,  0.0017,  0.0261,  0.0308,  0.0765,\n",
       "          -0.0724,  0.1259,  0.0043, -0.0267, -0.0638,  0.0474,  0.0937,\n",
       "           0.1008,  0.0814,  0.0063,  0.0056,  0.1025, -0.0400,  0.0604,\n",
       "           0.0630, -0.0626,  0.0745,  0.0086,  0.1145,  0.0264,  0.0242,\n",
       "          -0.0736, -0.0557, -0.0393,  0.0743, -0.0878, -0.0126,  0.0677,\n",
       "           0.0913,  0.0571,  0.1267, -0.0687,  0.0100,  0.0382,  0.0952,\n",
       "          -0.0650,  0.0020, -0.0551,  0.0372,  0.0652,  0.0618, -0.0100,\n",
       "           0.0564, -0.0584],\n",
       "         [-0.0578, -0.0742,  0.0068,  0.0483, -0.0218,  0.1089, -0.0279,\n",
       "          -0.0545, -0.0447,  0.1158,  0.0995,  0.0298,  0.0798,  0.1232,\n",
       "           0.0922, -0.0071,  0.0337,  0.0956,  0.0072,  0.0131,  0.1028,\n",
       "           0.0499,  0.0289, -0.0380,  0.0153,  0.0522, -0.0862, -0.0196,\n",
       "           0.0726, -0.0525, -0.0448, -0.0100,  0.1211, -0.0393,  0.0537,\n",
       "           0.0246,  0.0474, -0.0261,  0.0231,  0.0772,  0.0786,  0.1188,\n",
       "           0.0086, -0.0390,  0.0122, -0.0631,  0.0742, -0.0787, -0.0995,\n",
       "           0.0585,  0.0798,  0.1049,  0.0961,  0.0623,  0.0908,  0.0475,\n",
       "          -0.0177, -0.0318, -0.0096, -0.0369, -0.0838, -0.0355,  0.0407,\n",
       "           0.1163, -0.0140,  0.0635,  0.0256,  0.0756, -0.0697, -0.0464,\n",
       "          -0.0576,  0.0266, -0.0067,  0.0790,  0.0060,  0.0209,  0.0563,\n",
       "          -0.0065,  0.0218,  0.0441, -0.0316,  0.0598,  0.0634,  0.0584,\n",
       "           0.1214,  0.0610,  0.0624, -0.0657,  0.0822,  0.0466, -0.0354,\n",
       "           0.0560,  0.0687,  0.1115, -0.0485,  0.0334,  0.0775,  0.0429,\n",
       "          -0.0893,  0.0190],\n",
       "         [ 0.1058, -0.0630, -0.0127, -0.0059,  0.0355,  0.1263, -0.0435,\n",
       "           0.0751,  0.0075, -0.0506,  0.1130,  0.0738, -0.0629,  0.0569,\n",
       "           0.0976,  0.0525,  0.0768,  0.0945,  0.0763,  0.0095,  0.0797,\n",
       "          -0.0014, -0.0161,  0.0880,  0.0152, -0.0291,  0.0453, -0.0879,\n",
       "          -0.0776,  0.0169,  0.0822,  0.1098,  0.0441, -0.0887,  0.1000,\n",
       "           0.0794, -0.0809,  0.0562, -0.0713,  0.0211,  0.0465,  0.0395,\n",
       "           0.0191, -0.0406, -0.0139,  0.0332, -0.0279, -0.0064,  0.0773,\n",
       "           0.0933, -0.0392,  0.1218,  0.0244,  0.1010,  0.0421,  0.0170,\n",
       "           0.0091,  0.0556,  0.1076,  0.0085, -0.0756,  0.0561, -0.0686,\n",
       "           0.0165,  0.1117,  0.1233,  0.1231,  0.0034, -0.0314, -0.0397,\n",
       "           0.0033,  0.0384,  0.1144,  0.0460, -0.0397, -0.0575, -0.0637,\n",
       "          -0.0547,  0.1015,  0.1023,  0.0580, -0.0014, -0.0671,  0.0772,\n",
       "           0.0162, -0.0572,  0.1252, -0.0179,  0.0562,  0.0727,  0.0838,\n",
       "           0.0687,  0.0766,  0.0927,  0.0944, -0.0689, -0.0642,  0.0765,\n",
       "           0.0117,  0.0909],\n",
       "         [-0.0105,  0.0924, -0.0357,  0.0556,  0.1010,  0.0221,  0.0317,\n",
       "           0.0923,  0.0736,  0.0658,  0.0147, -0.0651, -0.0105,  0.1315,\n",
       "          -0.0345, -0.0575,  0.0929, -0.0551,  0.0067,  0.0307,  0.0707,\n",
       "          -0.0229, -0.0374,  0.0246, -0.0591,  0.0288, -0.0483, -0.0365,\n",
       "          -0.0280, -0.0404, -0.0035,  0.0319,  0.0561,  0.0265,  0.0121,\n",
       "           0.1024,  0.0132,  0.0386,  0.0445,  0.1195,  0.0309,  0.0035,\n",
       "          -0.0299, -0.0890, -0.0792,  0.0580, -0.0308, -0.0799,  0.0120,\n",
       "          -0.0063, -0.0132,  0.1005,  0.0477, -0.0415, -0.0434, -0.0441,\n",
       "           0.0654,  0.0294,  0.0809, -0.0612,  0.0113,  0.0675,  0.0098,\n",
       "           0.0476,  0.0114, -0.0535, -0.0065, -0.0617,  0.0152,  0.0013,\n",
       "           0.1064,  0.0740,  0.0747,  0.0543,  0.0799,  0.0010,  0.0470,\n",
       "           0.0286,  0.0142,  0.0924,  0.0808,  0.0917,  0.0003, -0.0604,\n",
       "          -0.0278, -0.0185,  0.0515, -0.0520, -0.0641,  0.1289,  0.0392,\n",
       "           0.0186, -0.0199,  0.0424,  0.0386,  0.0177,  0.0268,  0.0878,\n",
       "          -0.0827, -0.0125],\n",
       "         [-0.0776, -0.0897,  0.0262, -0.0066, -0.0156,  0.0873,  0.0968,\n",
       "          -0.0213,  0.1199,  0.0402, -0.0540, -0.0036,  0.0875,  0.0073,\n",
       "          -0.0475,  0.0786, -0.0286, -0.0723,  0.1058, -0.0335,  0.0690,\n",
       "           0.0863,  0.0035,  0.0094, -0.0586,  0.0636,  0.0347,  0.0702,\n",
       "          -0.0334,  0.0090,  0.0723,  0.0123, -0.0464,  0.0406, -0.0756,\n",
       "           0.0924,  0.0194, -0.0137,  0.0885, -0.0067,  0.0098,  0.0451,\n",
       "          -0.0585,  0.0674, -0.0051, -0.0216, -0.0200,  0.1056, -0.0581,\n",
       "           0.0460, -0.0831, -0.0568, -0.0033, -0.0415, -0.0374,  0.0576,\n",
       "          -0.0409,  0.1120,  0.0706, -0.0857, -0.0753,  0.0841, -0.0368,\n",
       "           0.0216,  0.0716, -0.0049,  0.1247, -0.0901, -0.0428,  0.0964,\n",
       "           0.0257,  0.0672,  0.0853,  0.0764,  0.0594,  0.0319,  0.0979,\n",
       "          -0.0896,  0.0300,  0.0140,  0.0600, -0.0570,  0.0929, -0.0665,\n",
       "           0.1122,  0.0731,  0.0169, -0.0752,  0.0479, -0.0242,  0.0294,\n",
       "           0.0816,  0.0286,  0.0711,  0.0004, -0.0275, -0.0045, -0.0499,\n",
       "           0.0957,  0.0849]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([ 0.1023,  0.0919,  0.0986,  0.0967,  0.1132,  0.1019], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0160, -0.0731,  0.0302,  0.1054,  0.0962, -0.0633, -0.0108,\n",
       "           0.0902,  0.0077,  0.1145,  0.0240,  0.0644,  0.0524,  0.1272,\n",
       "           0.0538, -0.0518,  0.1071,  0.0028,  0.0878, -0.0278,  0.0522,\n",
       "          -0.0327,  0.0040, -0.0138, -0.0549,  0.1288, -0.0436,  0.0022,\n",
       "          -0.0920,  0.0004,  0.0059,  0.0126,  0.0357, -0.0759, -0.0873,\n",
       "           0.0253,  0.0001,  0.0558,  0.0163,  0.0642,  0.0264,  0.0835,\n",
       "           0.0921, -0.0769, -0.0638, -0.0080,  0.0615,  0.0536,  0.0899,\n",
       "           0.0483,  0.0501,  0.0868, -0.0415,  0.0398, -0.0150,  0.0141,\n",
       "           0.0224, -0.0563,  0.0293,  0.0489,  0.0300, -0.0491, -0.0706,\n",
       "           0.0512,  0.0608,  0.0755,  0.1290, -0.0734,  0.0736,  0.0989,\n",
       "          -0.0774,  0.1127,  0.1186,  0.0188, -0.0057, -0.0425, -0.0305,\n",
       "           0.0831,  0.0119, -0.0369, -0.0043, -0.0283, -0.0058, -0.0657,\n",
       "           0.0437,  0.0581,  0.0831,  0.0361,  0.0061,  0.0943, -0.0900,\n",
       "          -0.0304,  0.0085, -0.0228,  0.0084,  0.0958,  0.0142, -0.0320,\n",
       "          -0.0304, -0.0817],\n",
       "         [ 0.0215,  0.0459, -0.0128,  0.0121,  0.1027,  0.1129,  0.0628,\n",
       "           0.0755,  0.1158, -0.0600,  0.1004,  0.1003,  0.0157,  0.0554,\n",
       "          -0.0565, -0.0559,  0.0836,  0.0111,  0.0089,  0.0713,  0.0677,\n",
       "           0.0011, -0.0854,  0.0088,  0.1107, -0.0065, -0.0632,  0.0329,\n",
       "          -0.0482,  0.0076,  0.0339, -0.0163,  0.0357,  0.0226,  0.0709,\n",
       "          -0.0314,  0.0690,  0.0209,  0.1220,  0.0961, -0.0040,  0.1073,\n",
       "          -0.0762,  0.0518,  0.0690, -0.0164,  0.0922, -0.0808,  0.0585,\n",
       "          -0.0409,  0.0988,  0.0259,  0.0015,  0.0340,  0.0994, -0.0580,\n",
       "          -0.0820,  0.0162,  0.0397, -0.0009,  0.0454, -0.0539, -0.0311,\n",
       "           0.0400, -0.0606,  0.0605,  0.1127, -0.0085,  0.0813,  0.0655,\n",
       "          -0.0511,  0.0218, -0.0558,  0.0319,  0.1164, -0.0544,  0.0272,\n",
       "           0.0746, -0.0329,  0.1089,  0.0656, -0.0397,  0.1195,  0.0000,\n",
       "           0.1167, -0.0475, -0.0059,  0.0432, -0.0149, -0.0112,  0.0931,\n",
       "           0.0643, -0.0517, -0.0419, -0.0134,  0.0044,  0.0842, -0.0389,\n",
       "           0.0722,  0.0075],\n",
       "         [ 0.0070,  0.1002,  0.0449,  0.0748, -0.0610, -0.0572,  0.0313,\n",
       "           0.0612,  0.1003, -0.0463,  0.0429, -0.0698, -0.0549,  0.0020,\n",
       "          -0.0514, -0.0186,  0.0420,  0.0666,  0.0185,  0.1132,  0.0519,\n",
       "           0.0373,  0.0088, -0.0334,  0.1124,  0.1052,  0.0675,  0.1022,\n",
       "           0.0662,  0.0404,  0.0706,  0.1142, -0.0268, -0.0238,  0.0922,\n",
       "           0.0558,  0.0766,  0.0467,  0.0805,  0.0955, -0.0922,  0.0737,\n",
       "          -0.0129, -0.0043, -0.0605,  0.0653, -0.0799,  0.0966, -0.0234,\n",
       "           0.0397, -0.0035, -0.0718,  0.0719, -0.0753, -0.0297,  0.0848,\n",
       "           0.0619,  0.1056, -0.0419,  0.0749, -0.0875, -0.0035,  0.1058,\n",
       "           0.0499, -0.0038, -0.0629,  0.0395, -0.0642, -0.0881,  0.0106,\n",
       "           0.0734, -0.0135,  0.0533,  0.1141,  0.0913, -0.0908, -0.0615,\n",
       "           0.0739,  0.0417,  0.0432,  0.0770, -0.0855,  0.1172,  0.0408,\n",
       "           0.0735,  0.0088,  0.0521,  0.0474,  0.0397,  0.1143,  0.0464,\n",
       "           0.0330,  0.0634,  0.0236, -0.0467, -0.0897,  0.1133,  0.1267,\n",
       "          -0.0361,  0.0059],\n",
       "         [-0.0314, -0.0360, -0.0373,  0.0745, -0.0335, -0.0043,  0.0417,\n",
       "           0.0085, -0.0265,  0.1052, -0.0073, -0.0673, -0.0729,  0.0408,\n",
       "           0.0944,  0.0096,  0.0480,  0.0806,  0.0744,  0.0140,  0.0497,\n",
       "          -0.0173,  0.0740, -0.0117,  0.0570, -0.0089,  0.0285,  0.0747,\n",
       "          -0.0428, -0.0275,  0.1140,  0.1219,  0.0395,  0.0327,  0.0466,\n",
       "          -0.0304,  0.0460, -0.0220,  0.1079, -0.0136,  0.0999,  0.0142,\n",
       "           0.0132, -0.0813, -0.0488,  0.0997,  0.0594,  0.0754, -0.0350,\n",
       "          -0.0263,  0.0156,  0.0569,  0.0952,  0.0537, -0.0114,  0.0544,\n",
       "          -0.0711, -0.0197,  0.0763, -0.0374,  0.0580,  0.0525,  0.0567,\n",
       "           0.1109,  0.0819, -0.0564,  0.0530, -0.0014, -0.0275,  0.0082,\n",
       "           0.0683, -0.0494,  0.0546,  0.1071,  0.0441,  0.1054, -0.0539,\n",
       "          -0.0695,  0.0452,  0.0570,  0.0574, -0.0353,  0.1226,  0.0869,\n",
       "          -0.0363,  0.0423, -0.0002, -0.0768,  0.0305,  0.0723, -0.0113,\n",
       "           0.1098,  0.1239,  0.0223,  0.0975,  0.0426,  0.0949, -0.0636,\n",
       "          -0.0071,  0.0369],\n",
       "         [-0.0515,  0.0358,  0.0178, -0.0874,  0.1231, -0.0288,  0.0323,\n",
       "           0.0298, -0.0330,  0.0037,  0.0966, -0.0539, -0.0732,  0.0605,\n",
       "           0.0755,  0.0292,  0.0296, -0.0880,  0.1064, -0.0056, -0.0584,\n",
       "          -0.0801,  0.0965, -0.0704, -0.0480,  0.0754, -0.0820,  0.0455,\n",
       "          -0.0361, -0.0007,  0.1149,  0.1117, -0.0597,  0.0719, -0.0871,\n",
       "           0.0819, -0.0473, -0.0657,  0.0170, -0.0019,  0.0752,  0.0361,\n",
       "          -0.0509, -0.0672, -0.0655, -0.0117,  0.0421, -0.0660,  0.0508,\n",
       "          -0.0702,  0.0669,  0.0185,  0.0273, -0.0825,  0.0538,  0.0762,\n",
       "          -0.0353,  0.0241,  0.0496,  0.0847, -0.0301, -0.0461,  0.1056,\n",
       "           0.0040,  0.0439, -0.0194,  0.0986,  0.0470,  0.1036,  0.0014,\n",
       "          -0.0752, -0.0069, -0.0039, -0.0310, -0.0273,  0.0367,  0.0045,\n",
       "           0.0374,  0.0016,  0.0854,  0.0476,  0.0752,  0.0561,  0.0809,\n",
       "           0.1142,  0.0039, -0.0387,  0.0382,  0.0589,  0.0908, -0.0314,\n",
       "           0.1182, -0.0174,  0.0023,  0.0944, -0.0435,  0.0877, -0.0090,\n",
       "           0.0166, -0.0805],\n",
       "         [-0.0646,  0.0841, -0.0338, -0.0042,  0.0450, -0.0240,  0.1123,\n",
       "           0.0868,  0.0664,  0.1099, -0.0601, -0.0688, -0.0322,  0.0542,\n",
       "          -0.0250,  0.0892,  0.0447, -0.0548,  0.0805,  0.0489, -0.0576,\n",
       "          -0.0443,  0.0446,  0.0490,  0.0070, -0.0174,  0.0376, -0.0699,\n",
       "           0.0228,  0.0002,  0.0041,  0.0282,  0.1200,  0.0385,  0.1055,\n",
       "           0.0582, -0.0192,  0.1218, -0.0051,  0.0534,  0.0558, -0.0258,\n",
       "           0.0028,  0.0765, -0.0911, -0.0674,  0.0022,  0.0227,  0.0942,\n",
       "          -0.0426,  0.0417,  0.0418, -0.0045, -0.0068,  0.0350,  0.0421,\n",
       "          -0.0054,  0.0683, -0.0171,  0.0274,  0.0751,  0.0951, -0.0386,\n",
       "           0.1144,  0.0391,  0.1068, -0.0632,  0.0139, -0.0149,  0.1047,\n",
       "           0.0264, -0.0213,  0.0738,  0.0364,  0.1188,  0.0495,  0.0591,\n",
       "          -0.0495, -0.0637, -0.0205,  0.0449, -0.0589,  0.0589, -0.0345,\n",
       "           0.0908,  0.0896,  0.1265,  0.0271,  0.1221, -0.0483,  0.0600,\n",
       "           0.0792,  0.0211, -0.0370,  0.0794, -0.0193, -0.0424,  0.1123,\n",
       "           0.0552,  0.0857]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([ 0.1152,  0.1073,  0.1052,  0.0932,  0.0897,  0.0972], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[-0.0037,  0.0495,  0.0120,  0.0438,  0.0485,  0.1069,  0.0471,\n",
       "          -0.0254, -0.0604,  0.0822,  0.0973, -0.0595, -0.0159,  0.1212,\n",
       "           0.0760,  0.0287, -0.0648, -0.0815, -0.0605, -0.0406, -0.0168,\n",
       "           0.0541,  0.0900, -0.0068,  0.1166,  0.0544, -0.0543, -0.0861,\n",
       "          -0.0525,  0.0059, -0.0118, -0.0314,  0.0562, -0.0236, -0.0648,\n",
       "          -0.0483,  0.0714,  0.0421, -0.0243,  0.1199,  0.0244,  0.0921,\n",
       "          -0.0898,  0.0736, -0.0840,  0.0109,  0.0808, -0.0643,  0.0463,\n",
       "           0.0851,  0.0722, -0.0072,  0.0128,  0.0178, -0.0279, -0.0708,\n",
       "           0.0807,  0.0081,  0.0387,  0.0505, -0.0808, -0.0574, -0.0617,\n",
       "          -0.0249,  0.0080,  0.0965,  0.0331, -0.0585, -0.0042,  0.0384,\n",
       "          -0.0045,  0.0494, -0.0689,  0.0192,  0.0583, -0.0796,  0.1072,\n",
       "          -0.0630,  0.0380, -0.0406,  0.0899,  0.0605,  0.0460,  0.0733,\n",
       "           0.0083,  0.1167,  0.0419,  0.0170,  0.0478,  0.1240,  0.0378,\n",
       "           0.0586, -0.0634, -0.0286,  0.0323, -0.0176, -0.0605,  0.1162,\n",
       "           0.0323, -0.0310],\n",
       "         [-0.0048, -0.0618,  0.0295,  0.0050,  0.1172, -0.0242, -0.0080,\n",
       "           0.1010, -0.0104,  0.1013,  0.0225, -0.0281, -0.0211,  0.0471,\n",
       "          -0.0381, -0.0626, -0.0710,  0.0757,  0.0480,  0.0100, -0.0020,\n",
       "          -0.0581,  0.0067, -0.0603,  0.0846, -0.0057,  0.0023,  0.0884,\n",
       "          -0.0029,  0.0454,  0.0427,  0.0497,  0.0001,  0.0301, -0.0907,\n",
       "           0.0897, -0.0050,  0.1248, -0.0239, -0.0315, -0.0639,  0.1199,\n",
       "           0.0824,  0.0550,  0.0725, -0.0199,  0.0493, -0.0012, -0.0239,\n",
       "           0.0829,  0.0798, -0.0513,  0.0175, -0.0607,  0.0397,  0.0927,\n",
       "          -0.0161,  0.0449,  0.0453, -0.0436,  0.0538,  0.0093, -0.0277,\n",
       "           0.1119,  0.0029,  0.0233,  0.1067, -0.0079,  0.0975,  0.0248,\n",
       "           0.1064, -0.0164, -0.0648,  0.0830, -0.0565, -0.0381,  0.0547,\n",
       "           0.0905,  0.0203,  0.1015,  0.1286, -0.0382,  0.0555,  0.0253,\n",
       "          -0.0637,  0.1102,  0.0168, -0.0704, -0.0341, -0.0202,  0.0908,\n",
       "           0.0394,  0.1056, -0.0524,  0.0864,  0.0917, -0.0287, -0.0262,\n",
       "           0.0937, -0.0697],\n",
       "         [-0.0736, -0.0418,  0.0503, -0.0788,  0.1050,  0.1134,  0.1189,\n",
       "          -0.0420,  0.0249,  0.0466, -0.0123,  0.1040, -0.0519, -0.0058,\n",
       "           0.0787, -0.0320,  0.0041, -0.0638, -0.0372,  0.0498,  0.0404,\n",
       "          -0.0229, -0.0736, -0.0605,  0.1134,  0.0508,  0.0874, -0.0020,\n",
       "           0.0870,  0.0606,  0.0441,  0.0043, -0.0489, -0.0793,  0.0202,\n",
       "          -0.0541, -0.0635,  0.0359,  0.1007,  0.0203, -0.0084, -0.0579,\n",
       "          -0.0887, -0.0792, -0.0855, -0.0602, -0.0783, -0.0123,  0.0670,\n",
       "           0.0741, -0.0154,  0.0670,  0.0636, -0.0759,  0.0745, -0.0928,\n",
       "          -0.0454,  0.0159,  0.0759, -0.0605,  0.0296,  0.0028, -0.0132,\n",
       "          -0.0067,  0.0255, -0.0518, -0.0389,  0.0969, -0.0233, -0.0083,\n",
       "           0.0089,  0.1137,  0.0167, -0.0717,  0.1264,  0.0462, -0.0375,\n",
       "           0.0657, -0.0068,  0.1048,  0.0453,  0.1000, -0.0311, -0.0853,\n",
       "           0.0432,  0.0380,  0.0452, -0.0267,  0.0315,  0.1244,  0.0632,\n",
       "           0.0350,  0.0077,  0.0513, -0.0395, -0.0560,  0.0703,  0.0096,\n",
       "          -0.0886, -0.0326],\n",
       "         [ 0.0165, -0.0326,  0.0570,  0.0794,  0.1111, -0.0331,  0.0226,\n",
       "          -0.0650, -0.0345, -0.0063, -0.0398,  0.0793,  0.0383,  0.0042,\n",
       "          -0.0236, -0.0737,  0.0378,  0.0315,  0.0790,  0.0149, -0.0223,\n",
       "          -0.0174,  0.0950,  0.0054,  0.1189, -0.0519, -0.0746, -0.0815,\n",
       "          -0.0810,  0.0397,  0.0680,  0.1269,  0.1188,  0.0817,  0.0556,\n",
       "          -0.0748, -0.0059,  0.0822, -0.0460, -0.0376,  0.0684,  0.0044,\n",
       "           0.0314,  0.0119,  0.0200,  0.0251, -0.0530,  0.0729, -0.0812,\n",
       "           0.0564, -0.0374, -0.0018,  0.0054, -0.0434,  0.1245,  0.0293,\n",
       "          -0.0223,  0.0945,  0.0023,  0.0819,  0.0916,  0.0360, -0.0500,\n",
       "          -0.0021, -0.0210, -0.0343,  0.0046, -0.0630,  0.0330, -0.0076,\n",
       "          -0.0906,  0.0359,  0.0371,  0.0296,  0.0069, -0.0182, -0.0614,\n",
       "           0.0141,  0.0143,  0.1234, -0.0501, -0.0910,  0.0317, -0.0112,\n",
       "           0.0126,  0.0951, -0.0362, -0.0506,  0.0753, -0.0250, -0.0436,\n",
       "           0.0198,  0.0394,  0.1145,  0.1287,  0.0080,  0.0742,  0.0380,\n",
       "           0.0963,  0.0491],\n",
       "         [-0.0292, -0.0694,  0.0041, -0.0866,  0.0535,  0.0054, -0.0501,\n",
       "          -0.0932,  0.0144,  0.1289,  0.1259, -0.0411, -0.0906,  0.0617,\n",
       "          -0.0591,  0.0534, -0.0732, -0.0549,  0.1205,  0.0838, -0.0348,\n",
       "           0.0319, -0.0856,  0.0793, -0.0202,  0.0661,  0.0101, -0.0239,\n",
       "           0.0346,  0.0952,  0.0676,  0.0117,  0.0646, -0.0587,  0.0812,\n",
       "           0.0450, -0.0762, -0.0293,  0.1177,  0.0277, -0.0773,  0.0272,\n",
       "          -0.0246,  0.0477,  0.0979, -0.0812, -0.0318, -0.0539,  0.0351,\n",
       "           0.0291, -0.0734,  0.0417, -0.0204, -0.0751, -0.0223, -0.0866,\n",
       "          -0.0199, -0.0659,  0.0681,  0.0204, -0.0192, -0.0668,  0.0681,\n",
       "           0.0525,  0.1298,  0.1116, -0.0147,  0.0760,  0.0261, -0.0181,\n",
       "          -0.0197,  0.0917,  0.1085, -0.0325,  0.1251,  0.0318,  0.0577,\n",
       "           0.0565,  0.0125,  0.0894, -0.0307, -0.0840,  0.0440, -0.0007,\n",
       "           0.0431,  0.0393, -0.0524, -0.0731, -0.0521, -0.0046,  0.0842,\n",
       "          -0.0432, -0.0230, -0.0373,  0.1201,  0.0325,  0.0002,  0.0713,\n",
       "          -0.0743,  0.0396],\n",
       "         [-0.0198, -0.0334, -0.0193, -0.0242,  0.1006,  0.0197,  0.0891,\n",
       "          -0.0897, -0.0581, -0.0322, -0.0463, -0.0792, -0.0272,  0.0176,\n",
       "           0.0242,  0.0996, -0.0711, -0.0877,  0.0853, -0.0137, -0.0172,\n",
       "           0.0361, -0.0593, -0.0414, -0.0384,  0.0943, -0.0791,  0.0398,\n",
       "           0.0349,  0.0782,  0.0377,  0.0138,  0.0983,  0.0726, -0.0591,\n",
       "           0.0831, -0.0398,  0.0162,  0.0394,  0.0739,  0.0444, -0.0341,\n",
       "          -0.0581,  0.0370, -0.0243, -0.0131, -0.0675,  0.0258,  0.0934,\n",
       "          -0.0883,  0.0222,  0.1272, -0.0464,  0.0442, -0.0738,  0.0338,\n",
       "          -0.0914, -0.0059,  0.0930, -0.0201,  0.0752,  0.0965, -0.0757,\n",
       "           0.0414, -0.0013, -0.0622,  0.0723, -0.0765,  0.0820, -0.0096,\n",
       "          -0.0802,  0.0596,  0.0120,  0.1087,  0.1083,  0.0209,  0.0825,\n",
       "           0.0655,  0.0747,  0.0394,  0.0639, -0.0547,  0.1022,  0.0919,\n",
       "          -0.0015,  0.0036,  0.0999, -0.0104, -0.0287,  0.0226, -0.0024,\n",
       "           0.0917,  0.1147, -0.0112,  0.1090,  0.0559,  0.0620,  0.0609,\n",
       "          -0.0450, -0.0211]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([ 0.1198,  0.1372,  0.1153,  0.1300,  0.1267,  0.1431], device='cuda:0')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Agents[0].parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
