{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "All the logarithms used are base 2. <br>\n",
    "Assumes 2 self-interested agents alternating turns. <br>\n",
    "Baseline (1 for each agent) gets updated after each episode ends (see corpses). <br>\n",
    "Rewards only possible at the end of each game. <br>\n",
    "Uses same (numerical) encoder for both item context and proposal. Reference code uses 3 distinct ones. It also has max_utility = num_types instead of 10 for us.<br>\n",
    "Check how message policy works again; paper seemed to imply that each output of the lstm is a letter. (we take the hidden output and make a probability over letters out of it).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Network\n",
    "import torch\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "# cuda\n",
    "use_cuda = True\n",
    "\n",
    "# Random seeds for testing\n",
    "num_seed = 0\n",
    "torch.manual_seed(num_seed)\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    torch.cuda.manual_seed(num_seed)\n",
    "np.random.seed(num_seed)\n",
    "\n",
    "# Utility functions\n",
    "from utility import truncated_poisson_sampling, create_item_pool, create_agent_utility, rewards_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Game setup\n",
    "num_agents = 2         # Number of agents playing the game\n",
    "num_types = 3          # Number of item types\n",
    "max_item = 5           # Maximum number of each item in a pool\n",
    "max_utility = 5       # Maximum utility value for agents\n",
    "\n",
    "# Turn sampling\n",
    "lam = 7                # Poisson parameter\n",
    "max_N = 10             # Maximum number of turns\n",
    "min_N = 4              # Minimum number of turns\n",
    "\n",
    "# Linguistic channel\n",
    "num_vocab = 10         # Symbol vocabulary size for linguistic channel\n",
    "len_message = 6        # Linguistic message length\n",
    "\n",
    "# Training\n",
    "alpha = 0.001          # learning rate\n",
    "N_ep = 5000              # Number of episodes\n",
    "num_games = 128        # Number of games per episode (batch size)\n",
    "\n",
    "# Appendix\n",
    "lambda1 = 0.05         # Entropy regularizer for pi_term\n",
    "lambda2 = 0.0001        # Entropy regularizer for pi_utt\n",
    "lambda3 = 0.005        # Entropy regularizer for pi_prop\n",
    "smoothing_const = 0.7  # Smoothing constant for the exponential moving average baseline\n",
    "\n",
    "# Miscellaneous\n",
    "ep_time = 100         # Print time every ep_time episodes\n",
    "ep_record = 10        # Record training curve every ep_record episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class combined_policy(nn.Module):\n",
    "    def __init__(self, embedding_dim = 100, batch_size = 128, num_layers = 1, bias = True, batch_first = False, dropout = 0, bidirectional = False):\n",
    "        super(combined_policy, self).__init__()\n",
    "        # Save variables\n",
    "        self.embedding_dim = embedding_dim # Hidden layer dimensions\n",
    "        self.batch_size = batch_size       # Batch size (updated every forward pass)\n",
    "        self.log_p = torch.zeros([batch_size,1])                     # Store policy log likelihood for REINFORCE\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            self.log_p = self.log_p.cuda()\n",
    "        \n",
    "        # Encoding -------------------------------------------------------------\n",
    "        \n",
    "        # Numerical encoder\n",
    "        self.encoder1 = nn.Embedding(max_utility+1, embedding_dim)\n",
    "        # Linguistic encoder\n",
    "        self.encoder2 = nn.Embedding(num_vocab+1, embedding_dim)\n",
    "        \n",
    "        # Item context LSTM\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        # Linguistic LSTM\n",
    "        self.lstm2 = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        # Proposal LSTM\n",
    "        self.lstm3 = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        \n",
    "        # Outputs of the 3 LSTMS get concatenated together\n",
    "        \n",
    "        # Feed-forward\n",
    "        self.ff = nn.Linear(3*embedding_dim, embedding_dim)\n",
    "        \n",
    "        # Output of feed-forward is the input for the policy networks\n",
    "        \n",
    "        # Policy ---------------------------------------------------------------\n",
    "        \n",
    "        # Termination policy\n",
    "        self.policy_term = nn.Linear(embedding_dim, 1)\n",
    "        # Linguistic policy\n",
    "        self.policy_ling = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        self.ff_ling = nn.Linear(embedding_dim, num_vocab)\n",
    "        # Proposal policies\n",
    "        self.policy_prop = nn.ModuleList([nn.Linear(embedding_dim, max_item+1) for i in range(num_types)])\n",
    "        \n",
    "    def forward(self, x, test, batch_size=128):\n",
    "        # Inputs --------------------------------------------------------------------\n",
    "        # x = list of three elements consisting of:\n",
    "        #   1. item context (longtensor of shape batch_size x (2*num_types))\n",
    "        #   2. previous linguistic message (longtensor of shape batch_size x len_message)\n",
    "        #   3. previous proposal (longtensor of shape batch_size x num_types)\n",
    "        # test = whether training or testing (testing selects actions greedily)\n",
    "        # batch_size = batch size\n",
    "        # Outputs -------------------------------------------------------------------\n",
    "        # term = binary variable where 1 indicates proposal accepted => game finished (longtensor of shape batch_size x 1)\n",
    "        # message = crafted linguistic message (longtensor of shape batch_size x len_message)\n",
    "        # prop = crafted proposal (longtensor of shape batch_size x num_types)\n",
    "        # entropy_loss = Number containing the sum of policy entropies (should be total entropy by additivity)\n",
    "        \n",
    "        # Update batch_size variable (changes throughout training due to sieving (see survivors below))\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Extract inputs ------------------------------------------------------------\n",
    "        \n",
    "        # Item context\n",
    "        x1 = x[0]\n",
    "        # Previous linguistic message\n",
    "        x2 = x[1]\n",
    "        # Previous proposal\n",
    "        x3 = x[2]  \n",
    "\n",
    "        # Encoding ------------------------------------------------------------------\n",
    "\n",
    "        # Initial embedding\n",
    "        x1 = self.encoder1(x1).transpose(0,1)\n",
    "        x2 = self.encoder2(x2).transpose(0,1)\n",
    "        x3 = self.encoder1(x3).transpose(0,1) # Same encoder as item context\n",
    "            \n",
    "        # LSTM for item context\n",
    "        h1 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial hidden\n",
    "        c1 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial cell\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            h1 = h1.cuda()\n",
    "            c1 = c1.cuda()\n",
    "\n",
    "        for i in range(x1.size()[0]):\n",
    "            _, (h1,c1) = self.lstm1(x1[i].view(1,self.batch_size,self.embedding_dim),(h1,c1))\n",
    "        x1_encoded = h1\n",
    "        \n",
    "        # LSTM for linguistic\n",
    "        h2 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial hidden\n",
    "        c2 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial cell\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            h2 = h2.cuda()\n",
    "            c2 = c2.cuda()\n",
    "\n",
    "        for i in range(x2.size()[0]):\n",
    "            _, (h2,c2) = self.lstm2(x2[i].view(1,self.batch_size,self.embedding_dim),(h2,c2))\n",
    "        x2_encoded = h2\n",
    "        \n",
    "        # LSTM for proposal\n",
    "        h3 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial hidden\n",
    "        c3 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial cell\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            h3 = h3.cuda()\n",
    "            c3 = c3.cuda()\n",
    "\n",
    "        for i in range(x3.size()[0]):\n",
    "            _, (h3,c3) = self.lstm2(x3[i].view(1,self.batch_size,self.embedding_dim),(h3,c3))\n",
    "        x3_encoded = h3\n",
    "\n",
    "        # Concatenate side-by-side\n",
    "        h = torch.cat([x1_encoded,x2_encoded,x3_encoded],2)\n",
    "\n",
    "        # Feedforward\n",
    "        h = self.ff(h)\n",
    "        h = F.relu(h) # Hidden layer input for policy networks\n",
    "        \n",
    "        # Policy ------------------------------------------------------------------\n",
    "\n",
    "        # Termination -----------------------------------------------\n",
    "        p_term = F.sigmoid(self.policy_term(h)).view(self.batch_size,1).float()\n",
    "\n",
    "        # Entropy\n",
    "        one_tensor = torch.ones(self.batch_size,1)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            one_tensor = one_tensor.cuda()\n",
    "        entropy_term = -(p_term * (p_term+1e-8).log()) - (one_tensor-p_term) * (one_tensor-p_term+1e-8).log()\n",
    "        entropy_term = torch.sum(entropy_term)\n",
    "        \n",
    "        if test:\n",
    "            # Greedy\n",
    "            term = torch.round(p_term).long()\n",
    "        else:\n",
    "            # Sample\n",
    "            term = torch.bernoulli(p_term).long()\n",
    "            \n",
    "        # log p for REINFORCE\n",
    "        log_p_term = torch.zeros(self.batch_size,1)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            log_p_term = log_p_term.cuda()\n",
    "\n",
    "        log_p_term = ((term.float() * p_term) + (one_tensor-term.float()) * (one_tensor-p_term)+1e-8).log()\n",
    "        \n",
    "        # Linguistic construction ----------------------------------\n",
    "        h_ling = h.clone() # Initial hidden state\n",
    "        c_ling = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial cell state\n",
    "        letter = torch.zeros(self.batch_size,1).long() # Initial letter (dummy)\n",
    "        entropy_letter = torch.zeros([self.batch_size,len_message])\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            c_ling = c_ling.cuda()\n",
    "            letter = letter.cuda()\n",
    "            entropy_letter = entropy_letter.cuda()\n",
    "        \n",
    "        # log p for REINFORCE \n",
    "        log_p_letter = torch.zeros([self.batch_size,1])\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            log_p_letter = log_p_letter.cuda()\n",
    "\n",
    "        message = torch.zeros(self.batch_size,len_message) # Message\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            message = message.cuda()\n",
    "        for i in range(len_message):\n",
    "            embedded_letter = self.encoder2(letter)\n",
    "\n",
    "            _, (h_ling,c_ling) = self.policy_ling(embedded_letter.view(1,self.batch_size,self.embedding_dim),(h_ling,c_ling))\n",
    "            logit = self.ff_ling(h_ling)\n",
    "            p_letter = F.softmax(logit,dim=2).view(self.batch_size,num_vocab).float()\n",
    "\n",
    "            entropy_letter[:,i] = -torch.sum(p_letter*(p_letter+1e-8).log(),1)\n",
    "\n",
    "            if test:\n",
    "                # Greedy\n",
    "                letter = p_letter.argmax(dim=1).view(self.batch_size,1).long()\n",
    "            else:\n",
    "                # Sample\n",
    "                letter = torch.multinomial(p_letter,1).long()\n",
    "                \n",
    "            # Gather the probabilities for the letters we've picked\n",
    "            probs = torch.gather(p_letter, 1, letter)\n",
    "            log_p_letter = log_p_letter + (probs+1e-8).log()\n",
    "                \n",
    "            message[:,i] = letter.squeeze()\n",
    "            \n",
    "        message = message.long()\n",
    "        entropy_letter = torch.sum(entropy_letter)     \n",
    "   \n",
    "        # Proposal ----------------------------------------------\n",
    "        p_prop = []\n",
    "        prop = []\n",
    "        \n",
    "        #prop = torch.zeros([self.batch_size,num_types]).long()\n",
    "        entropy_prop_list = [0,0,0]\n",
    "        \n",
    "        # log p for REINFORCE \n",
    "        log_p_prop = torch.zeros([self.batch_size,1])\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            log_p_prop = log_p_prop.cuda()\n",
    "\n",
    "        for i in range(num_types):\n",
    "            p_prop.append(F.sigmoid(self.policy_prop[i](h)))\n",
    "            \n",
    "            entropy_prop_list[i] = -torch.sum(p_prop[i]*p_prop[i].log())\n",
    "            \n",
    "            p_prop[i] = p_prop[i].view(self.batch_size,max_item+1)\n",
    "\n",
    "            if test:\n",
    "                # Greedy\n",
    "                #prop[:,i] = p_prop[i].argmax(dim=1)\n",
    "                prop.append(p_prop[i].argmax(dim=1))\n",
    "            else:\n",
    "                # Sample\n",
    "                #prop[:,i] = torch.multinomial(p_prop,1)\n",
    "                prop.append(torch.multinomial(p_prop,1))\n",
    "                \n",
    "            # Gather the probabilities for the letters we've picked\n",
    "            probs = torch.gather(p_prop[i], 1, prop[i].view(self.batch_size,1))\n",
    "            log_p_prop = log_p_prop + probs.log()\n",
    "              \n",
    "        prop = torch.stack(prop).transpose(0,1)\n",
    "        entropy_prop = sum(entropy_prop_list) # Entropy for exploration\n",
    "\n",
    "        # Combine -----------------------------------------------------------------\n",
    "        entropy_loss = torch.sum(lambda1*entropy_term + lambda3*entropy_prop + lambda2*entropy_letter)\n",
    "        self.log_p = log_p_term + log_p_letter + log_p_prop\n",
    "\n",
    "        return (term,message,prop, entropy_loss, log_p_term,log_p_letter,log_p_prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = combined_policy()\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    net = net.cuda()\n",
    "\n",
    "# Dummy inputs\n",
    "x = torch.randint(0,max_item,[128,6]).long()\n",
    "y = torch.randint(0,num_vocab,[128,6]).long()\n",
    "z = torch.randint(0,max_item,[128,3]).long()\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    z = z.cuda()\n",
    "\n",
    "blah = net([x,y,z],True)\n",
    "\n",
    "# Initialize agents\n",
    "Agents = []\n",
    "for i in range(num_agents):\n",
    "    Agents.append(combined_policy())\n",
    "    if torch.cuda.is_available() and use_cuda:\n",
    "        Agents[i] = Agents[i].cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start ----------------\n",
      "Runtime for episodes 0-100: 7.349941253662109s\n",
      "Runtime for episodes 100-200: 8.101256608963013s\n",
      "Runtime for episodes 200-300: 7.878236770629883s\n",
      "Runtime for episodes 300-400: 6.965450286865234s\n",
      "Runtime for episodes 400-500: 9.10617208480835s\n",
      "Runtime for episodes 500-600: 7.079211473464966s\n",
      "Runtime for episodes 600-700: 8.609218120574951s\n",
      "Runtime for episodes 700-800: 8.691840171813965s\n",
      "Runtime for episodes 800-900: 8.73936152458191s\n",
      "Runtime for episodes 900-1000: 8.977110624313354s\n",
      "Runtime for episodes 1000-1100: 9.347987651824951s\n",
      "Runtime for episodes 1100-1200: 9.300126075744629s\n",
      "Runtime for episodes 1200-1300: 8.119391441345215s\n",
      "Runtime for episodes 1300-1400: 9.660314798355103s\n",
      "Runtime for episodes 1400-1500: 12.29362154006958s\n",
      "Runtime for episodes 1500-1600: 13.776344299316406s\n",
      "Runtime for episodes 1600-1700: 12.981067895889282s\n",
      "Runtime for episodes 1700-1800: 12.702199220657349s\n",
      "Runtime for episodes 1800-1900: 11.084558010101318s\n",
      "Runtime for episodes 1900-2000: 13.18879222869873s\n",
      "Runtime for episodes 2000-2100: 15.353572130203247s\n",
      "Runtime for episodes 2100-2200: 16.343653917312622s\n",
      "Runtime for episodes 2200-2300: 14.59296202659607s\n",
      "Runtime for episodes 2300-2400: 13.712409496307373s\n",
      "Runtime for episodes 2400-2500: 13.787512302398682s\n",
      "Runtime for episodes 2500-2600: 15.691325426101685s\n",
      "Runtime for episodes 2600-2700: 14.466434240341187s\n",
      "Runtime for episodes 2700-2800: 14.749808311462402s\n",
      "Runtime for episodes 2800-2900: 15.281508445739746s\n",
      "Runtime for episodes 2900-3000: 14.815212726593018s\n",
      "Runtime for episodes 3000-3100: 13.831258058547974s\n",
      "Runtime for episodes 3100-3200: 13.240236759185791s\n",
      "Runtime for episodes 3200-3300: 15.09666633605957s\n",
      "Runtime for episodes 3300-3400: 15.360960483551025s\n",
      "Runtime for episodes 3400-3500: 15.725964069366455s\n",
      "Runtime for episodes 3500-3600: 12.947853565216064s\n",
      "Runtime for episodes 3600-3700: 16.00943398475647s\n",
      "Runtime for episodes 3700-3800: 13.516492366790771s\n",
      "Runtime for episodes 3800-3900: 14.565854549407959s\n",
      "Runtime for episodes 3900-4000: 14.268914461135864s\n",
      "Runtime for episodes 4000-4100: 10.50206971168518s\n",
      "Runtime for episodes 4100-4200: 13.619489192962646s\n",
      "Runtime for episodes 4200-4300: 15.580429792404175s\n",
      "Runtime for episodes 4300-4400: 14.172582149505615s\n",
      "Runtime for episodes 4400-4500: 14.94058108329773s\n",
      "Runtime for episodes 4500-4600: 15.819945812225342s\n",
      "Runtime for episodes 4600-4700: 12.918389081954956s\n",
      "Runtime for episodes 4700-4800: 12.344123840332031s\n",
      "Runtime for episodes 4800-4900: 11.102480173110962s\n",
      "End ------------------\n",
      "Total runtime: 620.1757080554962s\n"
     ]
    }
   ],
   "source": [
    "baselines = [0 for _ in range(num_agents)] # Baselines for reward calculation\n",
    "\n",
    "# Initialize optimizers for learning\n",
    "optimizers = []\n",
    "for i in range(num_agents):\n",
    "    optimizers.append(optim.Adam(Agents[i].parameters()))\n",
    "    \n",
    "# Train rewards\n",
    "r_list = []\n",
    "for i in range(num_agents):\n",
    "    r_list.append([])\n",
    "\n",
    "print('Start ----------------')\n",
    "time_start = time.time()\n",
    "time_p1 = time.time()\n",
    "# Loop over episodes\n",
    "for i_ep in range(N_ep):\n",
    "    # Setting up games -----------------------------------------------------------------------\n",
    "    # Game setup\n",
    "    # Truncated Poisson sampling for number of turns in each game\n",
    "    N = truncated_poisson_sampling(lam, min_N, max_N, num_games)\n",
    "    # Item pools for each game\n",
    "    pool = create_item_pool(num_types, max_item, num_games)\n",
    "    if torch.cuda.is_available() and use_cuda:\n",
    "        N = N.cuda()\n",
    "        pool = pool.cuda()\n",
    "    # Item contexts for each game\n",
    "    item_contexts = [] # Each agent has different utilities (but same pool)\n",
    "    for i in range(num_agents):\n",
    "        utility = create_agent_utility(num_types, max_utility, num_games)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            utility = utility.cuda()\n",
    "        item_contexts.append(torch.cat([pool, utility],1))\n",
    "    \n",
    "    # Initializations\n",
    "    survivors = torch.ones(num_games).nonzero()               # Keeps track of ongoing games; everyone alive initially\n",
    "    num_alive = len(survivors)                                # Actual batch size for each turn (initially num_games)\n",
    "    prev_messages = torch.zeros(num_games, len_message).long() # Previous linguistic message for each game\n",
    "    prev_proposals = torch.zeros(num_games, num_types).long()  # Previous proposal for each game\n",
    "    if torch.cuda.is_available() and use_cuda:\n",
    "        survivors = survivors.cuda()\n",
    "        prev_messages = prev_messages.cuda()\n",
    "        prev_proposals = prev_proposals.cuda()\n",
    "    \n",
    "    rewards = [torch.zeros(num_games), torch.zeros(num_games)]       # Rewards for each game for each agent\n",
    "    # Keep track of sum of all rewards (from all games in a batch) for baseline updates (see corpses below)\n",
    "    reward_sums = []\n",
    "    for i in range(num_agents):\n",
    "        reward_sums.append(torch.zeros(1)) # Just a number\n",
    "        Agents[i].log_p = torch.zeros([num_games,1])\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            rewards[i] = rewards[i].cuda()\n",
    "            reward_sums[i] = reward_sums[i].cuda()\n",
    "            Agents[i].log_p = Agents[i].log_p.cuda()\n",
    "\n",
    "    # Play the games -------------------------------------------------------------------------\n",
    "    for i_turn in range(max_N): # Loop through maximum possible number of turns for all games\n",
    "        \n",
    "        # Losses for each agent\n",
    "        reward_losses = [torch.zeros([],requires_grad=True), torch.zeros([],requires_grad=True)]  \n",
    "        entropy_losses = [torch.zeros([],requires_grad=True), torch.zeros([],requires_grad=True)] # Exploration\n",
    "        for j in range(num_agents):\n",
    "            Agents[j].log_p = torch.zeros([num_alive,1])\n",
    "            \n",
    "            if torch.cuda.is_available() and use_cuda:\n",
    "                reward_losses[j] = reward_losses[j].cuda()\n",
    "                entropy_losses[j] = entropy_losses[j].cuda()\n",
    "                Agents[j].log_p = Agents[j].log_p.cuda()\n",
    "        \n",
    "        # Agent IDs\n",
    "        id_1 = i_turn % 2    # Current player\n",
    "        id_2 = int(not id_1) # Other player\n",
    "        \n",
    "        # Remove finished games (batch size decreases)\n",
    "        N = N[survivors].view(num_alive, 1)\n",
    "        pool = pool[survivors].view(num_alive, num_types)\n",
    "        prev_messages = prev_messages[survivors].view(num_alive, len_message)\n",
    "        prev_proposals = prev_proposals[survivors].view(num_alive, num_types)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            N = N.cuda()\n",
    "            pool = pool.cuda()\n",
    "            prev_messages = prev_messages.cuda()\n",
    "            prev_proposals = prev_proposals.cuda()\n",
    "        # Quantities different for each agent\n",
    "        for j in range(num_agents):\n",
    "            item_contexts[j] = item_contexts[j][survivors].view(num_alive,num_types*2)\n",
    "            #rewards[j] = rewards[j][survivors].view(num_alive)\n",
    "            #reward_losses[j] = reward_losses[j][survivors].view(num_alive)\n",
    "            if torch.cuda.is_available() and use_cuda:\n",
    "                item_contexts[j] = item_contexts[j].cuda()\n",
    "        \n",
    "        # Agent currently playing\n",
    "        Agent = Agents[id_1]             \n",
    "        item_context = item_contexts[id_1]\n",
    "        \n",
    "        # Play the game -------------------------------------------------------------\n",
    "        term, prev_messages, proposals, entropy_loss, lt,ll,lp = Agent([item_context, prev_messages, prev_proposals], True, num_alive)\n",
    "        entropy_losses[id_1] = entropy_loss\n",
    "        \n",
    "        # Compute reward loss (assumes 2 agents) ------------------------------------\n",
    "        # Games terminated by the current agent (previous proposal accepted)\n",
    "        \n",
    "        finishers = term.squeeze().nonzero()          # squeeze is for getting rid of extra useless dimension that pops up for some reason\n",
    "        num_finishers = len(finishers)\n",
    "\n",
    "        if len(finishers) != 0:\n",
    "            pool_12 = pool[finishers].view(num_finishers,num_types)\n",
    "            \n",
    "            share_2 = prev_proposals[finishers].view(num_finishers,num_types) # Share of other (previous proposal) \n",
    "            share_1 = pool_12 - share_2 # Share of this agent (remainder)\n",
    "            \n",
    "            # Zero reward if proposal exceeds pool\n",
    "            invalid_batches = torch.sum(share_2>pool_12,1)>0\n",
    "            share_2[invalid_batches] = 0\n",
    "            share_1[invalid_batches] = 0\n",
    "            \n",
    "            utility_1 = item_contexts[id_1][:,num_types:] # Recall that item context is a concatenation of pool and utility\n",
    "            utility_1 = utility_1[finishers].view(num_finishers,num_types)\n",
    "            utility_2 = item_contexts[id_2][:,num_types:]\n",
    "            utility_2 = utility_2[finishers].view(num_finishers,num_types)\n",
    "\n",
    "            log_p_1 = Agents[id_1].log_p[finishers].view(num_finishers,1)\n",
    "            log_p_2 = Agents[id_2].log_p[finishers].view(num_finishers,1)\n",
    "\n",
    "            # Calculate reward and reward losses\n",
    "            r1, rl1 = rewards_func(share_1, utility_1, pool_12, log_p_1, baselines[id_1])\n",
    "            r2, rl2 = rewards_func(share_2, utility_2, pool_12, log_p_2, baselines[id_2])\n",
    "         \n",
    "            #for i in range(num_finishers):\n",
    "                #print(r1[i], r2[i])\n",
    "                #if r1[i]==0:\n",
    "                #    print(share_2[i])\n",
    "                #    print(share_1[i])\n",
    "                #    print(utility_1[i])\n",
    "                #    print(utility_2[i])\n",
    "                #    print(pool_12[i])\n",
    "                #print(lt[i])\n",
    "                #print(lp[i])\n",
    "                #print(ll[i])\n",
    "                #print(baselines)\n",
    "            \n",
    "            # Add rewards and reward losses\n",
    "            rewards[id_1] = r1.squeeze()\n",
    "            rewards[id_2] = r2.squeeze()\n",
    "            reward_losses[id_1] = rl1\n",
    "            reward_losses[id_2] = rl2\n",
    "            reward_sums[id_1] = reward_sums[id_1] + rewards[id_1].sum()\n",
    "            reward_sums[id_2] = reward_sums[id_2] + rewards[id_2].sum()\n",
    "\n",
    "        prev_proposals = proposals # Don't need previous proposals anymore so update it\n",
    "        \n",
    "        # Gradient descent -----------------------------------------------------------\n",
    "        \n",
    "        for i in range(num_agents):\n",
    "            # optimize\n",
    "            loss = reward_losses[i] - entropy_losses[i]\n",
    "            \n",
    "            optimizers[i].zero_grad()\n",
    "            loss.backward()\n",
    "            optimizers[i].step()\n",
    "        \n",
    "        # Wrapping up the end of turn ------------------------------------------------\n",
    "        # Remove finished games\n",
    "        # In term and term_N, element = 1 means die\n",
    "        term_N = (N <= (i_turn+1)).view(num_alive,1).long() # Last turn reached; i_turn + 1 since i_turn starts counting from 0\n",
    "        # In survivors, element = 1 means live\n",
    "        survivors = (term+term_N) == 0\n",
    "\n",
    "        # Check if everyone's dead\n",
    "        if survivors.sum() == 0: # If all games over, break episode\n",
    "            # Baseline updates\n",
    "            for i in range(num_agents):\n",
    "                # Update with batch-averaged rewards\n",
    "                baselines[i] = smoothing_const * baselines[i] + (1-smoothing_const)*reward_sums[i]/num_games\n",
    "            break;\n",
    "            \n",
    "        # Reshape\n",
    "        survivors = ((term+term_N) == 0).nonzero()[:,0].view(-1,1)\n",
    "        num_alive = len(survivors) # Number of survivors\n",
    "\n",
    "        #print('i_turn = ' + str(i_turn))\n",
    "        \n",
    "    #print('i_ep = ' + str(i_ep))\n",
    "    if (i_ep % ep_time == 0) and (i_ep != 0):\n",
    "        time_p2 = time.time()\n",
    "        print('Runtime for episodes ' + str(i_ep-ep_time) + '-' + str(i_ep) + ': ' + str(time_p2 - time_p1) + 's')\n",
    "        time_p1 = time_p2\n",
    "        \n",
    "    if (i_ep % ep_record == 0):\n",
    "        for j in range(num_agents):\n",
    "            r_list[j].append(reward_sums[j]/num_games)\n",
    "    \n",
    "    #print('----------------')\n",
    "    \n",
    "print('End ------------------')\n",
    "time_finish = time.time()\n",
    "print('Total runtime: ' + str(time_finish-time_start) + 's')\n",
    "\n",
    "#for i in range(num_agents):\n",
    "#    torch.save(Agents[0].state_dict(),'saved_model_agent_' + str(i) + '.pt')\n",
    "    \n",
    "#Agents[0].load_state_dict(torch.load('saved_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 2.7207, -0.1925, -1.8958,  1.9951, -0.2903, -0.2637, -0.5567,\n",
       "          -0.0103, -1.0963,  0.9641, -1.2105,  0.9033, -1.3388, -1.3014,\n",
       "           0.4377,  1.6048, -0.0768,  1.1014,  0.7351,  0.1221,  0.2684,\n",
       "           0.0953,  0.3607, -0.9299, -0.7488, -2.3050, -0.0182, -0.5840,\n",
       "           0.6187,  0.7136, -1.5628, -0.8639,  1.2477, -0.0086, -0.1145,\n",
       "           1.1252, -1.0409,  0.4033, -1.9823, -1.2089, -0.0206, -1.2471,\n",
       "          -0.8496,  0.0734,  0.4906,  0.2156,  1.6506, -0.1539, -0.2209,\n",
       "          -1.0930, -1.1917, -0.5622,  0.9872, -0.4218, -0.2463, -0.5697,\n",
       "           0.5870, -0.0346, -0.9418, -0.0023,  0.2938, -0.1924,  0.1654,\n",
       "          -0.5594, -0.4363, -0.6927,  0.1656, -0.0520,  0.5901, -0.0728,\n",
       "          -2.8147,  0.2504,  0.8803, -2.4679, -0.7304, -1.3730,  1.5416,\n",
       "          -0.6297, -1.7307, -0.0495,  0.4402,  1.1179, -0.2547, -0.7872,\n",
       "           0.9065, -1.7322,  1.0359,  0.0490,  2.1147,  0.0756, -0.0830,\n",
       "          -0.0426, -0.6781, -1.4980, -0.5784, -1.2056,  1.7099, -0.2972,\n",
       "          -1.6964,  1.9690],\n",
       "         [ 0.4449,  2.1773,  0.2186,  1.2223,  1.6831, -2.1357, -0.3061,\n",
       "           1.4245, -0.9638,  0.5557, -1.6008,  0.3808,  0.6406,  0.4113,\n",
       "           0.2339,  2.2354,  0.1184,  0.5035,  0.2679, -0.3131, -0.5065,\n",
       "          -0.6034, -2.3480, -1.0306,  0.5545, -1.0414,  0.2667, -0.6231,\n",
       "          -0.8668, -0.5051, -1.8559, -1.4342,  0.0454,  0.2777, -0.6413,\n",
       "           0.0678, -0.5910, -1.4141,  0.0579,  1.0942, -1.6512, -0.3062,\n",
       "           0.0119, -1.3098, -0.0684, -0.1892,  0.5252,  0.4174,  1.0192,\n",
       "           1.1057, -0.3787, -0.7711,  0.2084,  0.2686, -0.8492,  0.2047,\n",
       "           0.5234, -0.5453, -0.6722, -0.9975, -0.2154, -0.1993,  0.4991,\n",
       "          -0.1887,  0.1617,  1.7609,  1.6920, -0.0732, -0.3827,  0.5310,\n",
       "          -1.4560,  1.3012, -0.7739, -0.0628,  0.0987, -0.9100, -0.2766,\n",
       "          -0.8095,  0.4071,  0.0501, -0.3519,  0.4761,  0.1788,  0.0839,\n",
       "           0.9865, -0.7085, -0.2101, -0.7753, -0.4371, -0.0489, -0.3260,\n",
       "           0.6267,  0.8870, -0.6095, -1.0269, -0.7549, -1.8098, -0.8632,\n",
       "           1.0272,  0.9936],\n",
       "         [ 0.1613,  0.7389, -0.6296,  1.5166, -0.8290, -0.4532,  0.8168,\n",
       "           0.9202, -1.2247,  1.3582, -0.7497,  0.1716,  0.1732, -1.6599,\n",
       "          -2.1574, -1.3173, -1.6685,  0.5544,  2.2511,  0.5452, -0.5708,\n",
       "          -0.4474, -2.5262,  1.4889, -0.3956,  1.3871,  0.8328, -0.6982,\n",
       "           0.1464,  0.9918,  0.1697,  1.1609,  0.0606,  0.7123, -1.2196,\n",
       "           1.2445,  0.9841, -1.1469, -0.8883,  1.3298, -1.6171, -1.0273,\n",
       "           0.5892, -0.1051,  1.4624, -0.1975, -0.5082, -1.6029, -0.3305,\n",
       "           0.7154,  0.0119,  0.2847, -0.4242, -0.3115,  0.3766, -0.0696,\n",
       "           0.4510, -1.0260, -1.1000, -0.6836, -0.1216,  0.1232, -0.2826,\n",
       "          -0.1016, -1.1854,  0.5528,  0.0025,  0.0257,  0.7262,  1.5611,\n",
       "          -0.0757,  0.6300, -0.2802,  1.3641,  0.2486, -0.7100, -0.8903,\n",
       "           0.0241, -0.7278,  0.9057,  0.0689,  0.3778,  1.9295,  1.4201,\n",
       "           0.8090, -0.7408,  0.0773, -0.1730, -0.9811,  1.8183,  0.1023,\n",
       "           0.7848,  1.4024,  1.6419,  0.3772, -1.2462, -0.4244, -1.5170,\n",
       "           0.4972, -1.5988],\n",
       "         [-0.8373, -0.8427,  0.3243,  1.4762,  0.2575,  1.0587, -0.6701,\n",
       "          -0.8799,  0.9712, -0.6278,  0.4007, -1.5056,  0.7714,  0.4372,\n",
       "           0.6565, -0.2400, -0.0765,  2.7430, -0.4360, -1.8656, -1.0289,\n",
       "          -0.2424, -1.2321,  0.9775, -0.1792,  0.1869, -0.7059, -0.3062,\n",
       "          -0.5942, -1.0543, -0.4809,  0.6940,  1.6419,  0.1286, -0.8928,\n",
       "           0.0774,  0.3756,  0.4242,  1.6930,  0.1806, -2.1303,  1.0347,\n",
       "          -0.4276, -0.6102, -0.8407,  0.1253, -0.4090,  1.0186,  0.7018,\n",
       "          -0.8956, -1.6415, -0.2938,  1.0813, -0.4138, -0.5462, -0.0051,\n",
       "           0.5406,  0.3397, -2.5915,  1.0986, -0.8310,  1.1583,  0.3984,\n",
       "           1.3204,  0.2762,  1.2113, -0.4446, -1.8178,  0.5797,  2.3392,\n",
       "          -0.3838,  0.1409,  0.7863, -0.4014, -0.5904,  0.8558, -0.6986,\n",
       "          -0.7070, -0.4031, -0.0413, -0.4535, -0.4761, -0.4807,  1.1757,\n",
       "          -0.1789,  0.1218, -0.1308,  0.5667,  0.8816,  0.0574, -0.4306,\n",
       "           0.2746, -0.3673, -2.3397,  0.9020, -1.6750, -1.8568,  0.3296,\n",
       "           0.3509,  0.6000],\n",
       "         [ 0.5656,  0.3463,  0.5222, -1.2505, -0.3685,  0.9424,  0.1655,\n",
       "           0.4980,  0.9928,  1.3220, -0.4904,  0.0728,  1.4401, -0.7902,\n",
       "           0.1901,  0.5372, -0.9070,  0.8638,  1.0076,  0.4067, -0.2352,\n",
       "           0.5414, -0.5709, -1.8726, -1.7086, -0.7066,  0.0985,  0.5797,\n",
       "           0.7758,  0.5965, -0.2764,  0.7659,  0.8950, -0.9432,  0.4360,\n",
       "           0.2241,  1.6559,  0.2725,  0.3669, -2.6264,  0.5975,  0.8038,\n",
       "          -0.5961, -1.0701,  0.7752,  0.3890, -0.8980, -1.7767,  0.5763,\n",
       "           0.7687, -0.7019,  0.2224, -1.2353,  0.4258, -0.5076,  1.5046,\n",
       "          -1.6298,  1.7608,  1.3600, -0.4956,  0.3024,  0.6244, -0.0190,\n",
       "          -0.1190,  0.4943,  0.0235,  1.6348, -0.2264, -0.5825, -0.1410,\n",
       "           0.1258,  0.2575,  0.8994, -0.5380,  1.0195,  0.2973,  1.2434,\n",
       "           1.8315, -0.4688,  0.5050, -0.6802,  0.9635, -0.4282, -1.3238,\n",
       "           1.3447, -0.4990, -0.2759,  0.4784, -0.4138, -0.6868, -0.3932,\n",
       "          -0.2051,  2.3606,  0.1433, -0.4448,  1.5220, -1.4146, -1.3461,\n",
       "          -0.2872,  1.1607],\n",
       "         [-0.7871, -0.9963,  0.5019, -0.8715, -0.7220,  0.0738, -1.8072,\n",
       "          -0.9745, -1.0810, -2.6641, -2.4913,  0.1463, -0.5313, -0.6515,\n",
       "           0.2589,  0.5708, -1.3159,  0.0280, -0.0317, -1.2057, -0.0824,\n",
       "           1.4469, -0.6914, -0.9663, -0.3118,  1.5762, -0.0506,  0.8871,\n",
       "          -1.4469,  0.6802, -0.5485, -1.2953,  0.5966,  0.6962, -0.0094,\n",
       "           1.6524, -1.2167, -0.0756, -1.2153,  1.0699,  0.0116,  0.1099,\n",
       "          -0.0521, -0.4042,  1.7484,  0.9683,  1.3507, -0.7137, -1.1874,\n",
       "          -0.6638, -0.4065,  0.6268, -0.1512,  0.9748,  0.1509,  0.3897,\n",
       "          -1.6461,  0.7129,  1.5666,  2.6510, -0.0917, -0.5931, -0.4108,\n",
       "           0.2762, -0.7158, -0.8729, -0.6151, -0.5979,  1.2170,  0.3540,\n",
       "           0.1175,  0.1362, -1.2223, -0.0839, -0.4752,  1.4916,  0.6171,\n",
       "          -0.5466,  1.3360, -0.2283,  0.9810,  0.7555, -0.9456,  0.1594,\n",
       "          -1.6716, -0.8032,  0.7932, -0.3437,  0.9455,  1.8260,  0.5346,\n",
       "          -0.4494,  0.3755,  1.1268, -0.5025, -0.9232,  0.4642,  0.2486,\n",
       "           1.6096, -1.7842]], device='cuda:0'), Parameter containing:\n",
       " tensor([[-0.5822,  2.3576,  0.0624,  ...,  0.9927,  0.1270,  0.5524],\n",
       "         [-0.7148,  1.1609,  0.2431,  ..., -1.1144,  0.3737,  0.2415],\n",
       "         [ 1.1142, -0.7967, -2.1737,  ..., -0.0006,  0.7029,  0.0836],\n",
       "         ...,\n",
       "         [ 0.4531,  1.3636,  1.1805,  ..., -0.7202, -0.9531, -1.2754],\n",
       "         [ 0.3805, -0.1555,  1.0654,  ..., -1.9320, -0.2815, -1.2568],\n",
       "         [ 0.1691, -0.7152,  0.4977,  ..., -0.5718,  0.5991, -1.0786]], device='cuda:0'), Parameter containing:\n",
       " tensor([[ 9.5610e-02,  2.0116e-01,  3.0982e-02,  ..., -2.4418e-01,\n",
       "           1.8576e-02, -2.1683e-03],\n",
       "         [ 1.3195e-01, -6.5639e-02, -8.7419e-02,  ...,  8.2013e-03,\n",
       "           4.1245e-02,  2.1241e-01],\n",
       "         [ 2.3657e-01,  2.0137e-01,  7.7309e-02,  ..., -2.5425e-01,\n",
       "           4.7716e-02,  2.6725e-02],\n",
       "         ...,\n",
       "         [ 1.6804e-01,  9.9074e-02, -1.1101e-02,  ..., -1.0670e-01,\n",
       "           8.1163e-02,  4.8226e-02],\n",
       "         [ 1.1125e-01,  1.1834e-01, -1.8179e-01,  ...,  6.8801e-03,\n",
       "          -9.9067e-02,  1.8642e-01],\n",
       "         [ 8.7206e-02,  3.5189e-03,  4.8537e-02,  ..., -1.2952e-01,\n",
       "           3.0043e-02, -6.9748e-02]], device='cuda:0'), Parameter containing:\n",
       " tensor([[-1.6809e-01, -1.6554e-01,  2.0833e-01,  ...,  8.3044e-02,\n",
       "          -6.8855e-02, -2.4400e-01],\n",
       "         [ 4.8115e-02,  1.6116e-01, -2.2965e-01,  ..., -2.2732e-01,\n",
       "           1.1582e-01,  1.2130e-01],\n",
       "         [-8.6363e-02, -7.3286e-02,  1.0329e-01,  ..., -1.8123e-01,\n",
       "           2.7962e-02, -1.7791e-01],\n",
       "         ...,\n",
       "         [-1.6596e-01, -2.1308e-01,  9.6258e-02,  ..., -3.1819e-02,\n",
       "          -2.9338e-02, -1.6741e-01],\n",
       "         [-9.1253e-02, -1.5724e-01,  2.4524e-01,  ...,  1.6025e-01,\n",
       "           2.6072e-01,  1.6803e-01],\n",
       "         [-2.0123e-01, -1.5731e-02,  3.0259e-01,  ...,  1.1973e-01,\n",
       "          -6.5458e-02, -1.0976e-01]], device='cuda:0'), Parameter containing:\n",
       " tensor([-1.1473e-01,  1.2123e-02,  1.5679e-01, -9.0970e-02, -7.1003e-02,\n",
       "         -3.8688e-02,  2.8931e-01,  5.8456e-02, -1.5554e-01,  2.7190e-02,\n",
       "         -4.2814e-02,  2.0023e-01,  7.3466e-02,  1.8373e-01, -3.1099e-01,\n",
       "          9.1622e-02, -1.3964e-01,  3.2484e-01,  9.9451e-02, -9.2254e-03,\n",
       "         -1.1462e-01,  9.7559e-02,  2.5292e-01, -2.1124e-02,  5.2489e-02,\n",
       "         -8.5674e-03,  1.0092e-01, -8.2773e-02, -6.7992e-02,  5.8638e-02,\n",
       "         -1.4679e-01,  4.9434e-02, -1.5660e-02, -1.9268e-02, -5.4012e-02,\n",
       "          4.9508e-02,  1.7567e-02, -2.2176e-02, -8.4823e-02, -1.3100e-01,\n",
       "          1.7015e-02, -5.7011e-02, -1.6746e-01, -3.5563e-02, -2.7881e-01,\n",
       "          1.2578e-01,  7.6159e-02, -9.8511e-02, -5.4737e-02, -3.4349e-02,\n",
       "         -4.2338e-02,  1.2634e-01, -2.9657e-02,  1.4437e-01,  8.8783e-03,\n",
       "          1.9129e-01, -2.1032e-01,  4.3172e-02, -1.3804e-02,  7.6501e-02,\n",
       "          4.4165e-02, -1.9853e-01, -1.3639e-02, -2.7413e-02,  2.6265e-01,\n",
       "          5.9011e-02,  6.3701e-02, -2.3587e-01,  1.5335e-02, -1.6125e-01,\n",
       "         -1.1585e-01,  2.9431e-02, -4.0997e-02,  1.3271e-01,  1.8092e-02,\n",
       "          2.3177e-01, -6.7217e-02, -1.4763e-01,  6.1959e-02, -9.6678e-02,\n",
       "          2.4669e-02, -1.7001e-01, -7.0656e-02,  4.0613e-01,  1.1945e-01,\n",
       "         -4.2741e-02,  6.0216e-03,  1.1161e-01,  1.4034e-01,  2.5240e-01,\n",
       "          3.4961e-01, -1.7174e-01, -1.0228e-01, -1.9233e-01, -1.1476e-02,\n",
       "          2.1498e-01, -4.4397e-02,  3.2718e-01, -2.2166e-01, -2.8370e-01,\n",
       "         -1.0585e-01, -6.5984e-02, -6.3565e-02,  2.2195e-02, -1.3682e-01,\n",
       "          1.9889e-01,  3.7984e-01,  1.7543e-01,  1.8466e-02,  4.9845e-02,\n",
       "         -4.6344e-02,  1.1887e-02, -2.8001e-03,  7.4797e-02, -5.2270e-02,\n",
       "          1.6658e-01, -6.1905e-04,  1.8625e-01,  2.6470e-01, -2.2623e-01,\n",
       "          9.1176e-02, -8.3091e-02,  3.9384e-01,  6.7635e-02,  1.7735e-01,\n",
       "          5.5400e-02,  2.7029e-01, -1.1178e-01,  2.6196e-02,  6.3760e-02,\n",
       "         -1.7676e-01,  2.8943e-01,  1.0189e-01,  7.1191e-02,  7.5754e-02,\n",
       "          2.7296e-01,  9.1624e-02, -8.0456e-02,  5.5033e-02, -1.0979e-01,\n",
       "          3.6913e-02, -2.2412e-02,  5.2869e-02, -7.5630e-02, -1.0538e-01,\n",
       "          1.5140e-01,  2.2130e-02,  8.1774e-02,  1.9673e-02, -3.2551e-02,\n",
       "         -9.7928e-02,  2.0856e-01,  5.1975e-02,  4.4626e-01,  9.9128e-02,\n",
       "          3.2786e-01, -1.1355e-01, -2.9009e-03,  2.0408e-01,  1.2178e-03,\n",
       "          2.1876e-02, -9.3944e-02,  4.5030e-02,  2.1959e-01,  2.5232e-02,\n",
       "          1.6805e-02, -4.4396e-02,  1.2116e-01, -4.2171e-02, -1.4056e-01,\n",
       "          1.3345e-01, -2.2422e-01,  2.0968e-01,  3.6077e-02,  2.1197e-01,\n",
       "          2.2603e-01, -6.8015e-02, -1.2823e-01, -1.3340e-01, -1.0148e-01,\n",
       "          3.8403e-01, -8.7432e-02,  6.5783e-02,  3.1543e-01,  2.7484e-02,\n",
       "          8.2046e-02, -7.3215e-02, -1.3294e-02,  1.3729e-02,  1.8574e-02,\n",
       "          1.8764e-01,  1.2295e-01,  8.9131e-02,  4.7525e-02,  6.7933e-02,\n",
       "          1.6355e-01,  8.6605e-02,  4.3786e-01,  1.1093e-02, -7.1346e-02,\n",
       "          3.1708e-02, -3.3290e-02, -1.3983e-01,  1.9880e-02,  9.4573e-03,\n",
       "          1.2145e-01,  2.4394e-02, -4.5003e-03,  1.0117e-01, -2.2575e-01,\n",
       "         -2.8017e-02,  6.7708e-04,  2.3747e-02,  1.1751e-02,  1.0427e-01,\n",
       "         -9.2816e-02, -1.2969e-01, -2.6588e-02,  1.9225e-01, -2.9790e-02,\n",
       "          9.2050e-02,  1.7735e-01,  1.8864e-01, -6.0147e-02,  3.9856e-02,\n",
       "          6.3778e-02, -9.4040e-02, -1.4236e-01,  8.6686e-02,  9.8805e-03,\n",
       "          6.1781e-02, -7.3981e-02,  3.3369e-02, -8.6161e-02, -1.6087e-02,\n",
       "          2.2197e-02,  1.8334e-01,  7.1089e-02, -1.3229e-01,  1.2149e-01,\n",
       "         -1.0018e-01,  4.4714e-02, -5.0725e-03,  3.3035e-02,  1.9106e-02,\n",
       "         -1.0100e-01, -1.6912e-01, -8.1672e-02, -1.7308e-01,  1.7717e-01,\n",
       "          1.4107e-01,  1.9209e-01,  1.5214e-01, -1.5250e-01,  1.1560e-01,\n",
       "         -1.7048e-01, -1.7294e-02, -2.1755e-01, -8.9929e-02, -2.9862e-02,\n",
       "         -3.2679e-02,  1.9176e-01,  5.6629e-02,  1.2544e-01, -5.7266e-02,\n",
       "          5.1026e-03, -2.9075e-02, -2.0600e-02,  2.3070e-02,  6.3477e-02,\n",
       "         -2.1938e-01, -6.3616e-02, -3.4944e-02, -8.7253e-02, -1.8442e-02,\n",
       "         -2.3715e-01,  8.9707e-02,  3.2312e-02,  4.7810e-03,  8.2914e-02,\n",
       "          5.8661e-02,  7.7143e-03,  6.5178e-03, -9.3409e-02, -5.7416e-02,\n",
       "          1.4166e-02,  1.2334e-01,  5.0179e-02,  8.3238e-02, -4.5902e-02,\n",
       "          2.8896e-02,  2.6142e-03, -2.6251e-01,  1.6037e-02, -7.6938e-02,\n",
       "          2.0220e-01, -5.3105e-03, -1.1021e-03, -1.8791e-01, -1.4726e-01,\n",
       "         -2.2922e-01,  9.8195e-06, -1.1947e-01,  1.6263e-01,  1.1607e-01,\n",
       "          1.8239e-01,  2.3044e-01,  2.1069e-01, -1.7391e-01,  7.8593e-02,\n",
       "         -1.5847e-01,  9.1052e-02, -6.9205e-02,  1.6188e-01, -1.8462e-01,\n",
       "         -1.1605e-01, -1.7111e-01,  3.8544e-01,  1.0648e-01, -1.0837e-01,\n",
       "         -1.5116e-01,  2.4951e-02,  4.4572e-01,  9.0863e-02,  2.2748e-01,\n",
       "         -1.5239e-01, -9.0362e-02, -3.0276e-02, -5.1136e-02, -6.6444e-02,\n",
       "          4.5360e-02,  1.6969e-01, -1.9936e-01, -1.3437e-01, -1.1009e-01,\n",
       "         -1.2855e-01,  1.5535e-01, -8.2896e-03,  2.7226e-02,  9.5711e-02,\n",
       "          9.8169e-02, -9.5176e-02,  3.5113e-02, -1.6792e-01, -1.9279e-02,\n",
       "         -9.3490e-02, -8.1899e-02, -1.2455e-01, -2.8869e-01,  1.4700e-01,\n",
       "         -5.7759e-02,  1.4086e-01, -1.1043e-01,  4.0174e-01, -9.7587e-02,\n",
       "         -3.4929e-03, -1.0320e-01,  5.9178e-02, -1.3289e-01,  1.3555e-01,\n",
       "         -4.2963e-03, -1.9575e-01, -5.0410e-02, -1.4507e-01,  1.7706e-03,\n",
       "         -4.7897e-02,  1.0121e-02, -1.9469e-02, -8.7663e-02,  1.6049e-01,\n",
       "          9.1970e-03, -8.5598e-02,  5.6648e-02,  1.9970e-01,  4.0600e-02,\n",
       "          3.0250e-01, -4.2552e-02,  2.6814e-02,  9.9142e-02, -1.9051e-01,\n",
       "          1.8874e-01, -2.0381e-01,  1.1858e-01,  1.0752e-01, -1.5380e-01,\n",
       "          1.2462e-02, -1.6691e-01, -1.1926e-01,  1.2668e-01,  1.9382e-01,\n",
       "          1.6319e-01,  1.2971e-01,  8.4637e-02, -1.9639e-01,  8.5106e-02,\n",
       "          1.4824e-02, -1.5158e-01,  3.1926e-01, -1.2865e-01,  2.1725e-02], device='cuda:0'), Parameter containing:\n",
       " tensor([-0.0784, -0.1597,  0.1040, -0.0070, -0.0569, -0.0634,  0.2920,\n",
       "          0.1088, -0.0478, -0.0217, -0.1594,  0.3379,  0.1206,  0.1022,\n",
       "         -0.3237,  0.0490, -0.1694,  0.3056,  0.0559, -0.1170, -0.0756,\n",
       "          0.1297,  0.2123,  0.1473, -0.0252, -0.0923,  0.0491,  0.0570,\n",
       "         -0.0644,  0.0265, -0.0196, -0.0005,  0.1315, -0.0600, -0.0478,\n",
       "         -0.0168, -0.0632, -0.0728, -0.1139, -0.0747,  0.0977, -0.0100,\n",
       "         -0.1714,  0.0578, -0.2418, -0.0473, -0.0039, -0.0724, -0.0331,\n",
       "         -0.1557,  0.0107,  0.1356,  0.0626,  0.0860, -0.0156,  0.2053,\n",
       "         -0.2974, -0.0314,  0.1296,  0.0734,  0.1232, -0.1400, -0.0406,\n",
       "         -0.1770,  0.1946,  0.1462, -0.0125, -0.1358, -0.1275, -0.1056,\n",
       "         -0.0709,  0.0731,  0.0235,  0.0433,  0.1304,  0.1253, -0.0086,\n",
       "         -0.1497,  0.0039, -0.1588,  0.0101, -0.1715, -0.0405,  0.4264,\n",
       "         -0.0655, -0.0843, -0.1244, -0.0007,  0.1378,  0.1694,  0.3652,\n",
       "         -0.2208, -0.0446, -0.0646,  0.0843,  0.2216, -0.0955,  0.2215,\n",
       "         -0.3636, -0.2658, -0.1995, -0.0235, -0.0494,  0.1067, -0.0499,\n",
       "          0.2813,  0.2562,  0.1582,  0.0410, -0.0161, -0.0647,  0.0452,\n",
       "          0.0561,  0.2137, -0.1588,  0.3242, -0.0488,  0.0637,  0.0908,\n",
       "         -0.1730, -0.0812, -0.0785,  0.3991,  0.0427,  0.2740, -0.0863,\n",
       "          0.3456, -0.1505,  0.0099, -0.0475, -0.0963,  0.3098,  0.1610,\n",
       "          0.0334,  0.0795,  0.2466,  0.2421, -0.1814,  0.2133, -0.0713,\n",
       "          0.0385,  0.1578,  0.0327, -0.0489, -0.1005,  0.0418, -0.0802,\n",
       "          0.0506, -0.1102, -0.1161,  0.0306,  0.1308, -0.0432,  0.4359,\n",
       "          0.1823,  0.3321, -0.0334, -0.0135,  0.2181, -0.0521,  0.0100,\n",
       "          0.0155,  0.0698,  0.0748,  0.0233, -0.0151,  0.0342,  0.1433,\n",
       "          0.0873,  0.0014, -0.0044, -0.2556,  0.2925,  0.1372,  0.3626,\n",
       "          0.3395, -0.1169, -0.1316, -0.1416, -0.1201,  0.4086,  0.0237,\n",
       "          0.1443,  0.2761,  0.0584,  0.0997, -0.2154, -0.0946,  0.0120,\n",
       "         -0.0951,  0.1683, -0.0298,  0.1349, -0.0298,  0.1253,  0.1011,\n",
       "          0.0894,  0.4946,  0.0425, -0.0192,  0.0154, -0.1296, -0.1482,\n",
       "          0.0185, -0.0802,  0.0705,  0.1011,  0.0066,  0.2139, -0.2477,\n",
       "          0.0867,  0.1028,  0.0453,  0.1151,  0.0860, -0.1911, -0.1071,\n",
       "         -0.1329,  0.1737, -0.0679,  0.1467,  0.0420,  0.0244, -0.1386,\n",
       "         -0.0510, -0.0669, -0.1209, -0.0312, -0.0017,  0.0425,  0.2449,\n",
       "         -0.1923, -0.0326, -0.1670, -0.0042, -0.0390,  0.1869,  0.0859,\n",
       "         -0.1298, -0.0314, -0.0920, -0.0142,  0.0850, -0.0273,  0.0238,\n",
       "          0.0322, -0.1898,  0.0128, -0.1204,  0.3208,  0.1348,  0.1213,\n",
       "          0.0571, -0.1819,  0.0477, -0.0741, -0.0701, -0.1034, -0.1241,\n",
       "         -0.0591, -0.0688,  0.0420, -0.0069,  0.1051, -0.1710,  0.0772,\n",
       "         -0.0851,  0.0121,  0.1823,  0.0611, -0.2038, -0.0137, -0.0708,\n",
       "         -0.0917, -0.0679, -0.1448,  0.1444, -0.0077, -0.0351,  0.0878,\n",
       "          0.0075, -0.0829,  0.0831, -0.1314,  0.0485,  0.0765,  0.0313,\n",
       "          0.0046,  0.1213, -0.0042,  0.0557,  0.0311, -0.2423,  0.0838,\n",
       "          0.0748,  0.1011, -0.0240,  0.0009, -0.0872, -0.1687, -0.2326,\n",
       "         -0.0423, -0.1818,  0.0733,  0.1444,  0.1307,  0.2242,  0.0427,\n",
       "         -0.0804, -0.0028,  0.0042,  0.2603, -0.1504,  0.0325, -0.1337,\n",
       "         -0.1789, -0.0504,  0.2175,  0.1376, -0.2122, -0.1658,  0.0268,\n",
       "          0.4539,  0.0664,  0.2230, -0.1350,  0.0513,  0.0423,  0.0921,\n",
       "         -0.1876,  0.0059,  0.1354, -0.1100, -0.2253, -0.1755, -0.1364,\n",
       "          0.1471,  0.1259,  0.2047,  0.0237,  0.0896, -0.0322,  0.0799,\n",
       "         -0.0652, -0.0282,  0.0125, -0.0521, -0.1874, -0.3273,  0.1246,\n",
       "         -0.1458, -0.0130, -0.1166,  0.2566, -0.1398, -0.0145, -0.2177,\n",
       "          0.0947, -0.0485,  0.0921, -0.0430, -0.2383, -0.2245, -0.0630,\n",
       "         -0.0830, -0.0072,  0.2014, -0.1574, -0.0652,  0.1620,  0.0506,\n",
       "         -0.0972,  0.0218,  0.1758, -0.1213,  0.2642,  0.1074, -0.1271,\n",
       "          0.0629, -0.2370,  0.0434, -0.0558,  0.1440,  0.1961, -0.1336,\n",
       "         -0.0718, -0.2403, -0.0458,  0.0074,  0.2280,  0.2052,  0.0995,\n",
       "          0.2387, -0.1318,  0.0370, -0.0720, -0.0740,  0.1784, -0.1051,\n",
       "         -0.0362], device='cuda:0'), Parameter containing:\n",
       " tensor([[ 1.2625e-01,  3.9912e-02, -1.4644e-01,  ...,  9.1391e-02,\n",
       "          -6.5887e-02,  1.7429e-01],\n",
       "         [-2.1284e-03,  1.5639e-01,  3.8892e-02,  ...,  6.9561e-03,\n",
       "          -1.0712e-01,  8.0733e-02],\n",
       "         [ 1.1705e-01, -6.8166e-02, -3.6775e-02,  ...,  1.5046e-01,\n",
       "          -9.3003e-02,  2.3301e-01],\n",
       "         ...,\n",
       "         [-7.6001e-02, -2.1949e-02,  5.7584e-02,  ..., -6.4071e-02,\n",
       "          -1.0074e-01,  7.0231e-02],\n",
       "         [-3.1433e-02, -6.6036e-02, -1.2832e-02,  ...,  4.3759e-02,\n",
       "           9.9433e-02,  2.5084e-01],\n",
       "         [ 2.2016e-01, -8.5387e-03, -9.8533e-02,  ..., -1.3113e-01,\n",
       "           1.6771e-02, -4.8963e-02]], device='cuda:0'), Parameter containing:\n",
       " tensor([[ 9.1469e-03,  1.7154e-01,  1.3227e-01,  ..., -4.9609e-02,\n",
       "          -7.7423e-02, -1.9249e-01],\n",
       "         [-1.2020e-01, -3.4048e-01,  1.6467e-01,  ...,  1.0393e-01,\n",
       "           1.1020e-01, -2.0144e-01],\n",
       "         [ 1.8468e-01, -3.7035e-01,  1.2117e-01,  ...,  8.6097e-02,\n",
       "          -8.4395e-02, -1.5158e-01],\n",
       "         ...,\n",
       "         [-9.0843e-02,  2.0856e-01, -1.6066e-01,  ..., -7.0180e-02,\n",
       "           3.7241e-03,  7.9483e-02],\n",
       "         [-3.9775e-02,  1.0697e-01,  1.2431e-02,  ..., -2.3233e-01,\n",
       "          -1.1885e-01,  1.0906e-01],\n",
       "         [ 2.4495e-01, -2.8646e-01, -1.9867e-02,  ..., -4.4987e-02,\n",
       "           1.4249e-01, -3.6622e-02]], device='cuda:0'), Parameter containing:\n",
       " tensor([ 0.0392, -0.1888, -0.1965, -0.2892, -0.2650, -0.1581, -0.1347,\n",
       "         -0.0591, -0.1242, -0.2876, -0.1157, -0.1502, -0.1305, -0.1574,\n",
       "         -0.0166, -0.0846,  0.0105,  0.0514,  0.1462, -0.1574, -0.3232,\n",
       "         -0.2409, -0.2566, -0.1905, -0.1337, -0.1451, -0.2262,  0.0436,\n",
       "          0.1125,  0.1797,  0.1100, -0.1362,  0.0600, -0.0265, -0.3351,\n",
       "          0.1236, -0.1496, -0.5139, -0.0226, -0.1659, -0.1246, -0.0151,\n",
       "          0.0431, -0.1781, -0.2965, -0.0592, -0.1220, -0.2071, -0.2435,\n",
       "          0.0489, -0.1637, -0.1959,  0.0601,  0.0301, -0.0014, -0.0900,\n",
       "         -0.0970, -0.1028, -0.0786, -0.2247, -0.2254, -0.2453, -0.1393,\n",
       "         -0.1620, -0.2271,  0.1383, -0.2118, -0.4322, -0.1996,  0.2295,\n",
       "         -0.2310, -0.4064, -0.3041, -0.1125, -0.2360, -0.2153, -0.1651,\n",
       "         -0.1652,  0.1822,  0.0174, -0.2349, -0.1715, -0.0139, -0.1661,\n",
       "         -0.2670, -0.2048, -0.0601, -0.2330, -0.0746, -0.1218, -0.4311,\n",
       "         -0.2670, -0.3155, -0.2716,  0.0449, -0.0781, -0.1867, -0.2286,\n",
       "         -0.0475,  0.0558,  0.0782, -0.1374,  0.1738,  0.0942, -0.1160,\n",
       "         -0.1882, -0.1923, -0.0694,  0.1918, -0.2052,  0.1115, -0.2026,\n",
       "          0.1736,  0.0453,  0.1762,  0.1891,  0.2021,  0.0741,  0.0166,\n",
       "         -0.0776, -0.1079, -0.0443, -0.0504,  0.2891,  0.0319, -0.0779,\n",
       "          0.1371,  0.1871,  0.0709, -0.0039, -0.1316,  0.0109,  0.1158,\n",
       "         -0.0534,  0.1784,  0.3113, -0.0156, -0.2888, -0.0458, -0.1836,\n",
       "         -0.0226,  0.0909, -0.1291, -0.0987, -0.2085,  0.1400, -0.0592,\n",
       "         -0.1270,  0.2047,  0.1409, -0.1011, -0.0291,  0.0446, -0.0165,\n",
       "          0.0836, -0.1519,  0.1966, -0.1755, -0.0904, -0.2122,  0.0561,\n",
       "         -0.1534, -0.0541, -0.0953,  0.1734, -0.0265, -0.1982, -0.1794,\n",
       "         -0.1100,  0.2290, -0.1048, -0.1207, -0.1252,  0.0377, -0.2886,\n",
       "         -0.0530, -0.0755, -0.1963, -0.0983, -0.0948, -0.0928, -0.0560,\n",
       "          0.2483, -0.1674,  0.0823,  0.1723,  0.1763, -0.1681,  0.0573,\n",
       "         -0.0226, -0.2480,  0.0740,  0.1048, -0.0904,  0.1550,  0.0830,\n",
       "          0.0700,  0.0667,  0.0803,  0.3014,  0.1238,  0.0851, -0.1375,\n",
       "         -0.0861,  0.2159,  0.0704,  0.0060, -0.1670,  0.0565, -0.0725,\n",
       "         -0.2018,  0.0583, -0.0408, -0.1903,  0.0217,  0.0618,  0.0579,\n",
       "         -0.1476, -0.0193,  0.0061,  0.0225, -0.0464, -0.0809,  0.0709,\n",
       "         -0.0519, -0.0920, -0.0181, -0.0710, -0.0249, -0.0409, -0.0477,\n",
       "         -0.1423,  0.0068,  0.0239, -0.0387,  0.0887,  0.0750,  0.0710,\n",
       "         -0.0106,  0.0336,  0.1225, -0.0213, -0.1006,  0.0652,  0.0683,\n",
       "          0.0022, -0.1873, -0.1474, -0.0324, -0.0633, -0.0313,  0.0890,\n",
       "          0.0955,  0.0611,  0.0612, -0.0307, -0.0064,  0.0231, -0.1678,\n",
       "         -0.0101,  0.0028,  0.0401,  0.0145,  0.0639,  0.0116, -0.1008,\n",
       "         -0.1292,  0.0972,  0.0148, -0.2063, -0.0257, -0.0662, -0.0623,\n",
       "          0.0898, -0.0764,  0.0509,  0.1942,  0.1250, -0.1160, -0.1082,\n",
       "         -0.0536,  0.1169, -0.0005,  0.0248,  0.0075, -0.1199, -0.0827,\n",
       "         -0.1176, -0.2210, -0.1493,  0.1169,  0.1243,  0.1558, -0.0468,\n",
       "         -0.0789,  0.1526,  0.1176, -0.1702,  0.0416, -0.1066, -0.0265,\n",
       "         -0.2052, -0.0472, -0.1769, -0.2927, -0.3710, -0.3604, -0.2733,\n",
       "          0.0139, -0.2570,  0.0071, -0.1365,  0.0570, -0.2709, -0.0642,\n",
       "          0.0214,  0.0792,  0.0137,  0.2458,  0.0101, -0.3471, -0.0537,\n",
       "         -0.0498,  0.0189, -0.0142, -0.2815,  0.1445, -0.0406, -0.0144,\n",
       "          0.1349, -0.1622, -0.2809, -0.0970, -0.3179, -0.0870,  0.0553,\n",
       "         -0.1363, -0.5143, -0.0412, -0.0632, -0.0183, -0.0422, -0.1127,\n",
       "         -0.1972, -0.1168, -0.0410, -0.2752, -0.2070,  0.0872, -0.1171,\n",
       "         -0.1935, -0.2589,  0.0355,  0.0313, -0.1267, -0.1999, -0.1655,\n",
       "         -0.2283, -0.0907, -0.4584, -0.0878, -0.4452, -0.1294, -0.0044,\n",
       "          0.1328,  0.0645, -0.1687, -0.5170, -0.3446,  0.1627, -0.2733,\n",
       "         -0.3690, -0.2011,  0.0984, -0.3627, -0.1564,  0.1286, -0.1575,\n",
       "         -0.0187,  0.0076, -0.2548, -0.3715,  0.1308, -0.2015, -0.3033,\n",
       "          0.1594,  0.0688, -0.0789, -0.1726, -0.0876, -0.3319, -0.1229,\n",
       "         -0.0446, -0.3858, -0.0836,  0.0860, -0.1016,  0.0924,  0.0590,\n",
       "         -0.1188], device='cuda:0'), Parameter containing:\n",
       " tensor([ 0.1040, -0.2231, -0.0794, -0.4036, -0.1315, -0.2954, -0.0018,\n",
       "         -0.1587, -0.1094, -0.1960, -0.2935, -0.1790, -0.0753,  0.0054,\n",
       "         -0.1716,  0.0982, -0.0530,  0.0390,  0.1596, -0.3179, -0.3242,\n",
       "         -0.2924, -0.2636, -0.3018, -0.2448, -0.2425, -0.2124, -0.0338,\n",
       "         -0.0226,  0.2400,  0.1173, -0.0236,  0.0483, -0.1094, -0.1636,\n",
       "          0.1233, -0.0642, -0.3904, -0.0617, -0.1130,  0.0513,  0.0758,\n",
       "          0.0361, -0.2828, -0.3000, -0.1548, -0.2628, -0.2436, -0.2986,\n",
       "         -0.0593, -0.2302, -0.2113,  0.1582, -0.0330, -0.0112, -0.0924,\n",
       "          0.0200, -0.0638, -0.1111, -0.2980, -0.2121, -0.2624, -0.1157,\n",
       "         -0.1426, -0.2012,  0.2618, -0.2285, -0.4905, -0.3650,  0.2622,\n",
       "         -0.1882, -0.3431, -0.2843, -0.1580, -0.1708, -0.2750, -0.1922,\n",
       "         -0.2921,  0.0467,  0.0859, -0.3875, -0.0824,  0.0237, -0.1860,\n",
       "         -0.1595, -0.2125,  0.1113, -0.1593, -0.1592, -0.0806, -0.4568,\n",
       "         -0.1680, -0.3154, -0.1774,  0.0129,  0.0477, -0.1288, -0.2992,\n",
       "          0.0897, -0.0033,  0.1856, -0.1415,  0.0033, -0.0295, -0.0758,\n",
       "         -0.2427, -0.1827, -0.1219,  0.0515, -0.1897,  0.1220, -0.0604,\n",
       "          0.2540, -0.0129,  0.1142,  0.1390,  0.1857,  0.0733,  0.0223,\n",
       "         -0.0494, -0.0155, -0.1671, -0.1129,  0.2661,  0.0584, -0.0292,\n",
       "          0.2196,  0.0794, -0.0760,  0.1126, -0.0142, -0.0384,  0.2424,\n",
       "         -0.0458,  0.1300,  0.3742,  0.0935, -0.1290, -0.1260,  0.0073,\n",
       "          0.0246,  0.0382, -0.0706, -0.0511, -0.0870,  0.2234, -0.1491,\n",
       "         -0.2844,  0.1038,  0.2588, -0.0801,  0.0385, -0.0010,  0.0356,\n",
       "         -0.0143, -0.1007,  0.1719, -0.1314, -0.0134, -0.2978,  0.1419,\n",
       "         -0.1134, -0.0323, -0.0901,  0.2314,  0.0016, -0.2449, -0.2433,\n",
       "         -0.0474,  0.1668, -0.0422, -0.1332,  0.0067,  0.0575, -0.2588,\n",
       "          0.0086, -0.1569, -0.0099,  0.0321,  0.0724, -0.1848,  0.1058,\n",
       "          0.2713, -0.3025, -0.0961,  0.2549,  0.2176, -0.1721, -0.0678,\n",
       "          0.0252, -0.1858,  0.0798,  0.0788, -0.0710,  0.2221, -0.0587,\n",
       "         -0.0173,  0.1153,  0.0188,  0.2253,  0.1189, -0.0360, -0.0538,\n",
       "         -0.1093,  0.0390,  0.1496,  0.0036, -0.0804,  0.1095,  0.0297,\n",
       "         -0.1516, -0.0125,  0.1206, -0.0616,  0.0548,  0.1011,  0.1453,\n",
       "         -0.1069,  0.0327,  0.0710, -0.1283,  0.0689, -0.0834, -0.0140,\n",
       "         -0.1176, -0.0849, -0.0714,  0.0906, -0.0627, -0.0695, -0.0845,\n",
       "         -0.1382,  0.0307,  0.1721, -0.0268,  0.1106,  0.0476,  0.0602,\n",
       "         -0.0110, -0.0074,  0.0290, -0.1002, -0.0358,  0.0145,  0.1176,\n",
       "          0.0008, -0.1680, -0.0936, -0.0122, -0.1336, -0.0102,  0.1002,\n",
       "          0.0379,  0.0075, -0.1057, -0.1482,  0.0125,  0.0258, -0.0697,\n",
       "         -0.0451,  0.0970, -0.0996, -0.0424, -0.0539, -0.0627, -0.1312,\n",
       "         -0.2232,  0.1420, -0.0679, -0.1313,  0.0486, -0.1294, -0.0681,\n",
       "          0.1320, -0.0088,  0.0401,  0.2107,  0.0987, -0.1969,  0.0481,\n",
       "          0.0432,  0.0200,  0.0306, -0.0618,  0.0686, -0.1439, -0.0561,\n",
       "          0.0147, -0.2135, -0.1663, -0.0435,  0.0140,  0.0272,  0.0435,\n",
       "         -0.1195,  0.0526, -0.0112, -0.0483, -0.1127, -0.0240, -0.1306,\n",
       "         -0.2536, -0.0253, -0.1649, -0.3347, -0.2513, -0.2331, -0.2815,\n",
       "          0.0365, -0.2860,  0.0815, -0.0977, -0.0759, -0.1175, -0.1431,\n",
       "          0.0608, -0.0888,  0.1392,  0.1280,  0.0166, -0.2059, -0.0075,\n",
       "         -0.1158,  0.1013, -0.0269, -0.2219, -0.0363,  0.1028, -0.0006,\n",
       "          0.1591, -0.1230, -0.2629, -0.0884, -0.2573, -0.1579,  0.0884,\n",
       "         -0.0468, -0.3587, -0.0919,  0.0560, -0.0904, -0.0456, -0.0793,\n",
       "         -0.2642, -0.1725,  0.0535, -0.1614, -0.1975,  0.0778, -0.0490,\n",
       "         -0.0474, -0.2659,  0.0260, -0.1001, -0.1219, -0.1287, -0.1549,\n",
       "         -0.0994,  0.0449, -0.4436, -0.1754, -0.4135, -0.2249,  0.0662,\n",
       "          0.0107, -0.0234, -0.3503, -0.3833, -0.3288,  0.2352, -0.3826,\n",
       "         -0.1984, -0.1911, -0.0399, -0.3288,  0.0025,  0.0748, -0.0925,\n",
       "         -0.0278,  0.0307, -0.3745, -0.3486,  0.0615, -0.1571, -0.2715,\n",
       "          0.0004,  0.0229, -0.0684, -0.1753, -0.1639, -0.3913, -0.0568,\n",
       "         -0.0500, -0.3904, -0.0456,  0.0403, -0.0428, -0.0448,  0.1027,\n",
       "         -0.0461], device='cuda:0'), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [[-4.9957, -5.0493,  4.8943,  ..., -9.6665, -3.3142, -6.6056],\n",
       "         [ 9.4163, -0.9030, -1.7961,  ..., -1.9086, -7.8769, -6.1727],\n",
       "         [-9.8222, -1.4532, -3.8968,  ..., -7.3119, -1.6600,  7.5605],\n",
       "         ...,\n",
       "         [ 9.2948,  6.6328,  7.2446,  ...,  6.9955,  4.8313, -3.8831],\n",
       "         [ 6.2601, -5.4709, -7.0941,  ...,  7.8679, -2.9107, -0.3530],\n",
       "         [ 1.8559, -0.5101, -5.6463,  ...,  9.5111, -9.2415,  4.7246]], device='cuda:0'), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [[ 7.1951, -2.1992, -5.2030,  ...,  8.3474, -6.5150,  7.8907],\n",
       "         [-8.5412,  1.2654,  1.1435,  ...,  7.9462,  6.7444,  6.6016],\n",
       "         [ 1.9679,  6.2468,  2.5846,  ..., -7.3819, -1.9152,  7.7812],\n",
       "         ...,\n",
       "         [-1.4101, -6.5561,  0.1184,  ...,  2.9580,  9.0939,  9.8877],\n",
       "         [-6.5088, -4.2276,  1.2873,  ...,  5.6772, -4.6080, -2.4098],\n",
       "         [ 6.7238, -3.8762, -8.6834,  ...,  3.9128, -2.1742,  8.5389]], device='cuda:0'), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [-0.8584,  9.9270, -5.1127,  8.5558,  8.7461, -7.4120,  9.7643,\n",
       "         -4.1880,  7.2269, -9.0685, -2.4135, -8.9277,  9.5051, -5.0040,\n",
       "         -8.4353,  0.4425, -8.4910,  4.3618, -6.8033,  1.1184,  6.8683,\n",
       "          5.0428, -1.9263,  4.6503, -0.7769, -1.1145,  3.5286, -2.0654,\n",
       "         -4.6404, -2.0910,  5.5677,  1.9741, -2.2095,  4.0528, -7.1120,\n",
       "         -8.0313, -1.0063,  1.4740,  9.2399,  2.2588, -4.3819,  3.6229,\n",
       "         -9.3798, -4.3168, -2.1573, -4.9670,  5.2720,  0.5227,  4.2444,\n",
       "          7.5281, -3.8222, -7.3130, -7.1504, -1.5996,  0.4443, -2.1434,\n",
       "          3.2910, -5.7901, -0.8647, -8.9355, -6.2242, -3.2976,  7.5097,\n",
       "         -0.5212, -8.6226,  4.5219,  3.6492,  2.7357, -6.1787,  4.2176,\n",
       "          7.3092, -2.3681, -5.5308,  8.5563, -3.6006, -5.8223, -7.8686,\n",
       "          9.9829,  0.4467, -4.1343,  9.1240, -1.3707,  4.1270, -8.4779,\n",
       "         -5.4934,  7.9779,  3.8203, -5.9489, -9.5673, -4.4093, -9.7901,\n",
       "          7.2212, -4.2925,  8.4999, -0.1979, -5.4787, -1.2281,  7.5031,\n",
       "         -1.8212, -5.4545, -1.1825,  8.1973, -7.3934, -9.3735, -8.2557,\n",
       "          9.5907,  0.0094,  6.8170, -7.2418,  8.9871, -6.3777,  8.4697,\n",
       "         -1.3506, -3.4803,  0.3572, -9.8659,  9.7725,  2.3446, -7.6184,\n",
       "         -0.0759,  7.8898,  2.8782, -2.0830,  5.4369, -1.8430,  9.8200,\n",
       "         -1.7475, -5.3223, -6.4997,  9.5100, -1.6407,  5.2253, -8.1110,\n",
       "         -8.2790, -8.4360,  8.5427,  1.3312, -2.0169,  8.2265, -3.4138,\n",
       "          3.5244, -8.7929, -4.3564,  4.6698,  9.8563, -4.4161, -2.6785,\n",
       "          4.7352, -2.4356,  2.7931,  1.3804,  5.6269,  4.1216,  1.8598,\n",
       "         -2.9473,  6.4562, -6.9617,  4.0911,  0.6041, -9.6858, -4.2259,\n",
       "          6.1663, -1.9740, -6.2587,  6.5328, -7.0592, -1.9064,  4.6794,\n",
       "         -5.1457,  4.9028, -1.1730, -2.4686, -5.4393, -2.8796, -4.4212,\n",
       "         -6.1490, -0.0137,  5.8060, -0.2571,  4.9346,  4.3863, -0.5795,\n",
       "         -8.1028,  1.3572, -0.2643,  8.8116, -5.6071, -1.6906, -7.9218,\n",
       "          1.8240, -3.3683, -0.8844,  6.0641, -1.5149,  2.4480, -8.0748,\n",
       "         -9.9991,  3.1418,  6.6258,  6.6944,  6.2663,  7.4268,  5.2436,\n",
       "         -3.2244, -2.7986,  1.3372,  8.6857,  3.0865, -9.6401, -5.4483,\n",
       "          7.7323,  4.7834,  3.1930,  1.2575, -2.0181,  1.4608, -6.0763,\n",
       "          5.6025, -9.5956,  9.5244,  2.0512,  4.4422, -1.3190, -6.3544,\n",
       "         -5.9039,  5.1397,  6.8875, -4.9423,  7.4699, -0.7369, -0.8023,\n",
       "         -0.3062,  5.0479,  3.2547,  5.5197, -7.4828, -1.4201,  8.7153,\n",
       "          4.0514, -2.0695,  1.5118, -0.1022,  6.7395,  8.1976,  8.6128,\n",
       "         -7.6969, -3.0561,  7.8595,  1.7298, -5.8619, -5.1970,  3.5213,\n",
       "          1.5628, -8.5722,  0.8499,  0.9299, -9.2672, -6.6466,  2.9761,\n",
       "         -2.3238,  3.9721, -4.2762,  8.4082,  8.0187, -8.2546, -1.4901,\n",
       "         -9.6888, -5.9214, -7.6241, -5.8389,  3.5223, -8.0277, -4.0994,\n",
       "         -1.2968,  2.3608,  5.4769,  9.6585, -4.0144,  4.2121,  3.2430,\n",
       "          2.7585, -3.2207, -5.2774, -8.0872, -2.1439, -2.9521, -7.6086,\n",
       "         -4.2926, -7.3532,  5.9503,  9.7962, -7.5407, -6.4954,  7.1314,\n",
       "          7.5853, -7.9329, -1.6584,  6.6482, -9.9054, -7.6136, -6.4350,\n",
       "         -8.0522, -6.8536, -7.6750, -2.5771, -7.4409, -3.6152, -5.1710,\n",
       "          6.0902,  8.6535,  0.2933,  1.9100, -3.7132,  8.3516,  5.5671,\n",
       "         -0.7182, -7.3598, -6.9991, -5.5458,  5.7658, -9.9031,  2.7330,\n",
       "          5.5999, -3.4931,  1.7174, -1.8904,  1.7291,  2.0705,  8.7542,\n",
       "          7.8828,  7.1231,  0.6042,  3.1300,  7.5735,  7.6466, -9.6220,\n",
       "         -6.4480, -5.8253, -5.1354,  7.5797,  5.2985, -8.6765, -5.9898,\n",
       "         -4.8250,  6.3804,  1.8531,  3.9604, -5.1613, -8.2137, -9.1147,\n",
       "         -4.6804,  2.2776, -6.3003, -2.1330,  7.9287, -6.6873, -9.9689,\n",
       "          3.4856, -6.4544, -3.9784,  6.2652, -7.1402, -4.1390,  2.8277,\n",
       "          2.7651,  5.9869,  8.8012,  2.7921,  2.4421,  7.8807,  2.1834,\n",
       "         -2.7351,  2.4268, -3.5439,  8.6126,  7.3345, -0.8446, -0.5434,\n",
       "          0.5490,  7.3999, -8.8253,  4.6585,  8.6014, -3.1130,  0.1467,\n",
       "          2.9439, -4.9898,  6.8348,  6.8688, -4.6490, -3.4751, -1.7681,\n",
       "         -7.3612,  0.5028, -7.6583, -6.9232, -2.8801,  1.5934, -5.4184,\n",
       "          3.2050], device='cuda:0'), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [-7.2072,  7.3303, -8.7610, -9.1442,  8.8351,  3.3302,  2.2124,\n",
       "         -3.8361,  7.0364,  1.3697,  8.4967, -2.1301,  6.3535, -4.8746,\n",
       "          8.1279,  9.1119,  3.5925, -8.1315,  7.3222,  6.7298,  1.1059,\n",
       "         -5.6797, -2.3465,  3.1233, -6.8131, -0.3283,  9.6497, -4.1140,\n",
       "          3.8767, -3.7059,  9.8005, -5.5189,  1.9639,  0.0457, -8.6894,\n",
       "         -4.2934, -2.9530,  8.9667,  1.7518, -6.5270, -6.8530, -2.3651,\n",
       "          5.0536, -4.7441, -5.6697,  8.8736,  0.9631,  1.7823, -7.0596,\n",
       "         -0.9723,  7.6802, -5.9674, -0.4604,  1.8573, -7.1102,  7.9371,\n",
       "          1.5568,  7.1713,  4.3042,  0.8472,  7.3925,  1.3600, -7.3043,\n",
       "         -9.1437, -6.3277, -7.2554,  8.1310, -2.3297, -5.4835, -1.8063,\n",
       "         -1.1593, -5.6944, -2.2266,  6.8362,  6.7661,  3.6391,  4.1578,\n",
       "          7.3344,  9.0514,  4.2033,  7.5645,  7.8410,  0.8344,  4.6851,\n",
       "         -3.5981, -2.6358,  9.7144,  3.8008, -5.0403,  1.2724, -7.8634,\n",
       "         -4.0393,  9.3686, -7.4668, -3.1013,  5.3439, -3.5585,  9.1920,\n",
       "          3.9301,  6.5536, -9.4275,  6.4082,  5.6995, -3.4315,  9.8644,\n",
       "         -3.4523, -8.7353,  1.4212,  9.6139,  6.3623,  9.8158, -9.2468,\n",
       "         -2.8094, -3.8785, -8.3632, -9.1223, -1.2587,  4.6679,  7.7212,\n",
       "         -8.9563,  1.9814, -4.8819, -0.2091,  9.9305, -1.2085,  4.7667,\n",
       "         -7.0617, -9.8463, -1.5636, -0.1088,  3.0518, -0.4769, -6.2081,\n",
       "          4.5794,  1.3589, -1.0278, -2.3794, -6.3520,  1.8490, -2.1229,\n",
       "         -1.2676,  4.0944,  0.1985,  6.2731, -2.6658,  6.2398,  2.3387,\n",
       "          9.8692, -3.4971,  6.2453,  3.3140, -8.7536,  9.3421,  1.3837,\n",
       "         -3.0878,  7.3781, -2.6305,  8.7149,  4.1507, -0.5210,  9.3099,\n",
       "         -5.7583, -2.3409, -1.9285,  0.1755,  7.5030, -2.4323, -4.8324,\n",
       "         -9.2613,  5.5427,  5.9776, -5.0203, -0.6765,  0.1997, -9.1651,\n",
       "          7.0238, -7.4754, -2.8875, -4.8639,  4.8454, -3.5600,  0.8732,\n",
       "          7.0082, -1.3825,  2.6321, -5.7561,  4.7034,  7.0775,  9.2685,\n",
       "          5.4381,  7.1379,  0.0996, -8.3596,  5.0618, -9.2217,  0.3104,\n",
       "          8.8106, -4.4334, -7.2989, -7.2445,  9.4186,  4.0164,  0.3951,\n",
       "         -3.1164,  6.4040, -1.9668, -8.4934, -6.7777,  5.0377,  5.6068,\n",
       "          4.6168,  0.2203, -0.6396, -1.3871,  7.9783,  7.1219, -2.0903,\n",
       "          4.7123,  4.4267,  6.0478,  1.6023,  5.9909,  2.2462, -9.3411,\n",
       "          6.9711, -7.4486,  2.0580,  6.5508,  2.7019,  2.0563, -8.2528,\n",
       "         -0.0231, -1.2642,  5.9334, -5.2609, -0.2190,  5.9556,  0.9455,\n",
       "          3.8549,  6.6791, -7.7295,  3.9073,  5.0381,  1.9867, -3.8474,\n",
       "          6.6672,  5.3235, -5.7530, -4.8426, -6.6473, -5.0496,  1.4717,\n",
       "         -8.8010,  2.9305,  8.4956,  7.6528, -0.0859,  1.2361, -6.5319,\n",
       "          9.1399,  5.3133, -4.3493,  7.7204,  9.5643, -7.4146,  4.9076,\n",
       "          1.0705, -1.0044, -6.2458,  0.8208, -0.6782, -0.2929, -6.5209,\n",
       "          2.7549,  2.9378,  0.6514,  8.9202,  1.3379, -5.9792,  8.2582,\n",
       "          1.9566,  4.9640, -3.8827, -7.9123, -8.1519,  2.3266,  5.7375,\n",
       "         -6.8914, -7.1692,  6.7277, -3.7751, -0.3152,  7.2804, -3.1277,\n",
       "          0.5939, -1.2063, -2.5353, -1.0809, -3.7782, -3.3205,  5.3134,\n",
       "          6.4691,  6.7448, -2.0423,  6.3074,  5.5375,  3.3386, -4.2493,\n",
       "          0.8625,  3.6666, -2.4889, -7.6753, -8.3506,  4.3770, -0.0156,\n",
       "         -7.4798, -3.0190, -5.2787, -0.0167,  5.4043,  6.2373, -7.7994,\n",
       "          3.8294,  7.6538,  4.6504,  7.7404,  4.0857,  6.2723,  4.2667,\n",
       "         -1.1538, -1.9949, -4.4474, -8.5693,  8.4686, -2.6838, -5.3456,\n",
       "         -1.6491, -2.7309, -1.7734, -4.6247, -7.1821,  1.6037,  3.0778,\n",
       "          1.3982,  7.7993, -5.1071, -0.9103,  9.7604,  1.7663,  4.7298,\n",
       "         -2.6096, -3.2358, -2.1870, -1.5423,  2.0742,  9.3277, -9.2634,\n",
       "          8.2372, -2.3360,  7.7485,  8.3505, -3.8250, -0.8141,  5.3011,\n",
       "         -1.0220,  3.1489,  8.0967, -3.9889,  7.6268, -2.1349, -8.4346,\n",
       "         -7.2392, -3.4184, -3.2127,  3.2910, -2.4219,  0.9774,  9.0936,\n",
       "          3.1722,  2.7331, -2.7329, -6.7272,  4.3925,  7.9947,  0.7788,\n",
       "         -8.7669,  7.6677, -4.6093, -0.7233,  6.6011, -1.0628,  1.8514,\n",
       "         -5.0374,  9.7400, -7.1844,  4.1320, -7.5400,  6.5677,  7.0170,\n",
       "         -3.8935], device='cuda:0'), Parameter containing:\n",
       " tensor([[ 1.5664e-01, -6.8284e-03,  6.1502e-03,  ...,  5.5469e-02,\n",
       "           1.4354e-01,  2.1753e-02],\n",
       "         [ 1.0328e-01,  6.3458e-03, -1.9898e-01,  ...,  6.1273e-02,\n",
       "           1.6829e-02,  2.6659e-02],\n",
       "         [ 8.2695e-02,  4.0631e-02,  4.9101e-03,  ...,  5.1882e-02,\n",
       "          -9.7956e-02, -1.4019e-02],\n",
       "         ...,\n",
       "         [ 7.4687e-02,  3.6561e-02, -2.9677e-02,  ...,  1.7475e-01,\n",
       "          -4.3503e-02, -4.3620e-03],\n",
       "         [-3.8649e-02,  6.1502e-03, -3.2995e-02,  ...,  1.7429e-01,\n",
       "           7.5938e-02, -3.4448e-02],\n",
       "         [-3.1680e-02,  1.1081e-01,  5.4665e-02,  ...,  1.1841e-01,\n",
       "           6.6343e-02,  1.5691e-01]], device='cuda:0'), Parameter containing:\n",
       " tensor([-0.1549,  0.0265, -0.1931, -0.2429, -0.0498, -0.1424,  0.1270,\n",
       "         -0.1163,  0.2247,  0.0952, -0.1402,  0.0707, -0.1193, -0.1098,\n",
       "         -0.1635, -0.0086, -0.1350, -0.1402, -0.1907, -0.0607, -0.0415,\n",
       "         -0.2133, -0.1228,  0.0511, -0.1784, -0.2105, -0.1844, -0.1779,\n",
       "         -0.0653, -0.1557, -0.1936, -0.1111, -0.0985,  0.0770, -0.0438,\n",
       "         -0.2129, -0.0465, -0.1969, -0.1879, -0.1397, -0.1151, -0.1470,\n",
       "         -0.0828,  0.2409, -0.2547, -0.1315, -0.1309, -0.2180, -0.2436,\n",
       "          0.2569, -0.1293,  0.1332, -0.0652,  0.2870, -0.0722, -0.0573,\n",
       "          0.0131, -0.1390, -0.0900, -0.1058,  0.0262, -0.0813, -0.0620,\n",
       "         -0.1408, -0.1273, -0.2026,  0.0468, -0.1473,  0.2830,  0.1372,\n",
       "         -0.0967,  0.0919, -0.0211,  0.2087, -0.1517, -0.0025, -0.1471,\n",
       "         -0.0803, -0.2073, -0.1209, -0.0517, -0.2654, -0.0587, -0.0367,\n",
       "          0.0066, -0.0828,  0.0388,  0.0533, -0.1347, -0.0844, -0.0708,\n",
       "         -0.1912,  0.1939, -0.1273,  0.2726,  0.2387, -0.1650,  0.0113,\n",
       "         -0.0467, -0.0748], device='cuda:0'), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [[ 0.4146, -8.4441, -3.0826,  2.4582, -0.4587, -0.0628,  2.8096,\n",
       "           2.3672,  0.8404,  1.1842,  3.8853, -1.0957,  0.4243,  2.0823,\n",
       "           9.0240, -2.1760, -3.3503,  1.7833,  0.7310,  2.3096,  0.2661,\n",
       "           2.2699, -5.5007, -0.7039, -0.4218,  5.0793,  3.7989, -0.5013,\n",
       "           1.4217,  1.7381, -0.4303, -3.0672,  2.5365,  2.4234, -0.6447,\n",
       "           6.0892, -1.3002,  3.6550,  1.0699,  3.1475, -0.4027, -1.7155,\n",
       "          -1.0107,  5.3080,  0.2202, -1.1568,  2.3035,  1.0411,  3.8576,\n",
       "           6.0603, -5.6294, -0.0180, -2.2154,  2.4389, -2.1695, -2.4953,\n",
       "          -1.2600, -3.2523, -4.7592,  0.1391, -1.0084, -0.8400,  0.8638,\n",
       "          -0.8084,  0.3082, -0.8391, -0.5370, -0.0061,  1.7405, -2.2378,\n",
       "           8.2812, -4.2280, -1.3480, -1.8045,  2.2987, -0.6899, -1.0022,\n",
       "           0.9105, -1.4484, -0.2518,  1.3777,  2.0165, -0.5966,  4.1308,\n",
       "           0.3310,  1.5228, -3.4218,  1.1810, -2.3387,  0.1940,  0.8357,\n",
       "           7.2190, -1.6834,  2.6440, -2.2024,  2.1649,  3.9298, -1.0599,\n",
       "           2.6036,  1.2978]], device='cuda:0'), Parameter containing:\n",
       " tensor(1.00000e-03 *\n",
       "        [ 6.8870], device='cuda:0'), Parameter containing:\n",
       " tensor([[ 6.6152e-02, -1.8886e-01,  9.2497e-02,  ..., -1.1338e-01,\n",
       "          -1.8325e-01,  1.4342e-01],\n",
       "         [ 3.3563e-03,  1.4016e-01,  1.7711e-02,  ..., -1.8139e-02,\n",
       "          -8.5948e-03, -4.8828e-02],\n",
       "         [-3.3653e-02, -7.3913e-02, -1.4906e-02,  ...,  1.0767e-01,\n",
       "          -5.4070e-02,  2.6714e-02],\n",
       "         ...,\n",
       "         [ 1.3132e-02,  3.4775e-02, -2.4523e-01,  ...,  6.4981e-02,\n",
       "           1.5122e-01,  3.1543e-02],\n",
       "         [-2.2761e-02,  2.1631e-01, -3.1844e-04,  ...,  2.0707e-01,\n",
       "           2.7315e-02,  2.5716e-01],\n",
       "         [-9.8522e-02,  1.3174e-01, -1.9508e-01,  ...,  3.1405e-02,\n",
       "          -2.8084e-02, -3.2283e-02]], device='cuda:0'), Parameter containing:\n",
       " tensor([[ 2.7571e-01,  1.3249e-01,  1.8396e-01,  ...,  1.8183e-01,\n",
       "           1.8790e-01,  1.1647e-01],\n",
       "         [-2.0452e-01,  1.8649e-01, -1.2937e-01,  ...,  9.8108e-02,\n",
       "          -1.7173e-01, -1.5209e-01],\n",
       "         [ 1.6587e-01,  1.7309e-01,  4.4611e-03,  ...,  2.2284e-01,\n",
       "          -1.5079e-01, -9.3916e-02],\n",
       "         ...,\n",
       "         [ 1.9876e-01,  7.5146e-02,  3.6568e-02,  ...,  1.8342e-01,\n",
       "           1.0242e-02,  4.6307e-02],\n",
       "         [ 1.7893e-02,  1.2187e-01, -7.0111e-02,  ...,  9.5230e-02,\n",
       "          -4.6810e-02,  1.4229e-02],\n",
       "         [ 8.7969e-03,  1.0113e-01, -1.8074e-01,  ...,  2.8628e-02,\n",
       "           1.8492e-02, -4.0182e-02]], device='cuda:0'), Parameter containing:\n",
       " tensor([-0.0835, -0.0668, -0.1130, -0.0506,  0.0210, -0.0910, -0.0741,\n",
       "          0.0649, -0.0295, -0.0417, -0.1388, -0.1553, -0.1702, -0.1924,\n",
       "         -0.0562,  0.0054, -0.0446,  0.0535, -0.0294,  0.0390, -0.0671,\n",
       "         -0.2241, -0.0816, -0.1473, -0.1597, -0.0582, -0.1488,  0.0767,\n",
       "          0.0604, -0.0402, -0.2876, -0.1132,  0.0234, -0.2229, -0.1837,\n",
       "         -0.0129, -0.1556, -0.2788,  0.0097, -0.1130, -0.1398,  0.0768,\n",
       "          0.0355,  0.1863, -0.1383, -0.0927, -0.0633, -0.1728, -0.0112,\n",
       "          0.0603, -0.0591,  0.0315, -0.1645,  0.1541, -0.0644, -0.0970,\n",
       "         -0.1570, -0.1128, -0.1653, -0.1507, -0.0039, -0.0084, -0.1266,\n",
       "         -0.1949, -0.2398, -0.1170, -0.0412, -0.0081, -0.0314, -0.0250,\n",
       "         -0.0654,  0.0346,  0.2037, -0.1361, -0.0106, -0.1578, -0.2801,\n",
       "         -0.0826, -0.1686, -0.1077,  0.0273, -0.1643, -0.2044,  0.0104,\n",
       "          0.0538,  0.0210,  0.0463, -0.0651, -0.1181, -0.2686,  0.0096,\n",
       "         -0.0742,  0.1502, -0.2540,  0.1209,  0.1387,  0.0519, -0.0308,\n",
       "         -0.0838,  0.0928,  0.3271,  0.1031, -0.0275,  0.0897, -0.1169,\n",
       "          0.0557,  0.0717,  0.1085,  0.1697,  0.0200,  0.0183,  0.2704,\n",
       "          0.2234,  0.1953, -0.0627,  0.1109,  0.0141,  0.0289,  0.3198,\n",
       "          0.0854, -0.0613, -0.0493,  0.2778, -0.0661,  0.1221,  0.0804,\n",
       "         -0.0409,  0.0714, -0.0055, -0.1581,  0.0511, -0.1660,  0.1660,\n",
       "          0.2525, -0.0611, -0.1408, -0.1146,  0.2758,  0.0011,  0.0471,\n",
       "          0.0609,  0.1392, -0.1195,  0.4400, -0.1885, -0.0721,  0.1604,\n",
       "         -0.0136,  0.1096,  0.2228,  0.0091, -0.1879, -0.0984,  0.0143,\n",
       "          0.1681, -0.0224,  0.0880,  0.2398,  0.0891, -0.0308, -0.1901,\n",
       "         -0.0222,  0.0173,  0.0639, -0.0294, -0.0337,  0.0806,  0.0867,\n",
       "         -0.1598,  0.2582,  0.0993,  0.0947, -0.1238, -0.0049, -0.0092,\n",
       "          0.0344,  0.0844, -0.2434,  0.0081,  0.0992,  0.0630,  0.0267,\n",
       "         -0.1123,  0.0907,  0.0781,  0.1614, -0.1287, -0.0519,  0.1827,\n",
       "          0.0719,  0.0016, -0.0720,  0.0354,  0.3606,  0.0106,  0.0595,\n",
       "          0.3325,  0.0087,  0.1952, -0.0965, -0.0425,  0.0545, -0.0155,\n",
       "         -0.0306, -0.0094, -0.0271,  0.1337, -0.0618,  0.0283, -0.1683,\n",
       "          0.0493,  0.0272, -0.0558,  0.0869,  0.2002, -0.0114,  0.1698,\n",
       "          0.0923,  0.0866,  0.0666, -0.0604, -0.0060,  0.0739, -0.0115,\n",
       "          0.1033, -0.0534,  0.1087, -0.0210, -0.0172, -0.0462, -0.0200,\n",
       "          0.0106, -0.0607,  0.1345, -0.0521,  0.0945,  0.0011,  0.0293,\n",
       "          0.0750, -0.1039, -0.0025, -0.0019, -0.0137, -0.0133, -0.1319,\n",
       "          0.0303,  0.0719,  0.0079, -0.0391, -0.0398,  0.1138, -0.0434,\n",
       "         -0.1815, -0.0972, -0.0405, -0.0875,  0.0255, -0.0320,  0.1090,\n",
       "         -0.1315,  0.0075, -0.0570,  0.0373,  0.0922, -0.1219, -0.1630,\n",
       "          0.1294, -0.0962,  0.0626, -0.1185,  0.0286,  0.0173, -0.1819,\n",
       "          0.0312, -0.1886,  0.1878, -0.0133,  0.0107,  0.1563,  0.0051,\n",
       "          0.1186, -0.2147,  0.0437,  0.0826, -0.0701, -0.0190, -0.0922,\n",
       "         -0.0824, -0.0646, -0.0357, -0.1409, -0.0718,  0.0122,  0.1798,\n",
       "         -0.1895, -0.1446,  0.0909,  0.0496, -0.1099,  0.0798,  0.1188,\n",
       "         -0.0786, -0.0536, -0.0020,  0.1683, -0.0428, -0.0790,  0.0204,\n",
       "          0.0801, -0.0314, -0.1555, -0.0441,  0.1233,  0.0068, -0.0408,\n",
       "          0.1334, -0.0628, -0.0237, -0.0206, -0.0565, -0.1441, -0.1211,\n",
       "         -0.0306, -0.0348, -0.1023,  0.0150, -0.1473,  0.0419, -0.0000,\n",
       "         -0.0556, -0.2258,  0.0034, -0.1189,  0.0622, -0.0070, -0.0974,\n",
       "         -0.0515,  0.2873,  0.1032, -0.1229, -0.0477, -0.1162, -0.1105,\n",
       "          0.3494, -0.0541,  0.0978,  0.1986, -0.1711, -0.0354,  0.1146,\n",
       "         -0.0801, -0.1277, -0.0020,  0.2975,  0.0623, -0.0049, -0.1136,\n",
       "         -0.0251, -0.0548, -0.1268, -0.0958,  0.0039,  0.0162,  0.0361,\n",
       "         -0.0648,  0.0687,  0.0024,  0.0250, -0.0473, -0.0184, -0.0278,\n",
       "          0.1214,  0.1318, -0.1318,  0.0677, -0.1015, -0.2278, -0.3221,\n",
       "         -0.1862,  0.1133, -0.0870, -0.0754, -0.1217,  0.1966,  0.0984,\n",
       "         -0.0014, -0.0718, -0.0114,  0.1023,  0.0264, -0.0689, -0.1961,\n",
       "          0.0499,  0.0463,  0.1177,  0.1098, -0.2535, -0.0240, -0.0629,\n",
       "         -0.0976], device='cuda:0'), Parameter containing:\n",
       " tensor([-0.0339, -0.0355, -0.1282, -0.1615, -0.0219, -0.0273, -0.0199,\n",
       "         -0.0040,  0.0992, -0.1730, -0.0287, -0.2564, -0.1598, -0.3234,\n",
       "          0.0418, -0.0229, -0.1266, -0.0760, -0.0526, -0.0387, -0.2085,\n",
       "         -0.0880, -0.0427, -0.1694, -0.1039, -0.1368, -0.0781,  0.0513,\n",
       "         -0.0872, -0.1710, -0.2953, -0.1332,  0.1114, -0.2038, -0.1840,\n",
       "          0.0942, -0.0953, -0.1898, -0.0111, -0.0828, -0.1539, -0.1152,\n",
       "          0.0135,  0.1875, -0.0167, -0.1710, -0.0890, -0.0514, -0.1233,\n",
       "          0.2076,  0.0222, -0.0882, -0.0870,  0.1424,  0.0036, -0.1182,\n",
       "         -0.2131, -0.1303, -0.1597, -0.2409, -0.0295, -0.0879, -0.2363,\n",
       "         -0.1905, -0.3990, -0.0551, -0.0325, -0.1545, -0.0273, -0.0229,\n",
       "         -0.0624,  0.0820,  0.1627, -0.0038, -0.0926, -0.1574, -0.1909,\n",
       "         -0.0799, -0.2278, -0.2039,  0.0756, -0.0909, -0.0929, -0.0449,\n",
       "          0.0232,  0.0758, -0.1026,  0.0643, -0.1495, -0.2220, -0.0731,\n",
       "         -0.1866,  0.0987, -0.0784,  0.1234,  0.1481, -0.0779, -0.1568,\n",
       "         -0.1431,  0.0115,  0.2244,  0.1054, -0.0427,  0.0016, -0.1425,\n",
       "         -0.0014, -0.1147,  0.1554,  0.0802, -0.0270,  0.0788,  0.2106,\n",
       "          0.0617,  0.2061, -0.0267,  0.1572,  0.0025,  0.1446,  0.3064,\n",
       "         -0.0664, -0.0955, -0.0650,  0.1492, -0.0780,  0.1072,  0.0000,\n",
       "         -0.0448,  0.1861, -0.0285, -0.1377,  0.0341, -0.1011,  0.0885,\n",
       "          0.1604,  0.0422, -0.1835, -0.1524,  0.3426, -0.0726,  0.0848,\n",
       "         -0.0283,  0.1632, -0.0093,  0.3915, -0.2217, -0.1539,  0.0698,\n",
       "         -0.1847,  0.2026,  0.2356, -0.0505, -0.2091, -0.0890,  0.0789,\n",
       "          0.1253,  0.1384,  0.1094,  0.2571,  0.1409, -0.0648,  0.0054,\n",
       "         -0.0542, -0.0005, -0.0065,  0.0479,  0.0421,  0.1251,  0.1208,\n",
       "         -0.0981,  0.1610,  0.0454,  0.1172, -0.0392,  0.0246,  0.0329,\n",
       "         -0.0126,  0.1619, -0.1638,  0.0343,  0.0448,  0.1606, -0.0124,\n",
       "         -0.0638,  0.1279,  0.1022,  0.2114, -0.0880,  0.0337,  0.1543,\n",
       "          0.1895,  0.0477,  0.0541,  0.0525,  0.3126, -0.0416,  0.0288,\n",
       "          0.1999,  0.1511,  0.0265, -0.1179,  0.0016,  0.0207, -0.1030,\n",
       "          0.0338,  0.0258,  0.0434,  0.0369,  0.0599, -0.0488, -0.1139,\n",
       "         -0.0118,  0.0318, -0.1492,  0.0702,  0.0991, -0.0769,  0.1763,\n",
       "          0.0722,  0.0846, -0.0794, -0.1657,  0.0421,  0.0222, -0.0956,\n",
       "         -0.0473,  0.0223,  0.1273,  0.0287, -0.1462, -0.0846, -0.0273,\n",
       "         -0.0107, -0.0145,  0.0485, -0.0250,  0.1885, -0.0041, -0.0194,\n",
       "          0.1103,  0.0909,  0.0620,  0.0117,  0.0559, -0.0358, -0.1804,\n",
       "          0.0794, -0.0454,  0.1065, -0.0818,  0.0561,  0.1102, -0.0090,\n",
       "         -0.0763, -0.1917,  0.0162,  0.0201, -0.0626, -0.0750,  0.1736,\n",
       "         -0.0152, -0.0561, -0.1804, -0.0530,  0.0255, -0.0747,  0.0052,\n",
       "          0.0684, -0.0598, -0.0195, -0.0364,  0.0376,  0.1101, -0.1100,\n",
       "          0.0032, -0.0416,  0.0361,  0.0930,  0.0901,  0.1792,  0.0022,\n",
       "          0.1461, -0.1638, -0.1446, -0.0759,  0.0809,  0.0719, -0.1399,\n",
       "          0.0444,  0.0295,  0.0835, -0.0776,  0.0455,  0.0801,  0.0855,\n",
       "         -0.1490, -0.0830,  0.1746,  0.0917, -0.1179, -0.0355, -0.0319,\n",
       "         -0.0058, -0.1076, -0.1137,  0.1072, -0.0463, -0.0412,  0.0013,\n",
       "          0.1119, -0.0475, -0.1896, -0.0423, -0.0178,  0.0186,  0.1122,\n",
       "         -0.0300, -0.0117, -0.0325, -0.0442, -0.1884, -0.0545, -0.0732,\n",
       "         -0.0995, -0.1463, -0.1481,  0.0021, -0.1531,  0.1160,  0.1194,\n",
       "         -0.1252, -0.2774, -0.0698, -0.0904,  0.0332,  0.0166, -0.0683,\n",
       "         -0.1897,  0.3555,  0.0179, -0.0300, -0.0929, -0.0032, -0.0081,\n",
       "          0.2686, -0.1401,  0.0459,  0.0915, -0.1472,  0.0098,  0.0788,\n",
       "         -0.1280, -0.1759,  0.0345,  0.2773,  0.0802,  0.0579, -0.2540,\n",
       "         -0.0476, -0.1089, -0.0603, -0.2419,  0.1171, -0.1456,  0.0199,\n",
       "         -0.1226, -0.0477,  0.0247,  0.0520,  0.0627,  0.0842, -0.1716,\n",
       "          0.0905,  0.1163, -0.1830,  0.1084,  0.0281, -0.1351, -0.2196,\n",
       "         -0.0840, -0.0734,  0.0572,  0.0085, -0.0656,  0.1499,  0.1374,\n",
       "         -0.0265, -0.0888,  0.1013,  0.0547, -0.0324,  0.0597, -0.1917,\n",
       "          0.0186, -0.1374,  0.1686,  0.1596, -0.2692, -0.0522, -0.1525,\n",
       "         -0.0739], device='cuda:0'), Parameter containing:\n",
       " tensor([[ 0.1695,  0.1449, -0.3031, -0.1435, -0.0430, -0.1666,  0.1173,\n",
       "           0.1057,  0.0319, -0.1304,  0.0780,  0.0077, -0.1695,  0.1127,\n",
       "           0.2499,  0.1050, -0.0028,  0.1803,  0.2912,  0.2915,  0.0094,\n",
       "           0.0269,  0.2253, -0.1358,  0.2853, -0.1075,  0.1946, -0.2069,\n",
       "           0.3142, -0.1680, -0.0291,  0.0460, -0.1066,  0.1215, -0.1509,\n",
       "           0.2812, -0.0644,  0.1471,  0.0202, -0.0519,  0.1905, -0.0435,\n",
       "           0.1816, -0.0792, -0.2327,  0.0523,  0.0204,  0.1153, -0.2321,\n",
       "           0.0125,  0.2342,  0.1129, -0.1639, -0.0487,  0.0150, -0.1043,\n",
       "           0.0031, -0.1225,  0.2054, -0.0663, -0.0600, -0.1887, -0.2011,\n",
       "           0.2013,  0.1238, -0.2110,  0.1733, -0.1457, -0.0538,  0.0938,\n",
       "           0.1508,  0.0861, -0.1587,  0.0981, -0.2002,  0.1961,  0.2895,\n",
       "           0.0466,  0.1167, -0.1384,  0.1389, -0.1963,  0.0558,  0.1617,\n",
       "           0.2120, -0.0352,  0.0896,  0.0201,  0.0371,  0.0264, -0.2224,\n",
       "           0.1761,  0.0250,  0.2400, -0.2249, -0.0255,  0.4587,  0.1465,\n",
       "          -0.1993,  0.1625],\n",
       "         [-0.1497, -0.2289,  0.1297, -0.0786, -0.0703,  0.2411, -0.0807,\n",
       "          -0.1581, -0.0134,  0.0900, -0.0860, -0.0353, -0.0153, -0.0136,\n",
       "          -0.1905, -0.0465, -0.0355, -0.2174, -0.1155, -0.1671,  0.0025,\n",
       "          -0.0115, -0.1891,  0.1119, -0.1093,  0.0461, -0.0861,  0.0631,\n",
       "          -0.2825, -0.0289,  0.0313,  0.1487,  0.0538, -0.1667,  0.0340,\n",
       "          -0.2508,  0.0607, -0.1064, -0.1020,  0.0136, -0.0844,  0.1588,\n",
       "          -0.1997, -0.0048,  0.0890, -0.0507, -0.0124, -0.0876,  0.0811,\n",
       "          -0.0430,  0.1563, -0.0867, -0.1276, -0.0339, -0.0460,  0.2203,\n",
       "           0.0342,  0.0435, -0.1538,  0.0250, -0.1913,  0.2089, -0.0469,\n",
       "           0.0182, -0.0850,  0.3199,  0.0328, -0.0284, -0.0594, -0.1906,\n",
       "          -0.1038,  0.0314,  0.0250, -0.1226,  0.1040, -0.1419, -0.0775,\n",
       "           0.0127, -0.0694,  0.0589, -0.0881,  0.1907, -0.1008, -0.2214,\n",
       "          -0.0496,  0.1389,  0.0493, -0.0560, -0.1161,  0.0372,  0.1029,\n",
       "          -0.1347,  0.0234,  0.1099,  0.0993, -0.0610, -0.1620, -0.0319,\n",
       "           0.1564, -0.0751],\n",
       "         [-0.0964,  0.1354,  0.1468,  0.1818,  0.0716,  0.1860, -0.1179,\n",
       "          -0.1804, -0.0704,  0.1261, -0.1144,  0.2080,  0.1644, -0.2192,\n",
       "          -0.0115, -0.2551, -0.1044, -0.0936,  0.1564, -0.2220, -0.0539,\n",
       "          -0.0678, -0.0815,  0.1079, -0.4812,  0.1617, -0.0692,  0.2413,\n",
       "          -0.0781,  0.0087, -0.0300, -0.0993,  0.1362,  0.0987,  0.1387,\n",
       "          -0.0913,  0.1752,  0.1434,  0.0996,  0.0643, -0.1359, -0.0489,\n",
       "          -0.1408, -0.1236,  0.0719,  0.0611,  0.0436, -0.0614,  0.1965,\n",
       "          -0.1436, -0.2046, -0.1433,  0.1391,  0.0474,  0.0767,  0.0315,\n",
       "          -0.0525, -0.1739, -0.0806,  0.0600,  0.1428,  0.2196, -0.1300,\n",
       "          -0.1279,  0.0252,  0.2039, -0.1305,  0.1668, -0.1288, -0.0670,\n",
       "          -0.0664, -0.2371,  0.0251, -0.0866,  0.0565, -0.1058, -0.3095,\n",
       "          -0.0481,  0.0046,  0.0459, -0.2968,  0.1691, -0.0481,  0.0519,\n",
       "          -0.0067, -0.0736,  0.0392, -0.1379,  0.1637, -0.2217,  0.0356,\n",
       "          -0.1312, -0.1332, -0.1334,  0.0207, -0.0349, -0.2838, -0.2273,\n",
       "           0.1379, -0.1213],\n",
       "         [-0.0495, -0.1096,  0.2075, -0.0275,  0.0625,  0.1253, -0.1849,\n",
       "          -0.1935, -0.0424,  0.1625,  0.0076,  0.0015,  0.0434, -0.0337,\n",
       "          -0.1385, -0.0182, -0.1462, -0.1463,  0.0484, -0.2046, -0.0444,\n",
       "           0.0389, -0.1969,  0.1984, -0.1519, -0.0241, -0.1418,  0.1259,\n",
       "          -0.2282,  0.0377,  0.0980,  0.1629,  0.0493, -0.0437, -0.0024,\n",
       "          -0.2259,  0.0985, -0.3200, -0.0512, -0.0390, -0.1417,  0.0949,\n",
       "          -0.2206, -0.0161,  0.0921, -0.0141, -0.0621, -0.0632,  0.2573,\n",
       "          -0.0736,  0.1975, -0.0563, -0.0478,  0.0391, -0.0322,  0.0374,\n",
       "           0.0809,  0.1850, -0.1690, -0.0043, -0.1885,  0.3556, -0.0336,\n",
       "          -0.0310,  0.0661,  0.2155,  0.0567, -0.0508, -0.0416, -0.2481,\n",
       "          -0.0974,  0.0355,  0.0912, -0.0902,  0.1348, -0.1500, -0.1884,\n",
       "          -0.0841, -0.0708,  0.0385, -0.0296,  0.2346, -0.0059, -0.1596,\n",
       "          -0.0302,  0.1096,  0.0265, -0.0291,  0.0549, -0.0048,  0.0960,\n",
       "          -0.1310,  0.0299,  0.1317,  0.1105,  0.0504, -0.2498,  0.0308,\n",
       "           0.0867,  0.0708],\n",
       "         [ 0.0315, -0.0511,  0.2673,  0.0484,  0.1023,  0.1302, -0.2708,\n",
       "          -0.0773, -0.0284,  0.1500, -0.0452,  0.0182, -0.0379, -0.0747,\n",
       "          -0.2895, -0.1031, -0.1219, -0.1333, -0.0034, -0.2206, -0.1041,\n",
       "          -0.0150, -0.1153,  0.1254, -0.2639, -0.0592, -0.1360,  0.0866,\n",
       "          -0.2413,  0.0107,  0.0779,  0.1660,  0.0350, -0.1266,  0.0244,\n",
       "          -0.3552,  0.2296, -0.2401,  0.0353,  0.0955, -0.2180,  0.1225,\n",
       "          -0.1907, -0.0178,  0.1629, -0.0280, -0.0778, -0.0380,  0.2098,\n",
       "          -0.0796,  0.1245, -0.0726, -0.1051,  0.0554,  0.0418,  0.0681,\n",
       "          -0.0658,  0.0760, -0.2437, -0.0425, -0.2830,  0.1914,  0.0357,\n",
       "          -0.1527,  0.0728,  0.3217,  0.1148, -0.0207, -0.0722, -0.1049,\n",
       "          -0.1455,  0.0219,  0.0848, -0.0910,  0.0848, -0.1758, -0.0666,\n",
       "          -0.0286, -0.0047,  0.0450, -0.0557,  0.2389,  0.1579, -0.0971,\n",
       "          -0.0477,  0.1212,  0.1438, -0.0131,  0.0492, -0.0531,  0.1809,\n",
       "          -0.2541, -0.0361,  0.1748,  0.0954, -0.0167, -0.2347,  0.0466,\n",
       "          -0.0255, -0.0300],\n",
       "         [-0.0319, -0.0502,  0.2423, -0.0701,  0.0386,  0.0707, -0.1329,\n",
       "          -0.1234, -0.0692,  0.0126, -0.0131,  0.0709, -0.0245,  0.0385,\n",
       "          -0.1082, -0.0015,  0.0263, -0.0891,  0.0242, -0.2138, -0.0408,\n",
       "           0.0554, -0.1624,  0.1876, -0.1349, -0.0718, -0.1110,  0.1061,\n",
       "          -0.1569,  0.0272, -0.0405,  0.1612,  0.0266, -0.0916, -0.0052,\n",
       "          -0.2484,  0.1049, -0.2306, -0.0152, -0.0078, -0.2035,  0.1020,\n",
       "          -0.1948, -0.0192,  0.1459, -0.0214, -0.0811, -0.1084,  0.3156,\n",
       "          -0.0590,  0.1636, -0.1241, -0.0980, -0.0261, -0.0267,  0.0796,\n",
       "           0.0946,  0.1973, -0.1159,  0.0528, -0.2026,  0.3335, -0.0736,\n",
       "          -0.0054,  0.0049,  0.1656, -0.0035, -0.0177, -0.0509, -0.2937,\n",
       "          -0.1443,  0.0759,  0.0798, -0.1669,  0.1191, -0.1386, -0.1668,\n",
       "          -0.0477, -0.0565,  0.0313, -0.0771,  0.1699,  0.0580, -0.1855,\n",
       "          -0.0502,  0.0968,  0.0059, -0.0159,  0.0327,  0.0238,  0.1976,\n",
       "          -0.2146,  0.0334,  0.1311,  0.0509, -0.0261, -0.2304, -0.0356,\n",
       "           0.1534,  0.0436],\n",
       "         [-0.0143, -0.1248,  0.2775,  0.0601,  0.1151,  0.0735, -0.1103,\n",
       "          -0.1418, -0.0298,  0.0296,  0.0294, -0.0277,  0.0263,  0.0037,\n",
       "          -0.2191, -0.0870, -0.1126, -0.0764,  0.0337, -0.1773, -0.1208,\n",
       "           0.0337, -0.1982,  0.1771, -0.1603, -0.0083, -0.2020,  0.0948,\n",
       "          -0.2282,  0.0426,  0.0118,  0.1474,  0.0711, -0.0802, -0.0315,\n",
       "          -0.2875,  0.0182, -0.2187, -0.0386, -0.0522, -0.2270,  0.0871,\n",
       "          -0.1166, -0.0008,  0.1785, -0.0486, -0.0758, -0.1180,  0.1673,\n",
       "          -0.0419,  0.2386, -0.0480, -0.0658,  0.0428, -0.0373,  0.0337,\n",
       "           0.0391,  0.2332, -0.1888,  0.0247, -0.1971,  0.3621,  0.0392,\n",
       "           0.0284,  0.0485,  0.2650, -0.0243, -0.1097,  0.0019, -0.1729,\n",
       "          -0.0689,  0.0528,  0.1117, -0.1196,  0.1045, -0.1002, -0.1708,\n",
       "          -0.0860, -0.0974,  0.0152, -0.0028,  0.2085,  0.0160, -0.1098,\n",
       "          -0.0159,  0.1082,  0.0050, -0.0213, -0.0415, -0.0236,  0.1239,\n",
       "          -0.2013,  0.0498,  0.0917,  0.1362,  0.0158, -0.2026, -0.0172,\n",
       "          -0.0055,  0.0304],\n",
       "         [-0.1598, -0.0759,  0.2464, -0.0815,  0.0302,  0.0263, -0.1487,\n",
       "          -0.1107, -0.0587, -0.0094, -0.0239, -0.0826, -0.0693,  0.0817,\n",
       "          -0.1960,  0.0902, -0.1135, -0.1608,  0.0715, -0.3222, -0.1086,\n",
       "           0.0158, -0.2264,  0.0994, -0.1903, -0.0869, -0.1217,  0.2029,\n",
       "          -0.2168,  0.0378,  0.0340,  0.1870,  0.0813, -0.0112, -0.0021,\n",
       "          -0.2135, -0.0179, -0.2402, -0.1193, -0.0644, -0.1591,  0.0020,\n",
       "          -0.1703, -0.1464,  0.2113, -0.0085,  0.1112, -0.0007,  0.2476,\n",
       "          -0.1974,  0.0570, -0.1453, -0.0813,  0.0191,  0.1692,  0.1509,\n",
       "           0.0047,  0.1743, -0.1980,  0.0340, -0.1030,  0.3650,  0.0409,\n",
       "           0.0427, -0.0708,  0.1828,  0.0750, -0.0974, -0.1443, -0.2646,\n",
       "          -0.1429,  0.0609,  0.0507, -0.1500,  0.1106, -0.0577, -0.1143,\n",
       "          -0.0384, -0.0468,  0.0449, -0.0913,  0.1851, -0.0315, -0.1196,\n",
       "          -0.1013,  0.2319, -0.0255, -0.0113,  0.0751, -0.0001,  0.1741,\n",
       "          -0.2949,  0.1456,  0.1027, -0.0386, -0.0852, -0.2334,  0.0067,\n",
       "           0.0705,  0.1358],\n",
       "         [-0.0852,  0.0421,  0.2073, -0.1010,  0.0823,  0.2903, -0.1004,\n",
       "          -0.1287, -0.0542,  0.0050, -0.0533, -0.0771, -0.1516,  0.1119,\n",
       "          -0.1235, -0.0893,  0.0233, -0.0956, -0.0862, -0.1382, -0.0355,\n",
       "           0.0308, -0.1761,  0.0783, -0.1130, -0.1805, -0.1206,  0.0901,\n",
       "          -0.2806, -0.0493, -0.0366,  0.2011,  0.1374, -0.0351,  0.1324,\n",
       "          -0.1236,  0.1422, -0.1882, -0.0116,  0.1269, -0.0673,  0.2848,\n",
       "          -0.1330, -0.0365,  0.0589, -0.1008,  0.0246, -0.0633,  0.1217,\n",
       "          -0.0498,  0.2228, -0.0977, -0.1235, -0.0596, -0.0398,  0.0590,\n",
       "           0.0353,  0.3803, -0.0705, -0.0215, -0.1402,  0.1786, -0.0977,\n",
       "          -0.0179, -0.2871,  0.2635, -0.0610, -0.0886, -0.0682, -0.1715,\n",
       "          -0.0959,  0.0741,  0.0106, -0.1254,  0.0053, -0.0456, -0.2100,\n",
       "          -0.0216, -0.0213,  0.2542, -0.0001,  0.0771, -0.1729, -0.1947,\n",
       "          -0.5916,  0.1338,  0.1153, -0.1049,  0.1361, -0.0072, -0.0056,\n",
       "          -0.2100, -0.0082,  0.2007,  0.0146, -0.0849, -0.2182, -0.0272,\n",
       "           0.0140, -0.0308],\n",
       "         [-0.0162, -0.0067,  0.2643, -0.0192,  0.0892,  0.0730, -0.0826,\n",
       "          -0.1294, -0.0143,  0.0775, -0.0387,  0.0223, -0.0519,  0.0047,\n",
       "          -0.2104, -0.0721, -0.1033, -0.0632,  0.0959, -0.2638, -0.0720,\n",
       "           0.0163, -0.1598,  0.1269, -0.1179,  0.0292, -0.1549,  0.1164,\n",
       "          -0.1736,  0.0195,  0.0645,  0.0737,  0.0466, -0.0403, -0.0012,\n",
       "          -0.2801,  0.0576, -0.2451, -0.0695, -0.0123, -0.1619,  0.1162,\n",
       "          -0.1625, -0.0173,  0.1720, -0.0308, -0.0162, -0.0140,  0.2139,\n",
       "          -0.0446,  0.1162, -0.0569, -0.0236,  0.0535, -0.0437,  0.0878,\n",
       "           0.0306,  0.1946, -0.1788, -0.0113, -0.1777,  0.3113, -0.0423,\n",
       "           0.0089,  0.0668,  0.2584,  0.0178, -0.0520,  0.0079, -0.1956,\n",
       "          -0.0535,  0.0150,  0.0798, -0.0285,  0.0620, -0.1109, -0.1080,\n",
       "          -0.0401, -0.0127,  0.0423, -0.1533,  0.2116, -0.0270, -0.1239,\n",
       "          -0.0470,  0.0683,  0.0394, -0.0086,  0.1501, -0.0367,  0.1523,\n",
       "          -0.2025,  0.0364,  0.1038,  0.0423,  0.0400, -0.2685,  0.0256,\n",
       "           0.1210,  0.0058]], device='cuda:0'), Parameter containing:\n",
       " tensor([ 0.1825, -0.0969, -0.0079, -0.0914, -0.0585, -0.1338, -0.0915,\n",
       "          0.0374, -0.0085, -0.0910], device='cuda:0'), Parameter containing:\n",
       " tensor([[-0.0458,  0.2878,  0.0940, -0.0386,  0.0326, -0.0205,  0.4178,\n",
       "          -0.0060,  0.1856,  0.3384, -0.0697, -0.1318, -0.0081,  0.0383,\n",
       "          -0.0852,  0.2155, -0.0359, -0.1068, -0.0318,  0.0784,  0.1037,\n",
       "          -0.1816, -0.1381,  0.2616, -0.0441,  0.3254, -0.0563,  0.0106,\n",
       "          -0.0259, -0.1899, -0.0046, -0.0037,  0.0435,  0.3442,  0.0015,\n",
       "          -0.0008, -0.0164, -0.0489, -0.3890,  0.0784,  0.0209,  0.0253,\n",
       "          -0.0539,  0.4848, -0.1225, -0.0133, -0.2033,  0.1408,  0.0411,\n",
       "           0.2114,  0.0647,  0.0773, -0.0252,  0.2326, -0.0185,  0.0227,\n",
       "           0.2281,  0.0592, -0.0159,  0.0032, -0.0702,  0.1283,  0.0360,\n",
       "          -0.0694,  0.0559,  0.2658,  0.0383,  0.0149,  0.2044,  0.1554,\n",
       "          -0.0088,  0.0685,  0.0887,  0.0839, -0.1413,  0.0052,  0.1028,\n",
       "          -0.0282,  0.0912,  0.1346,  0.2848, -0.1111,  0.1042, -0.0191,\n",
       "          -0.3897,  0.0071,  0.0522,  0.2284, -0.0377,  0.2932,  0.0045,\n",
       "          -0.0069, -0.0730, -0.0717,  0.0959,  0.2784, -0.1726,  0.2105,\n",
       "           0.0484, -0.0073],\n",
       "         [-0.0741,  0.0008,  0.0533, -0.0024, -0.0089, -0.0180, -0.0127,\n",
       "          -0.0484,  0.0097,  0.0194, -0.0379, -0.0332,  0.0892,  0.0017,\n",
       "           0.1011,  0.0270, -0.0249, -0.0090,  0.0186,  0.0230,  0.0322,\n",
       "          -0.0159,  0.0306, -0.0124, -0.0140, -0.0237,  0.0545,  0.0111,\n",
       "          -0.0149,  0.0226,  0.0208,  0.0261, -0.0161, -0.0009,  0.0061,\n",
       "          -0.0241, -0.0111,  0.0354, -0.0193, -0.0056,  0.0303,  0.0308,\n",
       "          -0.0424, -0.0055,  0.0187, -0.0226, -0.0380,  0.0477, -0.0321,\n",
       "           0.0076, -0.0881, -0.0544, -0.0170, -0.0386, -0.0035, -0.0039,\n",
       "          -0.0187, -0.0250,  0.0669,  0.0002, -0.0206,  0.0262, -0.0118,\n",
       "          -0.0492,  0.0011,  0.0061, -0.0143,  0.0075,  0.0308,  0.0427,\n",
       "          -0.0133, -0.0011,  0.0202, -0.0208, -0.0539, -0.0322,  0.0321,\n",
       "           0.0042,  0.0053, -0.0495, -0.0288, -0.0284, -0.0030, -0.0292,\n",
       "          -0.0224,  0.0292,  0.0455,  0.0478, -0.0154,  0.0687, -0.0427,\n",
       "          -0.1677, -0.0170,  0.0308, -0.0478,  0.0168, -0.0191,  0.0269,\n",
       "           0.0069, -0.0854],\n",
       "         [-0.0245,  0.0100,  0.0132, -0.0161, -0.0364, -0.0097, -0.0148,\n",
       "          -0.0085, -0.0121,  0.0324, -0.0379, -0.0112, -0.0101, -0.0036,\n",
       "          -0.0408, -0.0016, -0.0033, -0.0492,  0.0159,  0.0044,  0.0251,\n",
       "           0.0467, -0.0034, -0.0030,  0.0291,  0.0056, -0.0405, -0.0383,\n",
       "          -0.0143,  0.0134,  0.0120,  0.0332, -0.0401, -0.0103,  0.0079,\n",
       "          -0.0660,  0.0022, -0.0249, -0.0011, -0.0157, -0.0213,  0.0642,\n",
       "          -0.0352,  0.0041,  0.0526, -0.0145, -0.0500,  0.0541, -0.0138,\n",
       "          -0.0353, -0.0389,  0.0460,  0.0584, -0.0074, -0.0167, -0.0051,\n",
       "          -0.0411,  0.0129, -0.0061, -0.0173,  0.0093,  0.0399,  0.0060,\n",
       "          -0.0251, -0.0059,  0.0144, -0.0097, -0.0588, -0.0243, -0.0009,\n",
       "           0.0325, -0.0250, -0.0273, -0.0251, -0.0242, -0.0129,  0.0426,\n",
       "          -0.0165,  0.0281,  0.0360, -0.0134,  0.0119, -0.0391, -0.0125,\n",
       "          -0.0354, -0.0049,  0.0386,  0.0587, -0.0280, -0.0539, -0.0189,\n",
       "          -0.0417,  0.0131,  0.0406, -0.0411,  0.0338, -0.0544, -0.0095,\n",
       "           0.0258,  0.0373],\n",
       "         [-0.0154,  0.0288,  0.0021, -0.0458, -0.0483,  0.0065,  0.0381,\n",
       "          -0.0281,  0.0403, -0.0052, -0.0823,  0.0245,  0.0625, -0.0307,\n",
       "          -0.0312,  0.0183,  0.0147, -0.0638, -0.0451, -0.0352,  0.0473,\n",
       "           0.0080, -0.0209, -0.0293,  0.0606, -0.0065,  0.0066, -0.0237,\n",
       "          -0.0542,  0.0334, -0.0012,  0.0215, -0.0386,  0.0171,  0.0036,\n",
       "          -0.0281,  0.0034,  0.0395, -0.0052, -0.0040,  0.0364,  0.0092,\n",
       "          -0.0572,  0.0150,  0.0357,  0.0019, -0.0540,  0.0283,  0.0053,\n",
       "          -0.0366,  0.0183, -0.0547,  0.0305, -0.0417, -0.0016, -0.0128,\n",
       "          -0.0251,  0.0181, -0.0182, -0.0265, -0.0324,  0.0439, -0.0108,\n",
       "          -0.0232,  0.0007,  0.0171, -0.0019,  0.0195, -0.0086, -0.0157,\n",
       "           0.0090,  0.0196,  0.0039, -0.0347, -0.0200, -0.0034, -0.0080,\n",
       "          -0.0145,  0.0077, -0.0587,  0.0180,  0.0326,  0.0071, -0.0188,\n",
       "          -0.0227, -0.0040, -0.0104,  0.0103, -0.0049,  0.0255, -0.0149,\n",
       "           0.0097,  0.0308, -0.0175, -0.0160,  0.0343, -0.0155, -0.0155,\n",
       "           0.0211, -0.0109],\n",
       "         [-0.0490,  0.0044,  0.0621, -0.0278, -0.0198, -0.0178,  0.0156,\n",
       "          -0.0232,  0.0291, -0.0215, -0.0310,  0.0053, -0.0451, -0.0039,\n",
       "          -0.0085, -0.0105,  0.0147, -0.0520, -0.0169,  0.0009, -0.0999,\n",
       "           0.0118,  0.0287, -0.0264,  0.0549,  0.0207, -0.0663, -0.0227,\n",
       "           0.0180, -0.0027,  0.0127,  0.0439, -0.0309,  0.0389,  0.0111,\n",
       "           0.0176,  0.0054, -0.0107,  0.0070, -0.0171, -0.0402,  0.0888,\n",
       "          -0.0562,  0.0356, -0.0251,  0.0447, -0.0456,  0.0089, -0.0166,\n",
       "          -0.0427, -0.0227, -0.0524,  0.0079,  0.0050,  0.0065, -0.0020,\n",
       "          -0.0702,  0.0096, -0.0186, -0.0002,  0.0166, -0.0175, -0.0220,\n",
       "          -0.0013, -0.0515, -0.0023, -0.0310,  0.0260,  0.0518,  0.0327,\n",
       "          -0.0552,  0.0009, -0.0089, -0.0212, -0.0274, -0.0108, -0.0167,\n",
       "          -0.0076,  0.0724, -0.0140, -0.0085,  0.0502, -0.0081,  0.0212,\n",
       "          -0.0525,  0.0018,  0.0028,  0.0294, -0.0083,  0.0101, -0.0025,\n",
       "          -0.0262, -0.0037,  0.0130, -0.0282, -0.0599, -0.0469, -0.0339,\n",
       "          -0.0036, -0.0051],\n",
       "         [-0.0080, -0.0144,  0.0498, -0.0488, -0.0335, -0.0129,  0.0065,\n",
       "          -0.0190,  0.0024, -0.0205, -0.0634, -0.0187,  0.0779, -0.0172,\n",
       "          -0.0484, -0.0320,  0.0004, -0.0381,  0.0054,  0.0016, -0.0004,\n",
       "           0.0393,  0.0040, -0.0249, -0.0084,  0.0171, -0.0379, -0.0092,\n",
       "           0.0102,  0.0179,  0.0046,  0.0485, -0.0226,  0.0121,  0.0004,\n",
       "          -0.0653, -0.0008,  0.0248,  0.0006, -0.0372,  0.0297,  0.0456,\n",
       "          -0.0408,  0.0270,  0.0348, -0.0228, -0.0501,  0.0344, -0.0242,\n",
       "          -0.0295,  0.0041,  0.0497,  0.0217, -0.0108,  0.0036, -0.0072,\n",
       "          -0.0544,  0.0187,  0.0014, -0.0113, -0.0124,  0.0342, -0.0074,\n",
       "          -0.0194, -0.0014,  0.0146, -0.0311, -0.0411, -0.0532, -0.0141,\n",
       "           0.0201, -0.0062, -0.0245, -0.0149, -0.0312,  0.0064,  0.0345,\n",
       "          -0.0208, -0.0001,  0.0131, -0.0056,  0.0269, -0.0231, -0.0227,\n",
       "          -0.0389,  0.0057,  0.0151,  0.0742, -0.0409, -0.0003, -0.0209,\n",
       "          -0.0198,  0.0186, -0.0018, -0.0108,  0.0062, -0.0524, -0.0044,\n",
       "           0.0063, -0.0033]], device='cuda:0'), Parameter containing:\n",
       " tensor([ 0.0597, -0.3474, -0.3394, -0.2938, -0.2878, -0.3356], device='cuda:0'), Parameter containing:\n",
       " tensor([[-1.0078e-02,  9.5446e-03,  6.2616e-02, -1.1182e-02, -4.7371e-02,\n",
       "          -1.4901e-02,  2.5103e-02, -3.0699e-02,  5.4625e-02,  2.4308e-02,\n",
       "          -4.2970e-02, -3.5616e-02, -5.5790e-02, -2.3340e-02,  1.4106e-02,\n",
       "           3.4190e-03,  2.2590e-02,  1.9785e-03, -1.6205e-03,  4.4860e-02,\n",
       "           7.7362e-03, -3.2893e-03, -1.8335e-02,  1.2926e-02,  3.2947e-02,\n",
       "           2.7240e-02, -7.0845e-02,  3.9537e-02, -1.2536e-02, -2.8175e-02,\n",
       "           3.3123e-03,  2.2010e-02, -3.0523e-03,  1.7166e-02,  2.0537e-02,\n",
       "          -5.3031e-02,  7.1034e-04,  2.3548e-02, -1.7800e-02, -3.5838e-02,\n",
       "          -3.4242e-02, -9.3591e-02, -3.4699e-02,  4.0295e-03,  3.2417e-02,\n",
       "           3.0635e-02, -3.0232e-02,  2.0510e-02, -2.7986e-02, -1.0713e-02,\n",
       "          -4.1185e-02, -1.4338e-02, -1.2994e-02, -3.0896e-02, -2.4580e-02,\n",
       "          -2.4994e-02,  3.2759e-02, -3.9522e-02, -1.8994e-02, -2.9717e-02,\n",
       "          -1.1095e-02, -9.5170e-03, -3.3015e-02, -3.4875e-02, -5.6652e-03,\n",
       "           2.4463e-02, -1.9196e-02, -1.2669e-02, -2.0930e-02,  4.5092e-02,\n",
       "          -2.0492e-02, -2.9137e-02, -1.4113e-02, -5.2146e-02, -6.8138e-02,\n",
       "           6.0626e-04,  1.5348e-02,  1.1754e-02, -1.6328e-02, -6.7791e-02,\n",
       "          -1.7020e-02,  1.3620e-02,  5.8642e-03,  3.7171e-03, -1.5574e-02,\n",
       "           5.2389e-03,  4.3741e-02,  2.7129e-02, -3.4476e-02,  3.7717e-03,\n",
       "          -1.6613e-02,  1.5762e-02,  7.9891e-03, -1.6226e-02, -4.1111e-02,\n",
       "          -9.2933e-03, -4.6438e-02, -2.8291e-02, -2.7363e-03, -6.3630e-02],\n",
       "         [-1.4317e-02,  7.5954e-03,  1.0953e-01, -7.3714e-02, -2.8136e-02,\n",
       "          -4.6532e-02,  1.0091e-03, -2.6497e-02,  5.7265e-02,  2.8718e-02,\n",
       "          -3.1327e-02, -1.3462e-03,  2.3650e-02, -1.9931e-02, -8.5326e-02,\n",
       "           8.6511e-03, -1.9910e-02, -5.4441e-02, -2.4938e-02, -4.0184e-02,\n",
       "          -9.3289e-02,  2.0344e-02,  2.4254e-02, -7.3835e-03,  6.0207e-02,\n",
       "          -5.3808e-03, -6.5496e-02, -1.0942e-02,  7.8450e-02, -4.0907e-02,\n",
       "           1.0342e-02,  1.4785e-02, -2.8923e-02, -2.0233e-02,  1.2728e-02,\n",
       "          -6.4784e-03,  5.5469e-03, -4.3555e-03,  1.8142e-02, -3.7321e-02,\n",
       "          -4.0730e-02, -1.5373e-02, -5.8302e-02,  1.3744e-02,  6.9636e-03,\n",
       "           4.1842e-02, -3.8625e-02, -7.9377e-03, -4.9600e-02, -2.8503e-02,\n",
       "          -2.7784e-02, -2.1741e-02,  7.3695e-03, -4.3243e-02, -2.5954e-02,\n",
       "          -5.4314e-03, -1.7186e-02,  1.2416e-02, -3.8517e-02, -6.4982e-03,\n",
       "           3.6775e-02, -1.0568e-02,  7.5547e-03, -8.0178e-03, -5.3816e-03,\n",
       "           1.0536e-02, -2.2684e-02,  1.2238e-02,  3.2007e-03,  4.1206e-03,\n",
       "           2.0506e-02, -2.0359e-02, -1.9459e-02, -2.6675e-02, -3.8899e-02,\n",
       "          -2.8454e-02,  1.3524e-02,  5.5413e-03,  6.2557e-02,  2.5542e-02,\n",
       "           1.7322e-02,  6.4014e-02, -8.8229e-03,  4.3692e-02, -4.1911e-02,\n",
       "          -1.6475e-03,  1.1430e-02,  3.4128e-02, -1.9564e-02, -3.1548e-02,\n",
       "          -1.7819e-02, -3.5466e-02,  1.5121e-02,  2.7598e-02, -2.7352e-02,\n",
       "          -1.5747e-02, -3.9660e-02,  2.8559e-02, -3.0946e-03,  5.4376e-03],\n",
       "         [-9.4704e-02,  2.0357e-01,  8.8282e-02,  1.1419e-02, -1.0473e-02,\n",
       "           7.0150e-02,  4.6631e-01, -3.1619e-02,  2.4566e-01,  2.4364e-01,\n",
       "          -7.5306e-02, -5.8569e-02, -2.1544e-02,  4.2559e-02, -1.5762e-01,\n",
       "           1.5559e-01, -2.6044e-02, -7.3484e-02, -6.7568e-02,  4.9187e-03,\n",
       "           1.9182e-01, -1.3544e-01, -1.2158e-01,  1.4896e-01,  4.1352e-02,\n",
       "           3.2778e-01, -9.8634e-02,  8.5684e-02, -1.0635e-02, -1.3568e-01,\n",
       "           3.9785e-03, -8.8261e-02,  6.1907e-02,  2.6418e-01,  7.5452e-03,\n",
       "           2.3194e-02, -2.4008e-02,  1.3872e-02, -4.3064e-01,  7.6810e-03,\n",
       "          -9.9543e-03,  9.4440e-02, -4.8023e-02,  5.3740e-01, -1.1934e-01,\n",
       "           2.7972e-02, -2.2096e-01,  1.4980e-01, -2.1545e-03,  2.3657e-01,\n",
       "           5.8468e-02,  1.5299e-01, -5.5336e-02,  1.9291e-01,  7.4395e-03,\n",
       "           5.4525e-02,  2.1841e-01,  9.1086e-02, -6.6731e-02, -1.6547e-02,\n",
       "           1.9649e-02,  4.5176e-02,  2.6953e-03, -4.4749e-02, -1.6910e-02,\n",
       "           1.9079e-01,  1.7379e-01, -4.1740e-02,  3.9138e-02,  6.6996e-02,\n",
       "          -8.3402e-02,  1.1337e-01,  1.0231e-01,  1.5362e-01, -1.4120e-01,\n",
       "          -2.2904e-02, -2.7714e-03, -1.4946e-04,  2.6491e-02,  1.5447e-01,\n",
       "           2.9316e-01, -1.2835e-01,  1.0247e-01, -4.0964e-02, -3.3797e-01,\n",
       "           1.5517e-02,  3.7247e-02,  1.6107e-01,  2.3744e-06,  3.0726e-01,\n",
       "          -7.9512e-03, -3.8399e-02, -4.3837e-02, -7.6735e-02,  1.0723e-01,\n",
       "           2.9783e-01, -1.6718e-01,  2.0844e-01,  7.6753e-02, -2.7417e-02],\n",
       "         [ 2.0797e-03,  8.7273e-03,  5.4441e-02, -1.0116e-02, -3.2317e-02,\n",
       "          -3.6336e-02, -1.7049e-02,  2.3051e-04, -4.9856e-02,  3.9158e-02,\n",
       "          -1.1365e-02,  6.2205e-03, -8.1573e-02, -1.3423e-02,  9.3926e-02,\n",
       "          -1.3336e-02, -5.0157e-02, -7.0067e-02, -4.8287e-02,  4.1685e-03,\n",
       "          -2.0469e-02,  2.0712e-02, -8.2039e-03, -1.6942e-03, -4.9049e-02,\n",
       "          -4.8359e-02,  9.4133e-03,  2.3371e-02, -2.7998e-02,  5.8955e-03,\n",
       "          -1.7404e-02,  3.0175e-02, -4.8048e-02, -2.7946e-02, -1.5814e-02,\n",
       "          -2.7569e-02,  2.5414e-03, -2.7227e-02,  3.5754e-03,  2.3286e-02,\n",
       "           1.5129e-02,  7.4806e-02, -5.6728e-02,  2.3262e-02,  5.8595e-02,\n",
       "           1.9363e-02, -4.2709e-02,  4.2050e-02, -4.2938e-02,  1.7058e-02,\n",
       "           3.0708e-02,  5.7841e-02,  4.9139e-02, -5.6417e-02,  3.0916e-02,\n",
       "          -1.7449e-02,  1.0046e-02, -1.8479e-02,  4.8153e-02,  1.8822e-02,\n",
       "           4.1600e-03, -2.6850e-02,  1.2116e-03, -6.4683e-02,  2.0193e-02,\n",
       "           1.3604e-03, -3.0305e-02,  4.4701e-03,  1.8660e-02,  2.2310e-02,\n",
       "          -3.9834e-02, -1.2906e-02, -3.8528e-02, -1.5014e-02,  1.7713e-02,\n",
       "           8.2282e-04, -1.7815e-02, -2.7394e-02,  2.7631e-02,  4.4880e-02,\n",
       "          -4.5510e-03, -2.9569e-02,  3.8761e-02,  2.0007e-03, -1.0923e-02,\n",
       "           2.0676e-02, -4.0533e-03,  6.8702e-03, -8.6319e-02, -7.5938e-02,\n",
       "          -7.6543e-03, -9.4590e-02,  1.1568e-03,  1.1238e-02, -2.7466e-02,\n",
       "          -1.1135e-02, -4.8532e-02, -4.7584e-02, -2.3758e-02, -4.6437e-03],\n",
       "         [-4.8844e-02,  1.7994e-02,  4.5584e-02, -8.4395e-03, -5.2878e-02,\n",
       "          -1.0626e-02, -1.0332e-02, -3.0801e-02, -5.5024e-02,  3.2227e-02,\n",
       "          -4.8040e-02,  4.1595e-02, -7.1812e-03, -6.0727e-02, -1.0772e-01,\n",
       "          -1.9304e-02, -1.7155e-02, -3.7904e-02, -4.4030e-02,  3.8043e-03,\n",
       "          -5.0471e-03,  2.2981e-03, -1.9037e-02,  1.8832e-02,  7.0573e-03,\n",
       "           1.5215e-02, -4.5829e-02,  7.7078e-02, -7.8817e-02,  4.5835e-03,\n",
       "          -9.8856e-03,  1.7467e-02, -1.8000e-02, -1.5660e-02,  5.5208e-03,\n",
       "          -5.2091e-05, -6.2876e-03, -1.3221e-02, -3.1703e-02, -2.1229e-02,\n",
       "          -2.2189e-02,  7.6274e-03, -4.5468e-02,  1.4031e-02,  3.6977e-02,\n",
       "          -3.8491e-02, -4.4506e-02,  3.4152e-02,  2.4844e-02, -2.0106e-02,\n",
       "          -1.0768e-02, -3.6034e-02,  1.6153e-02,  2.9225e-02,  2.3487e-02,\n",
       "          -8.6661e-03,  2.5561e-02,  3.9853e-02, -4.0902e-03, -1.0491e-02,\n",
       "          -2.4140e-02,  1.7905e-02, -1.7770e-02, -5.0215e-03,  1.0440e-02,\n",
       "           1.1240e-02, -3.7295e-02, -8.0652e-03,  4.9021e-02, -1.6592e-02,\n",
       "           1.2853e-02,  7.1738e-03, -5.1797e-03, -4.6519e-02, -2.7853e-02,\n",
       "          -1.5519e-02,  1.5071e-02, -3.5817e-05,  5.6637e-02, -6.9979e-02,\n",
       "           8.9730e-03,  1.8350e-02,  6.2077e-02, -1.9238e-02, -1.6844e-02,\n",
       "           1.1249e-02, -9.0613e-03,  6.4132e-02, -2.7191e-02,  4.9827e-02,\n",
       "          -4.8952e-02, -6.4053e-03, -1.6986e-02,  1.0306e-02, -1.4668e-02,\n",
       "          -2.0166e-02, -3.8787e-02, -1.0295e-03,  1.9948e-02, -1.3980e-02],\n",
       "         [-2.8632e-02, -9.9790e-03,  5.7072e-02, -1.6340e-02, -5.4703e-02,\n",
       "          -1.9673e-02,  3.0212e-03, -3.7102e-02,  1.5302e-02,  5.0404e-03,\n",
       "          -8.9261e-03, -6.4316e-03, -6.4736e-02,  4.2841e-03,  1.7726e-02,\n",
       "          -2.1898e-02, -2.8045e-02, -5.4685e-02, -1.2931e-02,  4.7937e-02,\n",
       "          -3.8113e-02,  4.9645e-02, -8.3272e-04, -2.8844e-02, -2.1451e-02,\n",
       "          -1.1524e-03, -6.4762e-02,  6.0133e-03, -2.1792e-02,  5.0585e-02,\n",
       "           1.2897e-02,  9.6377e-03, -3.5552e-02, -2.0102e-02,  1.5967e-02,\n",
       "          -3.7319e-02,  1.3375e-03,  3.0559e-02, -1.6300e-02, -2.8164e-02,\n",
       "           4.0725e-02,  6.4033e-02, -3.0327e-02,  3.1334e-02,  4.3626e-02,\n",
       "           4.3979e-02, -5.5983e-02,  2.5946e-02, -3.1345e-02, -2.2926e-02,\n",
       "          -8.2606e-02,  3.1156e-02, -1.3829e-03, -1.2495e-03,  2.5133e-02,\n",
       "          -1.0443e-02, -7.4679e-02,  2.2727e-02, -2.9589e-03,  8.3834e-03,\n",
       "          -6.3867e-02, -1.5938e-02, -5.1173e-02, -2.7965e-02, -2.4738e-02,\n",
       "           2.1657e-02, -3.8437e-02, -5.4783e-02, -4.5512e-02, -2.8180e-02,\n",
       "           3.9000e-02, -3.1142e-02,  3.2978e-04,  4.0562e-03, -6.5233e-02,\n",
       "           3.5038e-04, -4.2859e-02, -2.8428e-03, -1.1010e-02, -1.3796e-02,\n",
       "          -2.0513e-02, -1.0230e-02,  1.5568e-02, -1.9846e-03, -4.9092e-02,\n",
       "           4.6618e-03,  8.4402e-03,  5.6821e-02, -3.5746e-02, -3.3992e-02,\n",
       "          -2.4357e-02, -1.4664e-01, -2.2291e-02, -1.6186e-02, -1.0697e-02,\n",
       "          -9.8023e-03, -6.3855e-02, -3.2097e-03, -8.7149e-03, -2.2567e-03]], device='cuda:0'), Parameter containing:\n",
       " tensor([-0.3300, -0.3018,  0.0986, -0.3642, -0.3438, -0.2723], device='cuda:0'), Parameter containing:\n",
       " tensor([[-0.0933,  0.3122,  0.0085, -0.0257,  0.0607,  0.1014,  0.4188,\n",
       "          -0.0070,  0.1231,  0.2971, -0.1369, -0.0178,  0.0769,  0.0989,\n",
       "          -0.0617,  0.2383, -0.0065, -0.0897, -0.0605,  0.1793,  0.0946,\n",
       "          -0.1145, -0.0887,  0.2564, -0.0480,  0.2407,  0.0243,  0.0164,\n",
       "           0.0792, -0.1236, -0.0528, -0.0328,  0.1036,  0.2804,  0.0002,\n",
       "           0.0247, -0.0401,  0.0021, -0.3987,  0.0455,  0.0290,  0.0197,\n",
       "          -0.0881,  0.5347, -0.0243,  0.1160, -0.2088,  0.1599,  0.0719,\n",
       "           0.2069,  0.0054,  0.2225, -0.0527,  0.1414, -0.0308,  0.0462,\n",
       "           0.1639,  0.1030, -0.0711, -0.0066,  0.0249,  0.0123, -0.0055,\n",
       "          -0.0350, -0.0699,  0.2774,  0.1329, -0.0591,  0.1745,  0.1025,\n",
       "          -0.1695, -0.0374,  0.0409,  0.1564, -0.1598, -0.0281,  0.0948,\n",
       "          -0.0031, -0.0106,  0.1206,  0.2703, -0.1341, -0.0416, -0.0295,\n",
       "          -0.3500,  0.0165, -0.0884,  0.2471, -0.0162,  0.2913,  0.0403,\n",
       "           0.0483, -0.1596, -0.1133,  0.0753,  0.3260, -0.1410,  0.1425,\n",
       "           0.0998, -0.1021],\n",
       "         [-0.0361,  0.0191, -0.0164, -0.0099, -0.0368, -0.0320, -0.0053,\n",
       "          -0.0142,  0.0540,  0.0139, -0.0362,  0.0276, -0.0368, -0.0242,\n",
       "           0.0615, -0.0491, -0.0370, -0.0338, -0.0711, -0.0354, -0.0157,\n",
       "          -0.0032, -0.0124,  0.0257,  0.0052,  0.0175, -0.0525, -0.0025,\n",
       "           0.0176,  0.0474, -0.0187,  0.0085, -0.0208, -0.0197, -0.0301,\n",
       "           0.0174,  0.0163, -0.0218, -0.0097,  0.0377,  0.0329, -0.0228,\n",
       "          -0.0735,  0.0152,  0.0340,  0.0437, -0.0445,  0.0334,  0.0009,\n",
       "          -0.0108, -0.0498, -0.0289,  0.0450, -0.0146,  0.0184, -0.0205,\n",
       "           0.0193,  0.0153,  0.0076,  0.0094, -0.0119, -0.0149,  0.0097,\n",
       "          -0.0289, -0.0018, -0.0023, -0.0471, -0.0029, -0.0724, -0.0016,\n",
       "          -0.0607,  0.0091, -0.0110, -0.0548, -0.0171,  0.0085,  0.0126,\n",
       "          -0.0370,  0.0446, -0.0012,  0.0127,  0.0208,  0.0271,  0.0335,\n",
       "          -0.0332,  0.0155,  0.0346,  0.0596,  0.0005,  0.0574,  0.0067,\n",
       "          -0.1372,  0.0305,  0.0691, -0.0037,  0.0077, -0.0637, -0.0234,\n",
       "          -0.0197, -0.0235],\n",
       "         [-0.0365,  0.0295,  0.0686, -0.0585, -0.0434,  0.0026,  0.0346,\n",
       "          -0.0126, -0.0153, -0.0104, -0.0601,  0.0017,  0.0204, -0.0316,\n",
       "          -0.0414, -0.0258,  0.0179, -0.0432, -0.0159,  0.0238, -0.0597,\n",
       "          -0.0046,  0.0376, -0.0340, -0.0122, -0.0376, -0.0512,  0.0524,\n",
       "          -0.0261,  0.0119,  0.0021,  0.0089, -0.0126,  0.0014,  0.0056,\n",
       "           0.0162, -0.0043,  0.0522, -0.0031, -0.0185,  0.0284,  0.0443,\n",
       "          -0.0376, -0.0057,  0.0567,  0.0375, -0.0315,  0.0658,  0.0387,\n",
       "          -0.0290, -0.0168, -0.0190,  0.0355,  0.0495,  0.0257, -0.0214,\n",
       "          -0.0779,  0.0079, -0.0032,  0.0296, -0.0305, -0.0504, -0.0555,\n",
       "          -0.0191, -0.0407, -0.0281,  0.0026,  0.0561,  0.0236,  0.0051,\n",
       "          -0.0127, -0.0619,  0.0014, -0.0142, -0.0477, -0.0064, -0.0537,\n",
       "           0.0048,  0.0094, -0.0518, -0.0347, -0.0250,  0.0211, -0.0211,\n",
       "          -0.0523,  0.0121,  0.0247, -0.0018, -0.0858, -0.0084, -0.0007,\n",
       "          -0.0514, -0.0118, -0.0245, -0.0309, -0.0116, -0.0653, -0.0181,\n",
       "           0.0264,  0.0105],\n",
       "         [-0.0534,  0.0041,  0.0317, -0.0237, -0.0388, -0.0027,  0.0020,\n",
       "          -0.0150, -0.0013,  0.0225, -0.0398,  0.0273, -0.0343, -0.0270,\n",
       "          -0.0152, -0.0077,  0.0417, -0.0345, -0.0035, -0.0236, -0.0030,\n",
       "          -0.0068,  0.0073, -0.0452,  0.0441, -0.0073, -0.0164, -0.0116,\n",
       "          -0.0124,  0.0051,  0.0247, -0.0100, -0.0173, -0.0274,  0.0142,\n",
       "           0.0295,  0.0086,  0.0139, -0.0143, -0.0028, -0.0910, -0.0393,\n",
       "          -0.0101,  0.0407,  0.0189, -0.0157, -0.0495, -0.0081,  0.0217,\n",
       "          -0.0177, -0.0803,  0.0827, -0.0138,  0.0003,  0.0285, -0.0018,\n",
       "           0.0242,  0.0170, -0.0059,  0.0213,  0.0220, -0.0099, -0.0021,\n",
       "          -0.0226, -0.0278,  0.0075, -0.0345, -0.0354, -0.0336, -0.0519,\n",
       "          -0.0259, -0.0419,  0.0076, -0.0013, -0.0119, -0.0067, -0.0170,\n",
       "           0.0013, -0.0106, -0.0282, -0.0345, -0.0154,  0.0754, -0.0067,\n",
       "          -0.0409,  0.0036, -0.0118,  0.0467, -0.0330,  0.0425, -0.0199,\n",
       "           0.0027, -0.0099, -0.0024, -0.0243, -0.0289, -0.0103,  0.0076,\n",
       "          -0.0219, -0.0152],\n",
       "         [-0.0131,  0.0020,  0.0310,  0.0355, -0.0486,  0.0185,  0.0483,\n",
       "          -0.0085,  0.0096,  0.0141, -0.0400, -0.0077,  0.0412, -0.0459,\n",
       "           0.0440, -0.0237,  0.0409, -0.0031,  0.0054,  0.0125, -0.0994,\n",
       "           0.0055, -0.0169,  0.0021,  0.0574,  0.0202, -0.0499,  0.0204,\n",
       "          -0.0280, -0.0303,  0.0105, -0.0319, -0.0186, -0.0073,  0.0146,\n",
       "          -0.0400,  0.0028,  0.0083, -0.0177, -0.0254, -0.0131, -0.0487,\n",
       "          -0.0373,  0.0050,  0.0303, -0.0357, -0.0380,  0.0037, -0.0256,\n",
       "          -0.0486, -0.0311, -0.0385,  0.0123, -0.0026, -0.0144, -0.0063,\n",
       "           0.0375, -0.0120,  0.0076,  0.0049,  0.0112,  0.0121,  0.0127,\n",
       "          -0.0352, -0.0016,  0.0207, -0.0205,  0.0153,  0.0605,  0.0555,\n",
       "          -0.0434, -0.0171, -0.0183, -0.0218,  0.0044, -0.0010,  0.0156,\n",
       "          -0.0031, -0.0169,  0.0139,  0.0024,  0.0403,  0.0020,  0.0003,\n",
       "          -0.0170,  0.0017, -0.0045, -0.0077, -0.0159,  0.0502, -0.0079,\n",
       "           0.0193, -0.0033,  0.0057, -0.0181, -0.0202, -0.0372, -0.0138,\n",
       "          -0.0111, -0.0202],\n",
       "         [-0.0509,  0.0456,  0.0142, -0.0524, -0.0519, -0.0074,  0.0248,\n",
       "          -0.0113, -0.0184,  0.0386, -0.0798,  0.0034, -0.0311, -0.0426,\n",
       "          -0.0177,  0.0443,  0.0471, -0.0374, -0.0271,  0.0007,  0.0005,\n",
       "          -0.0174, -0.0363, -0.0134,  0.0169,  0.0176,  0.0072,  0.0456,\n",
       "          -0.1091,  0.0173, -0.0017,  0.0184, -0.0128, -0.0278, -0.0037,\n",
       "          -0.0128, -0.0013,  0.0465, -0.0032, -0.0144,  0.0707,  0.0356,\n",
       "          -0.0460,  0.0294,  0.0210, -0.0267, -0.0614,  0.0170, -0.0051,\n",
       "          -0.0055, -0.0205, -0.0288,  0.0225,  0.0243, -0.0027,  0.0059,\n",
       "          -0.0569,  0.0224, -0.0014, -0.0090,  0.0036,  0.0040,  0.0208,\n",
       "          -0.0120, -0.0287,  0.0466, -0.0158,  0.0405,  0.0074,  0.0186,\n",
       "          -0.0072,  0.0313,  0.0041, -0.0914, -0.0589, -0.0069,  0.0120,\n",
       "          -0.0189,  0.0422, -0.0224,  0.0152,  0.0354,  0.0558, -0.0267,\n",
       "          -0.0081,  0.0117,  0.0069,  0.0509, -0.0072,  0.0344, -0.0153,\n",
       "          -0.1932,  0.0409, -0.0024, -0.0402, -0.0612, -0.0218, -0.0546,\n",
       "          -0.0029, -0.0106]], device='cuda:0'), Parameter containing:\n",
       " tensor([ 0.1583, -0.3000, -0.3264, -0.3665, -0.3871, -0.2995], device='cuda:0')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Agents[1].parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f81c2064ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_list = np.arange(len(r_list[0]))*ep_record\n",
    "plt.plot(epoch_list,r_list[0], label='Agent 1')\n",
    "plt.plot(epoch_list,r_list[1], label='Agent 2')\n",
    "plt.ylabel('Average reward in epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "#plt.savefig('N_ep='+str(N_ep)+'_seed='+str(num_seed)+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1.00000e-02 *\n",
       "        [ 8.7494], device='cuda:0'),\n",
       " tensor([ 0.1890], device='cuda:0'),\n",
       " tensor([ 0.9922], device='cuda:0'),\n",
       " tensor([ 0.9922], device='cuda:0'),\n",
       " tensor([ 0.1268], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor(1.00000e-02 *\n",
       "        [ 9.0546], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.4571], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.1107], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.9034], device='cuda:0'),\n",
       " tensor([ 0.1730], device='cuda:0'),\n",
       " tensor([ 1.], device='cuda:0'),\n",
       " tensor([ 0.1909], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.8731], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9922], device='cuda:0'),\n",
       " tensor([ 0.7115], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.9922], device='cuda:0'),\n",
       " tensor([ 0.1803], device='cuda:0'),\n",
       " tensor([ 0.9922], device='cuda:0'),\n",
       " tensor([ 0.1369], device='cuda:0'),\n",
       " tensor([ 0.9922], device='cuda:0'),\n",
       " tensor([ 0.3157], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.2448], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.1221], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.8087], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9922], device='cuda:0'),\n",
       " tensor([ 0.4176], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.1247], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.1445], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.2031], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.1735], device='cuda:0'),\n",
       " tensor([ 0.9922], device='cuda:0'),\n",
       " tensor([ 0.9922], device='cuda:0'),\n",
       " tensor([ 0.1169], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9922], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.1348], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.1230], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.1112], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.1755], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.1072], device='cuda:0'),\n",
       " tensor([ 0.9922], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.1329], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor(1.00000e-02 *\n",
       "        [ 9.8843], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor(1.00000e-02 *\n",
       "        [ 9.9894], device='cuda:0'),\n",
       " tensor([ 0.9922], device='cuda:0'),\n",
       " tensor([ 0.3172], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.9114], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.7901], device='cuda:0'),\n",
       " tensor([ 0.9922], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.1832], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.1052], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 1.], device='cuda:0'),\n",
       " tensor([ 0.2209], device='cuda:0'),\n",
       " tensor([ 0.4569], device='cuda:0'),\n",
       " tensor([ 0.9922], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor(1.00000e-02 *\n",
       "        [ 6.9246], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.9922], device='cuda:0'),\n",
       " tensor([ 0.8556], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.8968], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9922], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.1313], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.2050], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.1524], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.6081], device='cuda:0'),\n",
       " tensor([ 1.], device='cuda:0'),\n",
       " tensor([ 0.7902], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.8418], device='cuda:0'),\n",
       " tensor([ 0.9280], device='cuda:0'),\n",
       " tensor([ 0.9266], device='cuda:0'),\n",
       " tensor([ 0.9208], device='cuda:0'),\n",
       " tensor([ 0.9922], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.1104], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor(1.00000e-02 *\n",
       "        [ 7.5232], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9161], device='cuda:0'),\n",
       " tensor([ 0.9297], device='cuda:0'),\n",
       " tensor([ 0.8984], device='cuda:0'),\n",
       " tensor([ 0.9276], device='cuda:0'),\n",
       " tensor([ 0.9355], device='cuda:0'),\n",
       " tensor([ 0.9177], device='cuda:0'),\n",
       " tensor([ 0.8954], device='cuda:0'),\n",
       " tensor([ 0.9277], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.8594], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9054], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9193], device='cuda:0'),\n",
       " tensor([ 0.6104], device='cuda:0'),\n",
       " tensor([ 0.9812], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9922], device='cuda:0'),\n",
       " tensor([ 0.9922], device='cuda:0'),\n",
       " tensor([ 1.], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9430], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.9219], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.3549], device='cuda:0'),\n",
       " tensor([ 0.1167], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.7618], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.1537], device='cuda:0'),\n",
       " tensor([ 0.7879], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.1403], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.9142], device='cuda:0'),\n",
       " tensor([ 0.9219], device='cuda:0'),\n",
       " tensor([ 0.9268], device='cuda:0'),\n",
       " tensor([ 0.8939], device='cuda:0'),\n",
       " tensor([ 0.8786], device='cuda:0'),\n",
       " tensor([ 0.9219], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.8896], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9062], device='cuda:0'),\n",
       " tensor([ 0.9635], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9820], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9268], device='cuda:0'),\n",
       " tensor([ 0.8984], device='cuda:0'),\n",
       " tensor([ 0.9297], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9219], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.9425], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9187], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9492], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9707], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9036], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.8213], device='cuda:0'),\n",
       " tensor([ 0.9714], device='cuda:0'),\n",
       " tensor([ 0.9245], device='cuda:0'),\n",
       " tensor([ 0.9551], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.9297], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9219], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.9473], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.8396], device='cuda:0'),\n",
       " tensor([ 0.8167], device='cuda:0'),\n",
       " tensor([ 0.9422], device='cuda:0'),\n",
       " tensor([ 0.9578], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9297], device='cuda:0'),\n",
       " tensor([ 0.9434], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9141], device='cuda:0'),\n",
       " tensor([ 0.8984], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9219], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9141], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9297], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9141], device='cuda:0'),\n",
       " tensor([ 0.8984], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9297], device='cuda:0'),\n",
       " tensor([ 0.6860], device='cuda:0'),\n",
       " tensor([ 1.], device='cuda:0'),\n",
       " tensor([ 0.9922], device='cuda:0'),\n",
       " tensor([ 0.9922], device='cuda:0'),\n",
       " tensor([ 0.9922], device='cuda:0'),\n",
       " tensor([ 0.1781], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9727], device='cuda:0'),\n",
       " tensor([ 0.9297], device='cuda:0'),\n",
       " tensor([ 0.9479], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.4619], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9734], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9590], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9156], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9414], device='cuda:0'),\n",
       " tensor([ 0.9805], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9648], device='cuda:0'),\n",
       " tensor([ 0.7080], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9187], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9141], device='cuda:0'),\n",
       " tensor([ 0.9297], device='cuda:0'),\n",
       " tensor([ 0.9323], device='cuda:0'),\n",
       " tensor([ 0.9336], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9625], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9316], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.9557], device='cuda:0'),\n",
       " tensor([ 0.8750], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.6239], device='cuda:0'),\n",
       " tensor([ 0.8451], device='cuda:0'),\n",
       " tensor([ 0.8711], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.1358], device='cuda:0'),\n",
       " tensor([ 0.6284], device='cuda:0'),\n",
       " tensor([ 0.8984], device='cuda:0'),\n",
       " tensor([ 0.9219], device='cuda:0'),\n",
       " tensor([ 0.9277], device='cuda:0'),\n",
       " tensor([ 0.9707], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.8906], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9062], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9219], device='cuda:0'),\n",
       " tensor([ 0.9297], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9141], device='cuda:0'),\n",
       " tensor([ 0.9792], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor(1.00000e-02 *\n",
       "        [ 2.9842], device='cuda:0'),\n",
       " tensor(1.00000e-02 *\n",
       "        [ 3.8352], device='cuda:0'),\n",
       " tensor(1.00000e-02 *\n",
       "        [ 4.1702], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor(1.00000e-02 *\n",
       "        [ 1.9886], device='cuda:0'),\n",
       " tensor([ 0.5969], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9344], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9219], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9484], device='cuda:0'),\n",
       " tensor([ 0.9656], device='cuda:0'),\n",
       " tensor([ 0.9922], device='cuda:0'),\n",
       " tensor([ 0.9922], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9245], device='cuda:0'),\n",
       " tensor([ 0.9219], device='cuda:0'),\n",
       " tensor([ 0.9805], device='cuda:0'),\n",
       " tensor([ 0.9570], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9219], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9297], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.9297], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9750], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.9590], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor(1.00000e-02 *\n",
       "        [ 9.8234], device='cuda:0'),\n",
       " tensor(1.00000e-02 *\n",
       "        [ 1.0735], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9414], device='cuda:0'),\n",
       " tensor([ 0.9297], device='cuda:0'),\n",
       " tensor([ 0.9375], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9844], device='cuda:0'),\n",
       " tensor([ 0.9570], device='cuda:0'),\n",
       " tensor([ 0.9531], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9609], device='cuda:0'),\n",
       " tensor([ 0.9688], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.9473], device='cuda:0'),\n",
       " tensor([ 0.9453], device='cuda:0'),\n",
       " tensor([ 0.9766], device='cuda:0'),\n",
       " tensor([ 0.7526], device='cuda:0')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
