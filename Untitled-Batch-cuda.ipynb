{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "All the logarithms used are base 2. <br>\n",
    "Assumes 2 self-interested agents alternating turns. <br>\n",
    "Baseline (1 for each agent) gets updated after each episode ends (see corpses). <br>\n",
    "Rewards only possible at the end of each game. <br>\n",
    "Uses same (numerical) encoder for both item context and proposal. Reference code uses 3 distinct ones. It also has max_utility = num_types instead of 10 for us.<br>\n",
    "Check how message policy works again; paper seemed to imply that each output of the lstm is a letter. (we take the hidden output and make a probability over letters out of it).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Network\n",
    "import torch\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "# cuda\n",
    "use_cuda = True\n",
    "\n",
    "# Random seeds for testing\n",
    "num_seed = 10\n",
    "torch.manual_seed(num_seed)\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    torch.cuda.manual_seed(num_seed)\n",
    "np.random.seed(num_seed)\n",
    "\n",
    "# Utility functions\n",
    "from utility import truncated_poisson_sampling, create_item_pool, create_agent_utility, rewards_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Game setup\n",
    "num_agents = 2         # Number of agents playing the game\n",
    "num_types = 3          # Number of item types\n",
    "max_item = 5           # Maximum number of each item in a pool\n",
    "max_utility = 10       # Maximum utility value for agents\n",
    "\n",
    "# Turn sampling\n",
    "lam = 7                # Poisson parameter\n",
    "max_N = 10             # Maximum number of turns\n",
    "min_N = 4              # Minimum number of turns\n",
    "\n",
    "# Linguistic channel\n",
    "num_vocab = 10         # Symbol vocabulary size for linguistic channel\n",
    "len_message = 6        # Linguistic message length\n",
    "\n",
    "# Training\n",
    "alpha = 0.001          # learning rate\n",
    "N_ep = 10000              # Number of episodes\n",
    "num_games = 128        # Number of games per episode (batch size)\n",
    "\n",
    "# Appendix\n",
    "lambda1 = 0.05         # Entropy regularizer for pi_term, pi_prop\n",
    "lambda2 = 0.001        # Entropy regularizer for pi_utt\n",
    "smoothing_const = 0.7  # Smoothing constant for the exponential moving average baseline\n",
    "\n",
    "# Miscellaneous\n",
    "ep_time = 50         # Print time every ep_time episodes\n",
    "ep_record = 10        # Record training curve every ep_record episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class combined_policy(nn.Module):\n",
    "    def __init__(self, embedding_dim = 100, batch_size = 128, num_layers = 1, bias = True, batch_first = False, dropout = 0, bidirectional = False):\n",
    "        super(combined_policy, self).__init__()\n",
    "        # Save variables\n",
    "        self.embedding_dim = embedding_dim # Hidden layer dimensions\n",
    "        self.batch_size = batch_size       # Batch size (updated every forward pass)\n",
    "        self.log_p = torch.zeros([batch_size,1])                     # Store policy log likelihood for REINFORCE\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            self.log_p = self.log_p.cuda()\n",
    "        \n",
    "        # Encoding -------------------------------------------------------------\n",
    "        \n",
    "        # Numerical encoder\n",
    "        self.encoder1 = nn.Embedding(max_utility+1, embedding_dim)\n",
    "        # Linguistic encoder\n",
    "        self.encoder2 = nn.Embedding(num_vocab+1, embedding_dim)\n",
    "        \n",
    "        # Item context LSTM\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        # Linguistic LSTM\n",
    "        self.lstm2 = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        # Proposal LSTM\n",
    "        self.lstm3 = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        \n",
    "        # Outputs of the 3 LSTMS get concatenated together\n",
    "        \n",
    "        # Feed-forward\n",
    "        self.ff = nn.Linear(3*embedding_dim, embedding_dim)\n",
    "        \n",
    "        # Output of feed-forward is the input for the policy networks\n",
    "        \n",
    "        # Policy ---------------------------------------------------------------\n",
    "        \n",
    "        # Termination policy\n",
    "        self.policy_term = nn.Linear(embedding_dim, 1)\n",
    "        # Linguistic policy\n",
    "        self.policy_ling = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        self.ff_ling = nn.Linear(embedding_dim, num_vocab)\n",
    "        # Proposal policies\n",
    "        self.policy_prop = nn.ModuleList([nn.Linear(embedding_dim, max_item+1) for i in range(num_types)])\n",
    "        \n",
    "    def forward(self, x, test, batch_size=128):\n",
    "        # Inputs --------------------------------------------------------------------\n",
    "        # x = list of three elements consisting of:\n",
    "        #   1. item context (longtensor of shape batch_size x (2*num_types))\n",
    "        #   2. previous linguistic message (longtensor of shape batch_size x len_message)\n",
    "        #   3. previous proposal (longtensor of shape batch_size x num_types)\n",
    "        # test = whether training or testing (testing selects actions greedily)\n",
    "        # batch_size = batch size\n",
    "        # Outputs -------------------------------------------------------------------\n",
    "        # term = binary variable where 1 indicates proposal accepted => game finished (longtensor of shape batch_size x 1)\n",
    "        # message = crafted linguistic message (longtensor of shape batch_size x len_message)\n",
    "        # prop = crafted proposal (longtensor of shape batch_size x num_types)\n",
    "        # entropy_loss = Number containing the sum of policy entropies (should be total entropy by additivity)\n",
    "        \n",
    "        # Update batch_size variable (changes throughout training due to sieving (see survivors below))\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Extract inputs ------------------------------------------------------------\n",
    "        \n",
    "        # Item context\n",
    "        x1 = x[0]\n",
    "        # Previous linguistic message\n",
    "        x2 = x[1]\n",
    "        # Previous proposal\n",
    "        x3 = x[2]  \n",
    "\n",
    "        # Encoding ------------------------------------------------------------------\n",
    "\n",
    "        # Initial embedding\n",
    "        x1 = self.encoder1(x1).transpose(0,1)\n",
    "        x2 = self.encoder2(x2).transpose(0,1)\n",
    "        x3 = self.encoder1(x3).transpose(0,1) # Same encoder as item context\n",
    "            \n",
    "        # LSTM for item context\n",
    "        h1 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial hidden\n",
    "        c1 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial cell\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            h1 = h1.cuda()\n",
    "            c1 = c1.cuda()\n",
    "\n",
    "        for i in range(x1.size()[0]):\n",
    "            _, (h1,c1) = self.lstm1(x1[i].view(1,self.batch_size,self.embedding_dim),(h1,c1))\n",
    "        x1_encoded = h1\n",
    "        \n",
    "        # LSTM for linguistic\n",
    "        h2 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial hidden\n",
    "        c2 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial cell\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            h2 = h2.cuda()\n",
    "            c2 = c2.cuda()\n",
    "\n",
    "        for i in range(x2.size()[0]):\n",
    "            _, (h2,c2) = self.lstm2(x2[i].view(1,self.batch_size,self.embedding_dim),(h2,c2))\n",
    "        x2_encoded = h2\n",
    "        \n",
    "        # LSTM for proposal\n",
    "        h3 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial hidden\n",
    "        c3 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial cell\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            h3 = h3.cuda()\n",
    "            c3 = c3.cuda()\n",
    "\n",
    "        for i in range(x3.size()[0]):\n",
    "            _, (h3,c3) = self.lstm2(x3[i].view(1,self.batch_size,self.embedding_dim),(h3,c3))\n",
    "        x3_encoded = h3\n",
    "\n",
    "        # Concatenate side-by-side\n",
    "        h = torch.cat([x1_encoded,x2_encoded,x3_encoded],2)\n",
    "\n",
    "        # Feedforward\n",
    "        h = self.ff(h)\n",
    "        h = F.relu(h) # Hidden layer input for policy networks\n",
    "        \n",
    "        # Policy ------------------------------------------------------------------\n",
    "\n",
    "        # Termination -----------------------------------------------\n",
    "        p_term = F.sigmoid(self.policy_term(h)).view(self.batch_size,1).float()\n",
    "\n",
    "        # Entropy\n",
    "        one_tensor = torch.ones(self.batch_size,1)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            one_tensor = one_tensor.cuda()\n",
    "        entropy_term = -(p_term * p_term.log()) - (one_tensor-p_term) * (one_tensor-p_term).log()\n",
    "        entropy_term = torch.sum(entropy_term)\n",
    "        \n",
    "        if test:\n",
    "            # Greedy\n",
    "            term = torch.round(p_term).long()\n",
    "        else:\n",
    "            # Sample\n",
    "            term = torch.bernoulli(p_term).long()\n",
    "            \n",
    "        # log p for REINFORCE\n",
    "        log_p_term = torch.zeros(self.batch_size,1)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            log_p_term = log_p_term.cuda()\n",
    "\n",
    "        log_p_term = (term.float() * (p_term+1e-8).log()) - (one_tensor-term.float()) * (one_tensor-p_term+1e-8).log()\n",
    "        \n",
    "        # Linguistic construction ----------------------------------\n",
    "        h_ling = h.clone() # Initial hidden state\n",
    "        c_ling = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial cell state\n",
    "        letter = torch.zeros(self.batch_size,1).long() # Initial letter (dummy)\n",
    "        entropy_letter = torch.zeros([self.batch_size,len_message])\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            c_ling = c_ling.cuda()\n",
    "            letter = letter.cuda()\n",
    "            entropy_letter = entropy_letter.cuda()\n",
    "        \n",
    "        # log p for REINFORCE \n",
    "        log_p_letter = torch.zeros([self.batch_size,1])\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            log_p_letter = log_p_letter.cuda()\n",
    "\n",
    "        message = torch.zeros(self.batch_size,len_message) # Message\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            message = message.cuda()\n",
    "        for i in range(len_message):\n",
    "            embedded_letter = self.encoder2(letter)\n",
    "\n",
    "            _, (h_ling,c_ling) = self.policy_ling(embedded_letter.view(1,self.batch_size,self.embedding_dim),(h_ling,c_ling))\n",
    "            logit = self.ff_ling(h_ling)\n",
    "            p_letter = F.softmax(logit,dim=2).view(self.batch_size,num_vocab).float()\n",
    "\n",
    "            entropy_letter[:,i] = -torch.sum(p_letter*p_letter.log(),1)\n",
    "\n",
    "            if test:\n",
    "                # Greedy\n",
    "                letter = p_letter.argmax(dim=1).view(self.batch_size,1).long()\n",
    "            else:\n",
    "                # Sample\n",
    "                letter = torch.multinomial(p_letter,1).long()\n",
    "                \n",
    "            # Gather the probabilities for the letters we've picked\n",
    "            probs = torch.gather(p_letter, 1, letter)\n",
    "            log_p_letter = log_p_letter + probs.log()\n",
    "                \n",
    "            message[:,i] = letter.squeeze()\n",
    "            \n",
    "        message = message.long()\n",
    "        entropy_letter = torch.sum(entropy_letter)     \n",
    "   \n",
    "        # Proposal ----------------------------------------------\n",
    "        p_prop = []\n",
    "        prop = []\n",
    "        \n",
    "        #prop = torch.zeros([self.batch_size,num_types]).long()\n",
    "        entropy_prop_list = [0,0,0]\n",
    "        \n",
    "        # log p for REINFORCE \n",
    "        log_p_prop = torch.zeros([self.batch_size,1])\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            log_p_prop = log_p_prop.cuda()\n",
    "\n",
    "        for i in range(num_types):\n",
    "            p_prop.append(F.sigmoid(self.policy_prop[i](h)))\n",
    "            \n",
    "            entropy_prop_list[i] = -torch.sum(p_prop[i]*p_prop[i].log())\n",
    "            \n",
    "            p_prop[i] = p_prop[i].view(self.batch_size,max_item+1)\n",
    "\n",
    "            if test:\n",
    "                # Greedy\n",
    "                #prop[:,i] = p_prop[i].argmax(dim=1)\n",
    "                prop.append(p_prop[i].argmax(dim=1))\n",
    "            else:\n",
    "                # Sample\n",
    "                #prop[:,i] = torch.multinomial(p_prop,1)\n",
    "                prop.append(torch.multinomial(p_prop,1))\n",
    "                \n",
    "            # Gather the probabilities for the letters we've picked\n",
    "            probs = torch.gather(p_prop[i], 1, prop[i].view(self.batch_size,1))\n",
    "            log_p_prop = log_p_prop + probs.log()\n",
    "              \n",
    "        prop = torch.stack(prop).transpose(0,1)\n",
    "        entropy_prop = sum(entropy_prop_list) # Entropy for exploration\n",
    "\n",
    "        # Combine -----------------------------------------------------------------\n",
    "        entropy_loss = torch.sum(lambda1*entropy_term + lambda1*entropy_prop + lambda2*entropy_letter)\n",
    "        self.log_p = log_p_term + log_p_letter + log_p_prop\n",
    "\n",
    "        return (term,message,prop, entropy_loss, log_p_term,log_p_letter,log_p_prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = combined_policy()\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    net = net.cuda()\n",
    "\n",
    "# Dummy inputs\n",
    "x = torch.randint(0,max_item,[128,6]).long()\n",
    "y = torch.randint(0,num_vocab,[128,6]).long()\n",
    "z = torch.randint(0,max_item,[128,3]).long()\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    z = z.cuda()\n",
    "\n",
    "blah = net([x,y,z],True)\n",
    "\n",
    "# Initialize agents\n",
    "Agents = []\n",
    "for i in range(num_agents):\n",
    "    Agents.append(combined_policy())\n",
    "    if torch.cuda.is_available() and use_cuda:\n",
    "        Agents[i] = Agents[i].cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start ----------------\n",
      "Runtime for episodes 0-50: 1.782008409500122s\n",
      "Runtime for episodes 50-100: 1.6930465698242188s\n",
      "Runtime for episodes 100-150: 1.6988131999969482s\n",
      "Runtime for episodes 150-200: 1.698721170425415s\n",
      "Runtime for episodes 200-250: 1.6896183490753174s\n",
      "Runtime for episodes 250-300: 1.691213846206665s\n",
      "Runtime for episodes 300-350: 1.704592227935791s\n",
      "Runtime for episodes 350-400: 1.8120160102844238s\n",
      "Runtime for episodes 400-450: 1.7026147842407227s\n",
      "Runtime for episodes 450-500: 1.693415880203247s\n",
      "Runtime for episodes 500-550: 1.6970655918121338s\n",
      "Runtime for episodes 550-600: 1.7007606029510498s\n",
      "Runtime for episodes 600-650: 1.6864972114562988s\n",
      "Runtime for episodes 650-700: 1.7085485458374023s\n",
      "Runtime for episodes 700-750: 1.6883587837219238s\n",
      "Runtime for episodes 750-800: 1.7014577388763428s\n",
      "Runtime for episodes 800-850: 1.70100998878479s\n",
      "Runtime for episodes 850-900: 1.6997475624084473s\n",
      "Runtime for episodes 900-950: 1.6976993083953857s\n",
      "Runtime for episodes 950-1000: 1.702756404876709s\n",
      "Runtime for episodes 1000-1050: 1.698744535446167s\n",
      "Runtime for episodes 1050-1100: 1.711083173751831s\n",
      "Runtime for episodes 1100-1150: 1.6990277767181396s\n",
      "Runtime for episodes 1150-1200: 1.6937081813812256s\n",
      "Runtime for episodes 1200-1250: 1.6972782611846924s\n",
      "Runtime for episodes 1250-1300: 1.696446180343628s\n",
      "Runtime for episodes 1300-1350: 1.6997921466827393s\n",
      "Runtime for episodes 1350-1400: 1.6968939304351807s\n",
      "Runtime for episodes 1400-1450: 1.7023801803588867s\n",
      "Runtime for episodes 1450-1500: 1.6932346820831299s\n",
      "Runtime for episodes 1500-1550: 1.6912565231323242s\n",
      "Runtime for episodes 1550-1600: 1.6968326568603516s\n",
      "Runtime for episodes 1600-1650: 1.6982016563415527s\n",
      "Runtime for episodes 1650-1700: 1.7022888660430908s\n",
      "Runtime for episodes 1700-1750: 1.6973679065704346s\n",
      "Runtime for episodes 1750-1800: 1.7054965496063232s\n",
      "Runtime for episodes 1800-1850: 1.6959214210510254s\n",
      "Runtime for episodes 1850-1900: 1.6943891048431396s\n",
      "Runtime for episodes 1900-1950: 1.6894762516021729s\n",
      "Runtime for episodes 1950-2000: 1.6939723491668701s\n",
      "Runtime for episodes 2000-2050: 1.7031745910644531s\n",
      "Runtime for episodes 2050-2100: 1.701251745223999s\n",
      "Runtime for episodes 2100-2150: 1.686570167541504s\n",
      "Runtime for episodes 2150-2200: 1.6961090564727783s\n",
      "Runtime for episodes 2200-2250: 1.6954281330108643s\n",
      "Runtime for episodes 2250-2300: 1.6903324127197266s\n",
      "Runtime for episodes 2300-2350: 1.6943817138671875s\n",
      "Runtime for episodes 2350-2400: 1.68658447265625s\n",
      "Runtime for episodes 2400-2450: 1.7011733055114746s\n",
      "Runtime for episodes 2450-2500: 1.695319652557373s\n",
      "Runtime for episodes 2500-2550: 1.6878442764282227s\n",
      "Runtime for episodes 2550-2600: 1.6922657489776611s\n",
      "Runtime for episodes 2600-2650: 1.6909229755401611s\n",
      "Runtime for episodes 2650-2700: 1.6974482536315918s\n",
      "Runtime for episodes 2700-2750: 1.6970224380493164s\n",
      "Runtime for episodes 2750-2800: 1.7066996097564697s\n",
      "Runtime for episodes 2800-2850: 1.7012627124786377s\n",
      "Runtime for episodes 2850-2900: 1.6915757656097412s\n",
      "Runtime for episodes 2900-2950: 1.7105498313903809s\n",
      "Runtime for episodes 2950-3000: 1.692474365234375s\n",
      "Runtime for episodes 3000-3050: 1.690525770187378s\n",
      "Runtime for episodes 3050-3100: 1.7002618312835693s\n",
      "Runtime for episodes 3100-3150: 1.6896493434906006s\n",
      "Runtime for episodes 3150-3200: 1.692122220993042s\n",
      "Runtime for episodes 3200-3250: 1.7013740539550781s\n",
      "Runtime for episodes 3250-3300: 1.692413568496704s\n",
      "Runtime for episodes 3300-3350: 1.6863892078399658s\n",
      "Runtime for episodes 3350-3400: 1.6890664100646973s\n",
      "Runtime for episodes 3400-3450: 1.6855039596557617s\n",
      "Runtime for episodes 3450-3500: 1.6920342445373535s\n",
      "Runtime for episodes 3500-3550: 1.6982183456420898s\n",
      "Runtime for episodes 3550-3600: 1.6717002391815186s\n",
      "Runtime for episodes 3600-3650: 1.6948800086975098s\n",
      "Runtime for episodes 3650-3700: 1.6982221603393555s\n",
      "Runtime for episodes 3700-3750: 1.705270767211914s\n",
      "Runtime for episodes 3750-3800: 1.694692611694336s\n",
      "Runtime for episodes 3800-3850: 1.704348087310791s\n",
      "Runtime for episodes 3850-3900: 1.7007009983062744s\n",
      "Runtime for episodes 3900-3950: 1.6845886707305908s\n",
      "Runtime for episodes 3950-4000: 1.6979444026947021s\n",
      "Runtime for episodes 4000-4050: 1.6989459991455078s\n",
      "Runtime for episodes 4050-4100: 1.6934852600097656s\n",
      "Runtime for episodes 4100-4150: 1.6910591125488281s\n",
      "Runtime for episodes 4150-4200: 1.6839840412139893s\n",
      "Runtime for episodes 4200-4250: 1.6847307682037354s\n",
      "Runtime for episodes 4250-4300: 1.695986270904541s\n",
      "Runtime for episodes 4300-4350: 1.6958661079406738s\n",
      "Runtime for episodes 4350-4400: 1.6918327808380127s\n",
      "Runtime for episodes 4400-4450: 1.6886065006256104s\n",
      "Runtime for episodes 4450-4500: 1.6786205768585205s\n",
      "Runtime for episodes 4500-4550: 1.6977458000183105s\n",
      "Runtime for episodes 4550-4600: 1.6974241733551025s\n",
      "Runtime for episodes 4600-4650: 1.6998169422149658s\n",
      "Runtime for episodes 4650-4700: 1.6989617347717285s\n",
      "Runtime for episodes 4700-4750: 1.687859058380127s\n",
      "Runtime for episodes 4750-4800: 1.6942400932312012s\n",
      "Runtime for episodes 4800-4850: 1.6837456226348877s\n",
      "Runtime for episodes 4850-4900: 1.6853349208831787s\n",
      "Runtime for episodes 4900-4950: 1.6926403045654297s\n",
      "Runtime for episodes 4950-5000: 1.691878080368042s\n",
      "Runtime for episodes 5000-5050: 1.6955897808074951s\n",
      "Runtime for episodes 5050-5100: 1.686532735824585s\n",
      "Runtime for episodes 5100-5150: 1.6875495910644531s\n",
      "Runtime for episodes 5150-5200: 1.68951416015625s\n",
      "Runtime for episodes 5200-5250: 1.6918702125549316s\n",
      "Runtime for episodes 5250-5300: 1.7102000713348389s\n",
      "Runtime for episodes 5300-5350: 1.7025251388549805s\n",
      "Runtime for episodes 5350-5400: 1.6895430088043213s\n",
      "Runtime for episodes 5400-5450: 1.6886603832244873s\n",
      "Runtime for episodes 5450-5500: 1.691263198852539s\n",
      "Runtime for episodes 5500-5550: 1.6839520931243896s\n",
      "Runtime for episodes 5550-5600: 1.6824007034301758s\n",
      "Runtime for episodes 5600-5650: 1.6821186542510986s\n",
      "Runtime for episodes 5650-5700: 1.6663415431976318s\n",
      "Runtime for episodes 5700-5750: 1.6673314571380615s\n",
      "Runtime for episodes 5750-5800: 1.6809914112091064s\n",
      "Runtime for episodes 5800-5850: 1.6835966110229492s\n",
      "Runtime for episodes 5850-5900: 1.683821678161621s\n",
      "Runtime for episodes 5900-5950: 1.6842834949493408s\n",
      "Runtime for episodes 5950-6000: 1.6920833587646484s\n",
      "Runtime for episodes 6000-6050: 1.735133171081543s\n",
      "Runtime for episodes 6050-6100: 1.893979549407959s\n",
      "Runtime for episodes 6100-6150: 1.8214240074157715s\n",
      "Runtime for episodes 6150-6200: 1.699408769607544s\n",
      "Runtime for episodes 6200-6250: 1.6913201808929443s\n",
      "Runtime for episodes 6250-6300: 1.685802698135376s\n",
      "Runtime for episodes 6300-6350: 1.691894292831421s\n",
      "Runtime for episodes 6350-6400: 1.6913211345672607s\n",
      "Runtime for episodes 6400-6450: 1.7096145153045654s\n",
      "Runtime for episodes 6450-6500: 1.7047491073608398s\n",
      "Runtime for episodes 6500-6550: 1.8062329292297363s\n",
      "Runtime for episodes 6550-6600: 1.8485922813415527s\n",
      "Runtime for episodes 6600-6650: 1.8737189769744873s\n",
      "Runtime for episodes 6650-6700: 1.8254344463348389s\n",
      "Runtime for episodes 6700-6750: 1.8355252742767334s\n",
      "Runtime for episodes 6750-6800: 1.8147313594818115s\n",
      "Runtime for episodes 6800-6850: 1.7542245388031006s\n",
      "Runtime for episodes 6850-6900: 1.8684031963348389s\n",
      "Runtime for episodes 6900-6950: 1.7790942192077637s\n",
      "Runtime for episodes 6950-7000: 1.7422657012939453s\n",
      "Runtime for episodes 7000-7050: 1.8089210987091064s\n",
      "Runtime for episodes 7050-7100: 1.8163120746612549s\n",
      "Runtime for episodes 7100-7150: 1.866837739944458s\n",
      "Runtime for episodes 7150-7200: 1.7918152809143066s\n",
      "Runtime for episodes 7200-7250: 1.8747854232788086s\n",
      "Runtime for episodes 7250-7300: 1.824819564819336s\n",
      "Runtime for episodes 7300-7350: 1.6881186962127686s\n",
      "Runtime for episodes 7350-7400: 1.698373556137085s\n",
      "Runtime for episodes 7400-7450: 1.6973953247070312s\n",
      "Runtime for episodes 7450-7500: 1.697638750076294s\n",
      "Runtime for episodes 7500-7550: 1.6873416900634766s\n",
      "Runtime for episodes 7550-7600: 1.694115400314331s\n",
      "Runtime for episodes 7600-7650: 1.697397232055664s\n",
      "Runtime for episodes 7650-7700: 1.702517032623291s\n",
      "Runtime for episodes 7700-7750: 1.6998224258422852s\n",
      "Runtime for episodes 7750-7800: 1.6887128353118896s\n",
      "Runtime for episodes 7800-7850: 1.6993298530578613s\n",
      "Runtime for episodes 7850-7900: 1.6958856582641602s\n",
      "Runtime for episodes 7900-7950: 1.694868803024292s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime for episodes 7950-8000: 1.6938610076904297s\n",
      "Runtime for episodes 8000-8050: 1.6973121166229248s\n",
      "Runtime for episodes 8050-8100: 1.6959850788116455s\n",
      "Runtime for episodes 8100-8150: 1.6886398792266846s\n",
      "Runtime for episodes 8150-8200: 1.684142827987671s\n",
      "Runtime for episodes 8200-8250: 1.6930618286132812s\n",
      "Runtime for episodes 8250-8300: 1.6870453357696533s\n",
      "Runtime for episodes 8300-8350: 1.6971094608306885s\n",
      "Runtime for episodes 8350-8400: 1.697159767150879s\n",
      "Runtime for episodes 8400-8450: 1.6973462104797363s\n",
      "Runtime for episodes 8450-8500: 1.6790857315063477s\n",
      "Runtime for episodes 8500-8550: 1.6994712352752686s\n",
      "Runtime for episodes 8550-8600: 1.7051057815551758s\n",
      "Runtime for episodes 8600-8650: 1.6906943321228027s\n",
      "Runtime for episodes 8650-8700: 1.68544340133667s\n",
      "Runtime for episodes 8700-8750: 1.7153825759887695s\n",
      "Runtime for episodes 8750-8800: 1.6954240798950195s\n",
      "Runtime for episodes 8800-8850: 1.6827995777130127s\n",
      "Runtime for episodes 8850-8900: 1.6795852184295654s\n",
      "Runtime for episodes 8900-8950: 1.7052373886108398s\n",
      "Runtime for episodes 8950-9000: 1.7012755870819092s\n",
      "Runtime for episodes 9000-9050: 1.7044739723205566s\n",
      "Runtime for episodes 9050-9100: 1.6863746643066406s\n",
      "Runtime for episodes 9100-9150: 1.697340965270996s\n",
      "Runtime for episodes 9150-9200: 1.6983919143676758s\n",
      "Runtime for episodes 9200-9250: 1.6896257400512695s\n",
      "Runtime for episodes 9250-9300: 1.692819356918335s\n",
      "Runtime for episodes 9300-9350: 1.6863610744476318s\n",
      "Runtime for episodes 9350-9400: 1.6992273330688477s\n",
      "Runtime for episodes 9400-9450: 1.6968631744384766s\n",
      "Runtime for episodes 9450-9500: 1.6860809326171875s\n",
      "Runtime for episodes 9500-9550: 1.6781156063079834s\n",
      "Runtime for episodes 9550-9600: 1.6706995964050293s\n",
      "Runtime for episodes 9600-9650: 1.6795639991760254s\n",
      "Runtime for episodes 9650-9700: 1.6818041801452637s\n",
      "Runtime for episodes 9700-9750: 1.6873843669891357s\n",
      "Runtime for episodes 9750-9800: 1.6803276538848877s\n",
      "Runtime for episodes 9800-9850: 1.6723952293395996s\n",
      "Runtime for episodes 9850-9900: 1.6827590465545654s\n",
      "Runtime for episodes 9900-9950: 1.691162347793579s\n",
      "End ------------------\n",
      "Total runtime: 341.24870204925537s\n"
     ]
    }
   ],
   "source": [
    "baselines = [0 for _ in range(num_agents)] # Baselines for reward calculation\n",
    "\n",
    "# Initialize optimizers for learning\n",
    "optimizers = []\n",
    "for i in range(num_agents):\n",
    "    optimizers.append(optim.Adam(Agents[i].parameters()))\n",
    "    \n",
    "# Train rewards\n",
    "r_list = []\n",
    "for i in range(num_agents):\n",
    "    r_list.append([])\n",
    "\n",
    "print('Start ----------------')\n",
    "time_start = time.time()\n",
    "time_p1 = time.time()\n",
    "# Loop over episodes\n",
    "for i_ep in range(N_ep):\n",
    "    # Setting up games -----------------------------------------------------------------------\n",
    "    # Game setup\n",
    "    # Truncated Poisson sampling for number of turns in each game\n",
    "    N = truncated_poisson_sampling(lam, min_N, max_N, num_games)\n",
    "    # Item pools for each game\n",
    "    pool = create_item_pool(num_types, max_item, num_games)\n",
    "    if torch.cuda.is_available() and use_cuda:\n",
    "        N = N.cuda()\n",
    "        pool = pool.cuda()\n",
    "    # Item contexts for each game\n",
    "    item_contexts = [] # Each agent has different utilities (but same pool)\n",
    "    for i in range(num_agents):\n",
    "        utility = create_agent_utility(num_types, max_utility, num_games)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            utility = utility.cuda()\n",
    "        item_contexts.append(torch.cat([pool, utility],1))\n",
    "    \n",
    "    # Initializations\n",
    "    survivors = torch.ones(num_games).nonzero()               # Keeps track of ongoing games; everyone alive initially\n",
    "    num_alive = len(survivors)                                # Actual batch size for each turn (initially num_games)\n",
    "    prev_messages = torch.zeros(num_games, len_message).long() # Previous linguistic message for each game\n",
    "    prev_proposals = torch.zeros(num_games, num_types).long()  # Previous proposal for each game\n",
    "    if torch.cuda.is_available() and use_cuda:\n",
    "        survivors = survivors.cuda()\n",
    "        prev_messages = prev_messages.cuda()\n",
    "        prev_proposals = prev_proposals.cuda()\n",
    "    \n",
    "    rewards = [torch.zeros(num_games), torch.zeros(num_games)]       # Rewards for each game for each agent\n",
    "    # Keep track of sum of all rewards (from all games in a batch) for baseline updates (see corpses below)\n",
    "    reward_sums = []\n",
    "    for i in range(num_agents):\n",
    "        reward_sums.append(torch.zeros(1)) # Just a number\n",
    "        Agents[i].log_p = torch.zeros([num_games,1])\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            rewards[i] = rewards[i].cuda()\n",
    "            reward_sums[i] = reward_sums[i].cuda()\n",
    "            Agents[i].log_p = Agents[i].log_p.cuda()\n",
    "\n",
    "    # Play the games -------------------------------------------------------------------------\n",
    "    for i_turn in range(max_N): # Loop through maximum possible number of turns for all games\n",
    "        \n",
    "        # Losses for each agent\n",
    "        reward_losses = [torch.zeros([],requires_grad=True), torch.zeros([],requires_grad=True)]  \n",
    "        entropy_losses = [torch.zeros([],requires_grad=True), torch.zeros([],requires_grad=True)] # Exploration\n",
    "        for j in range(num_agents):\n",
    "            Agents[j].log_p = torch.zeros([num_alive,1])\n",
    "            \n",
    "            if torch.cuda.is_available() and use_cuda:\n",
    "                reward_losses[j] = reward_losses[j].cuda()\n",
    "                entropy_losses[j] = entropy_losses[j].cuda()\n",
    "                Agents[j].log_p = Agents[j].log_p.cuda()\n",
    "        \n",
    "        # Agent IDs\n",
    "        id_1 = i_turn % 2    # Current player\n",
    "        id_2 = int(not id_1) # Other player\n",
    "        \n",
    "        # Remove finished games (batch size decreases)\n",
    "        N = N[survivors].view(num_alive, 1)\n",
    "        pool = pool[survivors].view(num_alive, num_types)\n",
    "        prev_messages = prev_messages[survivors].view(num_alive, len_message)\n",
    "        prev_proposals = prev_proposals[survivors].view(num_alive, num_types)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            N = N.cuda()\n",
    "            pool = pool.cuda()\n",
    "            prev_messages = prev_messages.cuda()\n",
    "            prev_proposals = prev_proposals.cuda()\n",
    "        # Quantities different for each agent\n",
    "        for j in range(num_agents):\n",
    "            item_contexts[j] = item_contexts[j][survivors].view(num_alive,num_types*2)\n",
    "            #rewards[j] = rewards[j][survivors].view(num_alive)\n",
    "            #reward_losses[j] = reward_losses[j][survivors].view(num_alive)\n",
    "            if torch.cuda.is_available() and use_cuda:\n",
    "                item_contexts[j] = item_contexts[j].cuda()\n",
    "        \n",
    "        # Agent currently playing\n",
    "        Agent = Agents[id_1]             \n",
    "        item_context = item_contexts[id_1]\n",
    "        \n",
    "        # Play the game -------------------------------------------------------------\n",
    "        term, prev_messages, proposals, entropy_loss, lt,ll,lp = Agent([item_context, prev_messages, prev_proposals], True, num_alive)\n",
    "        entropy_losses[id_1] = entropy_loss\n",
    "        \n",
    "        # Compute reward loss (assumes 2 agents) ------------------------------------\n",
    "        # Games terminated by the current agent (previous proposal accepted)\n",
    "        \n",
    "        finishers = term.squeeze().nonzero()          # squeeze is for getting rid of extra useless dimension that pops up for some reason\n",
    "        num_finishers = len(finishers)\n",
    "\n",
    "        if len(finishers) != 0:\n",
    "            pool_12 = pool[finishers].view(num_finishers,num_types)\n",
    "            \n",
    "            share_2 = prev_proposals[finishers].view(num_finishers,num_types) # Share of other (previous proposal) \n",
    "            share_1 = pool_12 - share_2 # Share of this agent (remainder)\n",
    "            \n",
    "            # Zero reward if proposal exceeds pool\n",
    "            invalid_batches = torch.sum(share_2>pool_12,1)>0\n",
    "            share_2[invalid_batches] = 0\n",
    "            share_1[invalid_batches] = 0\n",
    "            \n",
    "            utility_1 = item_contexts[id_1][:,num_types:] # Recall that item context is a concatenation of pool and utility\n",
    "            utility_1 = utility_1[finishers].view(num_finishers,num_types)\n",
    "            utility_2 = item_contexts[id_2][:,num_types:]\n",
    "            utility_2 = utility_2[finishers].view(num_finishers,num_types)\n",
    "\n",
    "            log_p_1 = Agents[id_1].log_p[finishers].view(num_finishers,1)\n",
    "            log_p_2 = Agents[id_2].log_p[finishers].view(num_finishers,1)\n",
    "\n",
    "            # Calculate reward and reward losses\n",
    "            r1, rl1 = rewards_func(share_1, utility_1, pool_12, log_p_1, baselines[id_1])\n",
    "            r2, rl2 = rewards_func(share_2, utility_2, pool_12, log_p_2, baselines[id_2])\n",
    "            '''\n",
    "            for i in range(num_finishers):\n",
    "                print(r1[i], r2[i])\n",
    "                if r1[i]==0:\n",
    "                    print(share_2[i])\n",
    "                    print(share_1[i])\n",
    "                    print(utility_1[i])\n",
    "                    print(utility_2[i])\n",
    "                    print(pool_12[i])\n",
    "                #print(lt[i])\n",
    "                #print(lp[i])\n",
    "                #print(ll[i])\n",
    "                #print(baselines)\n",
    "            '''\n",
    "            \n",
    "            # Add rewards and reward losses\n",
    "            rewards[id_1] = r1.squeeze()\n",
    "            rewards[id_2] = r2.squeeze()\n",
    "            reward_losses[id_1] = rl1\n",
    "            reward_losses[id_2] = rl2\n",
    "            reward_sums[id_1] = reward_sums[id_1] + rewards[id_1].sum()\n",
    "            reward_sums[id_2] = reward_sums[id_2] + rewards[id_2].sum()\n",
    "\n",
    "        prev_proposals = proposals # Don't need previous proposals anymore so update it\n",
    "        \n",
    "        # Gradient descent -----------------------------------------------------------\n",
    "        \n",
    "        for i in range(num_agents):\n",
    "            # optimize\n",
    "            loss = reward_losses[i] + entropy_losses[i]\n",
    "            optimizers[i].zero_grad()\n",
    "            loss.backward()\n",
    "            optimizers[i].step()\n",
    "        \n",
    "        # Wrapping up the end of turn ------------------------------------------------\n",
    "        # Remove finished games\n",
    "        # In term and term_N, element = 1 means die\n",
    "        term_N = (N <= (i_turn+1)).view(num_alive,1).long() # Last turn reached; i_turn + 1 since i_turn starts counting from 0\n",
    "        # In survivors, element = 1 means live\n",
    "        survivors = (term+term_N) == 0\n",
    "\n",
    "        # Check if everyone's dead\n",
    "        if survivors.sum() == 0: # If all games over, break episode\n",
    "            # Baseline updates\n",
    "            for i in range(num_agents):\n",
    "                # Update with batch-averaged rewards\n",
    "                baselines[i] = smoothing_const * baselines[i] + (1-smoothing_const)*reward_sums[i]/num_games\n",
    "            break;\n",
    "            \n",
    "        # Reshape\n",
    "        survivors = ((term+term_N) == 0).nonzero()[:,0].view(-1,1)\n",
    "        num_alive = len(survivors) # Number of survivors\n",
    "\n",
    "        #print('i_turn = ' + str(i_turn))\n",
    "        \n",
    "    #print('i_ep = ' + str(i_ep))\n",
    "    if (i_ep % ep_time == 0) and (i_ep != 0):\n",
    "        time_p2 = time.time()\n",
    "        print('Runtime for episodes ' + str(i_ep-ep_time) + '-' + str(i_ep) + ': ' + str(time_p2 - time_p1) + 's')\n",
    "        time_p1 = time_p2\n",
    "        \n",
    "    if (i_ep % ep_record == 0):\n",
    "        for j in range(num_agents):\n",
    "            r_list[j].append(reward_sums[j]/num_games)\n",
    "    \n",
    "    #print('----------------')\n",
    "    \n",
    "print('End ------------------')\n",
    "time_finish = time.time()\n",
    "print('Total runtime: ' + str(time_finish-time_start) + 's')\n",
    "\n",
    "#for i in range(num_agents):\n",
    "#    torch.save(Agents[0].state_dict(),'saved_model_agent_' + str(i) + '.pt')\n",
    "    \n",
    "#Agents[0].load_state_dict(torch.load('saved_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         ...,\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "         [-0.3956, -0.0309,  0.8178,  ...,  0.7031, -0.2338,  0.4726],\n",
       "         [-0.0526,  1.8759, -0.5193,  ..., -0.0214, -1.1368, -0.0454],\n",
       "         ...,\n",
       "         [ 1.2561,  1.3435, -0.7072,  ...,  0.3423,  0.5639, -0.7454],\n",
       "         [-1.3382, -0.0348,  1.1147,  ..., -0.4353,  0.0818,  2.5513],\n",
       "         [-0.4034,  2.7296, -0.3818,  ...,  0.8555, -0.0709,  0.1424]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         ...,\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         ...,\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         ...,\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         ...,\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [[-4.9897,  7.9273, -7.1534,  ..., -1.5733,  1.0746, -7.8625],\n",
       "         [-8.3045, -9.1077,  8.8252,  ..., -6.3387, -2.0222, -1.2168],\n",
       "         [ 4.2525,  3.0842, -7.3089,  ..., -3.7768,  2.2078, -4.1774],\n",
       "         ...,\n",
       "         [ 5.7293,  1.6716, -3.7338,  ..., -4.1193,  3.1395, -7.6268],\n",
       "         [ 0.4562, -5.5581, -8.1713,  ..., -1.5914, -6.0851, -6.0692],\n",
       "         [ 9.3738, -6.1679, -8.8159,  ...,  2.7480, -7.7048,  1.6967]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[ 3.5687e-02, -1.6257e-02, -8.1897e-02,  ..., -4.3716e-02,\n",
       "          -5.5455e-02,  5.8361e-02],\n",
       "         [ 4.5466e-02, -7.5018e-02,  9.1287e-02,  ...,  4.2284e-02,\n",
       "           8.4800e-02, -5.9457e-04],\n",
       "         [-8.8670e-02, -3.1728e-03, -3.1320e-02,  ...,  1.2624e-03,\n",
       "           9.8489e-02, -5.9928e-02],\n",
       "         ...,\n",
       "         [-9.0822e-02, -1.1628e-02,  7.5518e-02,  ..., -3.7081e-02,\n",
       "           3.7510e-02,  2.9639e-02],\n",
       "         [-8.6731e-02,  9.5317e-02, -4.4712e-02,  ..., -6.9351e-02,\n",
       "          -6.4129e-02,  5.0160e-02],\n",
       "         [ 6.5983e-02,  8.8125e-02, -8.9883e-02,  ..., -2.7142e-02,\n",
       "          -2.1720e-03,  5.5583e-02]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [-6.9461,  1.4980, -5.8525,  6.0680,  7.3710,  9.2109, -3.9236,\n",
       "          7.2854, -2.8439, -5.6658, -1.7657, -8.6008,  2.1669, -1.2826,\n",
       "         -6.6877,  2.9615,  3.2906, -9.0263,  3.6039,  9.8347,  8.5116,\n",
       "          6.1568,  8.2833, -5.8681, -3.7165,  0.1026,  9.7395, -3.9042,\n",
       "          6.3059,  6.3025, -6.8940, -5.3976,  8.8460, -6.8485,  6.3132,\n",
       "          7.9817, -8.4129,  9.2679,  6.2693, -8.8745, -6.5056,  4.2293,\n",
       "          1.2315, -5.3922,  9.2523,  6.2886,  6.3617, -8.0655, -8.4013,\n",
       "         -8.9335,  6.3908, -9.8929,  7.6784,  0.2663,  3.5204,  8.8349,\n",
       "          0.7599, -6.8195,  7.0281,  3.7594, -4.2055,  2.5055,  5.0373,\n",
       "          7.3080,  6.6457,  3.6375,  4.2698, -8.4702, -5.1327, -7.1292,\n",
       "         -4.2798,  5.7516, -6.1447, -3.8542, -7.3904,  6.9003, -4.3356,\n",
       "         -5.3484,  6.1510,  6.6455, -1.2264, -4.3090, -0.2883,  7.1934,\n",
       "          2.5719, -2.0637,  9.5191,  5.9059, -4.4406, -7.6848, -0.4487,\n",
       "         -8.5867,  7.5109, -1.1195, -2.8133, -2.2650,  5.8793, -1.5773,\n",
       "         -4.6133,  1.2751, -9.9517,  7.3754,  4.7250,  4.4207, -7.3277,\n",
       "         -5.8125, -9.0930,  0.8286, -2.8017,  6.3863, -6.3677,  5.6586,\n",
       "         -7.3544,  9.8739, -3.8337, -6.7495, -5.9106, -6.4399, -8.6259,\n",
       "         -5.1595,  0.0740, -9.3180,  8.6645, -1.2092, -2.2701, -0.0216,\n",
       "          4.8452, -6.7301,  2.2584, -0.8411, -5.0777,  2.8061,  5.6839,\n",
       "         -0.7528,  4.5396,  1.3395,  2.0826,  4.0142,  4.5427, -8.5047,\n",
       "          1.2225, -2.8092,  9.9431, -2.6004,  4.4878,  1.2050,  6.9731,\n",
       "         -1.9892, -6.0511,  9.2609, -1.3211,  5.7656,  1.9878,  7.0622,\n",
       "          6.3904, -3.7825, -8.9316, -3.1440, -7.5401,  7.2689,  1.7315,\n",
       "         -3.9537,  0.6009,  6.7351,  9.9576,  6.2141,  1.3814,  2.5859,\n",
       "         -4.7399, -3.4537,  0.8826, -9.2186, -2.3490, -7.0405,  4.2042,\n",
       "         -2.8817, -3.4687,  3.8450,  4.6995, -8.0681, -0.6521, -2.3692,\n",
       "          9.0711,  5.4941,  5.8597,  6.1620,  0.2785, -6.8352, -2.4713,\n",
       "          7.0159, -0.5502, -1.8839,  9.0380,  9.1969, -3.0482,  3.1005,\n",
       "          8.5411,  7.9927,  3.0233, -1.3995, -2.8164, -8.5714, -8.3421,\n",
       "          3.6590,  0.5696, -0.8493,  4.6827,  8.9081,  8.7561,  0.4786,\n",
       "         -5.9623,  1.8069,  0.6197,  6.5453, -3.6340, -6.4812, -3.6920,\n",
       "          5.7259,  1.9113, -3.3371, -7.1374,  4.0883, -4.8431, -5.3688,\n",
       "          6.2971,  6.6559, -3.4850, -5.9530, -5.1916,  7.2151,  5.8260,\n",
       "         -2.8928, -4.9811,  5.2484, -4.2918,  7.2516, -8.9975,  4.6549,\n",
       "          3.7655, -9.0519, -7.8188,  8.2692, -3.8607,  1.6206,  9.0915,\n",
       "          0.9560,  1.3613,  3.5521,  5.8136,  0.7279, -2.3368, -9.3083,\n",
       "          2.7515,  9.9946,  7.7663, -2.6346,  4.2412, -2.5386, -1.6505,\n",
       "          3.2750, -2.0067,  9.8683, -4.0149,  4.1013,  5.4953, -6.8951,\n",
       "         -8.1038,  9.2888, -9.7283,  7.3275,  0.3655,  9.1896,  6.2804,\n",
       "          0.6148,  8.7848, -6.0854,  9.1504, -8.2748,  2.7330,  5.8901,\n",
       "          9.6393,  2.1046, -2.5033, -0.4068, -5.4589, -2.5439,  5.3669,\n",
       "         -3.3543, -4.8123, -3.5735,  2.2103,  2.8965,  9.1435,  9.8215,\n",
       "         -9.4714,  1.1835,  2.3470,  8.3378,  5.9190, -5.2424, -0.1169,\n",
       "          4.3842, -4.7632,  6.5648, -6.4738,  3.1028,  3.9814, -5.5574,\n",
       "          9.4214, -8.3282,  0.6599,  7.5679,  4.6634, -9.7886,  2.5257,\n",
       "          4.3848, -5.5872,  7.5091,  8.7422,  5.4527,  9.2247, -4.0902,\n",
       "          1.7134, -6.5688, -6.2250, -2.3322,  7.6190,  2.4538, -9.4527,\n",
       "         -7.4888,  2.6626, -5.6438, -6.3928,  4.7474,  7.4005, -7.2879,\n",
       "          3.0777, -3.8319, -6.1407, -8.4546, -0.1946,  7.5918,  3.8752,\n",
       "         -1.9872, -8.2773, -6.4323, -6.0297, -1.2678, -0.4909,  3.8756,\n",
       "         -0.9294,  5.5648,  1.7948, -3.7115, -9.0211,  5.3922,  9.7773,\n",
       "          7.5272, -2.9485,  6.1427,  4.3626, -8.5610, -3.7668,  2.6372,\n",
       "         -0.1802, -7.4657, -6.5111, -9.1750,  5.2694, -8.4931, -6.6555,\n",
       "          7.8848,  9.5497, -7.1671,  8.6374,  3.4206, -7.0570, -3.0123,\n",
       "          0.8030, -7.9561,  7.0536,  1.4806, -6.8131, -2.7378,  0.1240,\n",
       "         -9.6981,  1.6491, -3.3857, -3.6447, -3.9595, -0.5445,  7.3905,\n",
       "         -3.4783, -4.0315,  9.0240,  9.2041, -1.7940,  1.6875, -6.9444,\n",
       "          3.1540], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [-8.3393, -5.9950, -0.6829,  8.7684,  7.1455,  1.7593, -3.3962,\n",
       "          2.9411,  1.2052, -4.1504, -6.8350, -4.0702, -7.8806,  0.0644,\n",
       "          9.7526, -3.6220, -7.4862, -1.9777,  7.3950,  7.2143,  9.4051,\n",
       "          7.7354,  8.4634,  7.3025, -2.8350, -5.4065, -7.1907, -1.0424,\n",
       "          3.9626, -2.0777,  9.0893,  1.1845,  3.7863,  7.8479,  9.9010,\n",
       "          3.4901,  3.1903,  7.2552, -0.3876,  6.9845, -2.8246,  9.4428,\n",
       "         -4.4315, -5.9479, -1.6819, -1.6469, -2.4348, -3.4411,  6.5795,\n",
       "          5.3225,  3.9452,  6.8598, -1.5422,  8.6516,  0.2572,  1.4512,\n",
       "          8.5388, -3.2819, -1.3979,  7.4015,  1.0136, -7.9150,  0.5738,\n",
       "         -6.6733, -6.5187, -0.5422, -5.3607, -2.4794,  9.2472,  6.7557,\n",
       "         -3.2487,  9.4752, -8.8352, -2.9892, -5.0308, -3.5179, -5.6929,\n",
       "         -0.1817,  6.2077,  9.3850,  3.5304, -0.8226, -3.0451, -8.5307,\n",
       "          9.8799,  5.4701,  7.7326,  0.8502, -5.3727,  5.4263,  4.1720,\n",
       "          7.6343, -1.9026, -7.2567,  4.7320,  5.6503,  9.0851, -5.9474,\n",
       "         -8.5963, -4.7093, -8.8163, -4.3753,  8.9372,  5.1866, -9.8214,\n",
       "          3.0321,  4.0204, -6.0747,  3.0616,  6.3822, -1.1794,  4.0189,\n",
       "         -9.3645, -3.6433,  0.0861, -9.9955, -2.1243,  4.6130,  0.8185,\n",
       "          5.9204, -4.8643, -8.0619, -8.7858,  0.8086, -5.8405, -3.2948,\n",
       "         -9.3943,  8.2293, -9.6073, -3.7851, -4.4762, -7.1762, -5.8112,\n",
       "         -3.4225,  8.8971, -3.3201,  7.5230,  3.4507, -3.8372,  6.6614,\n",
       "          5.6936, -9.9112,  1.9046, -9.3115,  8.6764,  1.3223, -6.9845,\n",
       "          6.8903, -6.5129,  1.6067, -1.2617,  6.2796,  2.4054,  6.3020,\n",
       "         -8.0363,  7.6292,  9.7893, -7.7940,  9.2416,  7.1134, -9.1609,\n",
       "          3.6369,  9.9152,  9.6105, -4.7902,  8.7298,  7.5538, -4.5733,\n",
       "         -0.6946,  7.6495,  0.0203, -0.8101, -5.3652,  5.2940,  6.8569,\n",
       "         -3.4520, -1.8271, -0.2799, -3.6668,  4.4132,  6.1942, -7.3754,\n",
       "         -0.7847,  3.0467,  4.0855, -1.4149, -3.1124, -8.5972, -9.4552,\n",
       "         -4.2130,  3.8310,  3.7891, -6.8170,  1.4619,  7.7162, -1.5164,\n",
       "         -4.8577, -8.6084, -5.2216, -8.4547, -7.2623, -7.1383,  4.1202,\n",
       "          4.0929, -2.1540, -4.7543,  4.9236,  8.7454, -5.0233,  6.2399,\n",
       "          2.2348, -0.4510,  1.1817,  6.9338,  5.9756,  0.0798, -9.0500,\n",
       "          9.7549, -0.9024, -7.2015, -8.0638, -1.1425,  9.9158,  6.2579,\n",
       "          4.6851, -2.5619,  9.6975, -2.2898,  9.4039,  9.9298, -7.8949,\n",
       "          0.2252, -6.3831,  7.3778,  5.7443,  6.6246, -6.8941, -9.5296,\n",
       "          5.5380, -0.3410,  2.5584, -7.9526,  0.4723,  6.1564,  5.5024,\n",
       "         -6.9262, -5.6828, -2.9198, -2.4425,  3.1720, -4.1858,  9.7615,\n",
       "         -7.7418, -5.5537, -4.3249, -2.2208,  2.3596,  1.6472,  9.9568,\n",
       "         -2.5934,  0.4530,  1.3025, -9.4980, -9.2294, -6.3325,  2.1761,\n",
       "          1.0663, -5.7079, -7.8546, -1.4823,  8.8820,  2.4148, -8.2994,\n",
       "         -6.0889, -5.9236,  5.8512, -7.4986, -6.3192,  0.8395, -3.7443,\n",
       "          3.7859, -4.7953,  8.1393, -6.9162, -1.9320, -7.8937,  0.3937,\n",
       "          3.3467, -6.9376, -2.2389, -6.9268,  0.5271,  5.8923,  9.8216,\n",
       "         -8.8286,  6.5567,  0.7356,  4.4875, -0.5415, -9.3249,  5.5887,\n",
       "         -8.9347, -3.6209,  0.7820,  2.0638,  5.1845,  8.6964,  6.5052,\n",
       "         -5.0836, -9.6231, -6.0512, -7.0497,  1.0128, -5.3240,  9.6272,\n",
       "          8.1373,  3.7575,  9.3146, -4.3712,  6.2517,  0.0570, -3.6485,\n",
       "          2.1713,  0.1857, -5.9976, -0.8332, -8.1326,  3.7293,  6.6569,\n",
       "         -7.5188, -7.2452, -5.3983, -9.0478, -7.3587, -5.8640, -6.0037,\n",
       "         -7.8894,  8.4889, -0.4410, -1.1574,  6.1353, -1.6802, -8.8409,\n",
       "         -5.5017,  4.2154, -6.0500, -1.3400, -2.6747,  5.6086,  0.7198,\n",
       "         -1.2665, -3.7476,  8.1271, -9.8158,  7.0789, -5.9950, -5.4635,\n",
       "          2.7715,  3.2472,  2.5204, -7.9307,  8.5554,  4.3873,  4.5121,\n",
       "          0.1156,  5.8802, -6.1353,  6.1483, -1.3079,  2.5223, -6.2820,\n",
       "         -7.1831, -1.2476, -9.0649,  7.4518, -3.4206,  3.0344, -9.6128,\n",
       "          9.7322, -3.0953,  6.3011, -6.1737, -2.1498, -2.9660,  9.5961,\n",
       "          2.0213,  6.3944, -9.6423, -6.0517, -4.1512, -0.1353,  6.7831,\n",
       "          8.6119, -4.4245, -2.5534,  6.7136, -2.9174,  6.7288, -6.5785,\n",
       "          7.4438], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         ...,\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [    nan,  3.8268,  2.5283,     nan, -1.0945, -0.5818,     nan,\n",
       "             nan, -3.9859, -4.1038,  1.8646, -2.5663, -3.9969,     nan,\n",
       "         -6.1228,     nan,     nan,     nan, -3.9712,     nan, -2.1820,\n",
       "         -1.1619,     nan,     nan, -1.9722,     nan,  3.0406,  2.3372,\n",
       "             nan,  1.9513, -1.4669,     nan,     nan,     nan,  2.4599,\n",
       "             nan, -6.0442,     nan,     nan,     nan, -5.5441,     nan,\n",
       "             nan,     nan,  2.2822,  1.8075,  2.0488,     nan, -0.9824,\n",
       "          4.5960, -5.5615,     nan,     nan, -5.4945,     nan, -6.0221,\n",
       "             nan,     nan,     nan,     nan, -0.3568,     nan,     nan,\n",
       "         -3.1694,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "         -5.8219,     nan,     nan, -2.7450,     nan,     nan, -1.5162,\n",
       "          0.6937,     nan,     nan, -1.5562,     nan,     nan,  3.1980,\n",
       "         -1.7216,  1.8351,     nan,     nan,     nan, -0.6346,  1.0185,\n",
       "             nan, -1.2119, -5.7469, -3.6724,     nan,  0.9713,     nan,\n",
       "         -0.2820,     nan], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([nan.], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         ...,\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         ...,\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "         [nan., nan., nan.,  ..., nan., nan., nan.]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.],\n",
       "         [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.],\n",
       "         [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.],\n",
       "         [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.],\n",
       "         [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.],\n",
       "         [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.],\n",
       "         [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.],\n",
       "         [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.],\n",
       "         [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.],\n",
       "         [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
       "          nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0604, -0.0373, -0.0501,  0.0927, -0.0256, -0.0789, -0.0510,\n",
       "          -0.0623, -0.0273,  0.0552,  0.0599, -0.0046,  0.0999,  0.0103,\n",
       "           0.0441,  0.0347,  0.1039,  0.0744,  0.0132,  0.0795,  0.0155,\n",
       "          -0.0915,  0.0305, -0.0067,  0.0896, -0.0601,  0.0651, -0.0123,\n",
       "           0.0920,  0.0671, -0.0326,  0.0184,  0.0278, -0.0355, -0.0639,\n",
       "           0.0886, -0.0281,  0.0810,  0.0419,  0.1091,  0.0865,  0.0735,\n",
       "          -0.0525, -0.0038, -0.0491,  0.0884, -0.0230, -0.0366, -0.0862,\n",
       "          -0.0391,  0.0543,  0.0042,  0.0294,  0.0008,  0.1072,  0.0185,\n",
       "           0.0250,  0.1115,  0.1217,  0.0962,  0.0934,  0.0972,  0.0979,\n",
       "          -0.0216,  0.0475,  0.0830, -0.0657, -0.0273, -0.0511,  0.1035,\n",
       "           0.0335, -0.0501,  0.1137,  0.0757,  0.0759,  0.0743,  0.0227,\n",
       "           0.0169,  0.0481, -0.0323, -0.0023, -0.0450,  0.0982, -0.0582,\n",
       "          -0.0082, -0.0374,  0.0662,  0.0123, -0.0105, -0.0241, -0.0302,\n",
       "           0.0374, -0.0439,  0.0469,  0.0563,  0.0214, -0.0873,  0.0705,\n",
       "          -0.0360,  0.0313],\n",
       "         [ 0.0564, -0.0365,  0.0559, -0.0341, -0.0143,  0.0390,  0.1213,\n",
       "           0.1228,  0.0814,  0.0775,  0.0292,  0.0050, -0.0158,  0.0371,\n",
       "          -0.0126,  0.0902,  0.1008, -0.0451, -0.0734, -0.0092, -0.0657,\n",
       "           0.0743,  0.0123,  0.0742, -0.0663,  0.0993, -0.0929,  0.1074,\n",
       "           0.0370, -0.0614, -0.0209,  0.0693,  0.0106,  0.0902,  0.0748,\n",
       "           0.0834, -0.0240,  0.0971,  0.0472,  0.0649,  0.0355,  0.0269,\n",
       "           0.0909,  0.0776,  0.0021,  0.0280,  0.0443, -0.0478,  0.0338,\n",
       "          -0.0506,  0.0467,  0.0645, -0.0071, -0.0802,  0.1140, -0.0213,\n",
       "          -0.0391,  0.1160, -0.0692, -0.0076, -0.0817,  0.0805, -0.0024,\n",
       "           0.0480,  0.0784,  0.1121,  0.0761,  0.0415,  0.1024,  0.0114,\n",
       "          -0.0644,  0.0614, -0.0075, -0.0747,  0.0789,  0.0372, -0.0609,\n",
       "           0.0858,  0.0020,  0.0440, -0.0295,  0.0685,  0.1065,  0.0096,\n",
       "          -0.0018, -0.0194, -0.0088,  0.0792,  0.0168,  0.0930, -0.0097,\n",
       "          -0.0341, -0.0560,  0.0958, -0.0477,  0.0447, -0.0903,  0.0384,\n",
       "           0.0450,  0.0708],\n",
       "         [ 0.0170,  0.0075, -0.0683,  0.0797,  0.0118,  0.0559,  0.0282,\n",
       "          -0.0420, -0.0563, -0.0012,  0.0442, -0.0225, -0.0768,  0.0703,\n",
       "          -0.0763,  0.0756, -0.0070, -0.0259, -0.0553,  0.0488, -0.0293,\n",
       "           0.0408, -0.0484,  0.1121, -0.0238,  0.0184, -0.0903, -0.0027,\n",
       "          -0.0138,  0.0024,  0.0077, -0.0132,  0.0107,  0.0451, -0.0327,\n",
       "          -0.0289,  0.0752, -0.0013,  0.1177,  0.1080,  0.0715,  0.0458,\n",
       "           0.0845,  0.1205, -0.0082, -0.0689,  0.0436,  0.0257,  0.0539,\n",
       "          -0.0295, -0.0506,  0.0361, -0.0583, -0.0114,  0.0860,  0.0586,\n",
       "          -0.0195, -0.0203, -0.0533,  0.0823, -0.0280,  0.0753,  0.0371,\n",
       "          -0.0417,  0.0429,  0.0169,  0.0790,  0.0242, -0.0564, -0.0446,\n",
       "          -0.0913,  0.1201,  0.1219,  0.0052,  0.0371,  0.0635,  0.0301,\n",
       "          -0.0320, -0.0560, -0.0675, -0.0078, -0.0647,  0.0250, -0.0329,\n",
       "          -0.0307,  0.0871,  0.1242, -0.0431,  0.0621, -0.0624,  0.0018,\n",
       "           0.1096,  0.0520,  0.0910, -0.0847,  0.0538,  0.0150, -0.0675,\n",
       "           0.0225,  0.0329],\n",
       "         [ 0.0363, -0.0550,  0.0184,  0.0801, -0.0365,  0.0687,  0.0438,\n",
       "           0.0685,  0.0314,  0.0432,  0.0435,  0.0894,  0.0598,  0.0653,\n",
       "          -0.0650, -0.0175, -0.0075,  0.0592,  0.0402,  0.1226,  0.0200,\n",
       "          -0.0331, -0.0430, -0.0725,  0.0609,  0.0715, -0.0383, -0.0144,\n",
       "          -0.0077, -0.0581, -0.0328,  0.0733,  0.0258,  0.1098,  0.0330,\n",
       "          -0.0146,  0.0192,  0.1013,  0.0457, -0.0044,  0.0894,  0.0270,\n",
       "           0.0363,  0.0069, -0.0553,  0.0237, -0.0370, -0.0461, -0.0672,\n",
       "          -0.0310, -0.0559,  0.1100,  0.0888,  0.0036, -0.0489, -0.0007,\n",
       "           0.0195,  0.0471,  0.0741,  0.1233, -0.0278,  0.1126,  0.0300,\n",
       "           0.1045, -0.0234, -0.0268,  0.0168,  0.0833,  0.0491,  0.1068,\n",
       "          -0.0383,  0.0503,  0.1055,  0.0476,  0.0996, -0.0170, -0.0112,\n",
       "          -0.0109,  0.1066,  0.0051,  0.0649,  0.0469,  0.1130,  0.0707,\n",
       "           0.0067,  0.0008, -0.0661, -0.0618,  0.1179, -0.0581, -0.0364,\n",
       "           0.0393,  0.0643, -0.0938,  0.0911, -0.0211,  0.0365,  0.0297,\n",
       "           0.0025,  0.0442],\n",
       "         [ 0.1175, -0.0736, -0.0351,  0.0280, -0.0521,  0.0010,  0.0292,\n",
       "           0.0474,  0.0042,  0.0505, -0.0815, -0.0627,  0.0557,  0.0060,\n",
       "          -0.0759, -0.0534,  0.0502,  0.0897,  0.0706,  0.0626,  0.0602,\n",
       "          -0.0679,  0.0236,  0.0879, -0.0078,  0.1124, -0.0486, -0.0791,\n",
       "           0.0732,  0.0981,  0.0253,  0.0515, -0.0008, -0.0606,  0.0086,\n",
       "           0.0371,  0.0563,  0.0937, -0.0573,  0.0001,  0.0769,  0.0457,\n",
       "           0.0213,  0.0369,  0.0227,  0.0235, -0.0333,  0.0738, -0.0087,\n",
       "           0.0075, -0.0765,  0.0438,  0.0023, -0.0482,  0.1225,  0.0633,\n",
       "           0.0007, -0.0167,  0.0588, -0.0292,  0.0462,  0.0351, -0.0085,\n",
       "           0.0773, -0.0567,  0.1158,  0.0284,  0.1009, -0.0514,  0.0333,\n",
       "           0.0286,  0.1011, -0.0478, -0.0479,  0.0951,  0.0222, -0.0573,\n",
       "           0.0059,  0.0917,  0.0085, -0.0205, -0.0293,  0.1008, -0.0825,\n",
       "          -0.0681, -0.0342,  0.0323, -0.0119, -0.0680, -0.0563,  0.0156,\n",
       "           0.0297,  0.0590, -0.0054,  0.0631, -0.0071, -0.0087,  0.0247,\n",
       "          -0.0170,  0.1285],\n",
       "         [ 0.1021,  0.0808, -0.0056, -0.0324, -0.0405,  0.0273,  0.0600,\n",
       "           0.0536, -0.0294,  0.0709, -0.0717, -0.0510,  0.0035,  0.0493,\n",
       "          -0.0433,  0.0088,  0.1135, -0.0141, -0.0467, -0.0664,  0.0309,\n",
       "          -0.0606, -0.0165,  0.0302,  0.0233,  0.0364, -0.0448,  0.0260,\n",
       "           0.0826,  0.0505,  0.0714, -0.0649,  0.0620, -0.0479,  0.0013,\n",
       "          -0.0338, -0.0584, -0.0256,  0.1240,  0.1062,  0.0631,  0.0518,\n",
       "           0.0985,  0.0248, -0.0677, -0.0012, -0.0053,  0.0746,  0.0573,\n",
       "           0.0391,  0.0178,  0.0533,  0.0821, -0.0468, -0.0599,  0.0744,\n",
       "           0.1234,  0.0785,  0.0747, -0.0290,  0.0719,  0.0048,  0.0689,\n",
       "           0.0810,  0.0190,  0.0899,  0.0894, -0.0124, -0.0104, -0.0245,\n",
       "           0.0680,  0.1110,  0.0321,  0.0725,  0.0616,  0.0685,  0.1050,\n",
       "          -0.0387,  0.0695, -0.0097, -0.0464, -0.0090, -0.0331, -0.0109,\n",
       "           0.0525,  0.0085,  0.1193,  0.1004,  0.0291,  0.0937, -0.0821,\n",
       "          -0.0328, -0.0086, -0.0158, -0.0820, -0.0625,  0.0868,  0.0314,\n",
       "           0.0381,  0.0022]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([ 6.0395,  6.0684,  6.0284,  6.0612,  6.0511,  6.0385], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[ 0.1082,  0.0809, -0.0919,  0.0814,  0.0013,  0.0681,  0.1074,\n",
       "           0.0469, -0.0155, -0.0175, -0.0289, -0.0570,  0.0172,  0.1163,\n",
       "           0.0507,  0.0487,  0.0288,  0.0992, -0.0306, -0.0417, -0.0547,\n",
       "          -0.0862,  0.0595,  0.0642, -0.0853,  0.0349, -0.0345, -0.0164,\n",
       "          -0.0153, -0.0268, -0.0428,  0.0779,  0.0659,  0.0895, -0.0561,\n",
       "           0.0807, -0.0888,  0.0059,  0.1204,  0.0126,  0.0829,  0.0755,\n",
       "          -0.0425,  0.0203, -0.0739, -0.0122, -0.0097,  0.1156,  0.0520,\n",
       "          -0.0236, -0.0314,  0.0810, -0.0575, -0.0574, -0.0352,  0.0943,\n",
       "          -0.0020,  0.0791, -0.0252,  0.0659,  0.0860,  0.0127,  0.0057,\n",
       "          -0.0436,  0.0838, -0.0235, -0.0178,  0.0216,  0.1041, -0.0323,\n",
       "          -0.0873,  0.0110,  0.1239, -0.0650,  0.0558,  0.0794, -0.0084,\n",
       "           0.0822,  0.0674, -0.0282, -0.0154,  0.0274,  0.0812,  0.0900,\n",
       "          -0.0166, -0.0431,  0.0689,  0.0502,  0.0982,  0.0077, -0.0520,\n",
       "          -0.0041,  0.0415, -0.0102, -0.0856, -0.0013,  0.0018,  0.0023,\n",
       "          -0.0705,  0.0090],\n",
       "         [ 0.0703, -0.0682, -0.0431,  0.0260, -0.0003, -0.0454,  0.0318,\n",
       "           0.0121, -0.0232,  0.0435,  0.0174,  0.0133,  0.0160,  0.0121,\n",
       "          -0.0737,  0.0976, -0.0137,  0.0971,  0.0364,  0.0219,  0.0369,\n",
       "          -0.0711,  0.0966,  0.0693, -0.0693,  0.0138, -0.0285,  0.0405,\n",
       "           0.0503,  0.0273, -0.0587,  0.0333,  0.0125, -0.0038,  0.0309,\n",
       "           0.1103, -0.0724,  0.0076,  0.0392,  0.0774, -0.0499,  0.0662,\n",
       "           0.0783,  0.0159,  0.1057, -0.0537,  0.0821,  0.0586, -0.0200,\n",
       "          -0.0643, -0.0835,  0.0869,  0.1042, -0.0241,  0.0153, -0.0934,\n",
       "           0.0508, -0.0303,  0.1125,  0.0923,  0.0740,  0.0765,  0.0246,\n",
       "          -0.0441,  0.0953, -0.0681, -0.0588,  0.0595, -0.0017,  0.1087,\n",
       "           0.0689, -0.0498,  0.0605,  0.0958, -0.0363,  0.0748, -0.0630,\n",
       "          -0.0112,  0.1065,  0.0910,  0.0571,  0.0791,  0.0129,  0.0114,\n",
       "           0.0837, -0.0445,  0.0908,  0.0056,  0.0376,  0.0223,  0.0710,\n",
       "           0.0486, -0.0110,  0.0740,  0.0893,  0.0566,  0.0171, -0.0648,\n",
       "           0.0583,  0.0894],\n",
       "         [ 0.0032, -0.0460, -0.0664, -0.0094,  0.0832,  0.0950, -0.0143,\n",
       "           0.0782, -0.0384,  0.0278, -0.0769, -0.0825, -0.0070,  0.0088,\n",
       "           0.0856,  0.0995,  0.0125,  0.0852,  0.0269,  0.0034, -0.0438,\n",
       "          -0.0724,  0.0885,  0.0692, -0.0853,  0.0873, -0.0894,  0.0085,\n",
       "          -0.0484, -0.0579,  0.0119, -0.0707,  0.0653, -0.0066, -0.0200,\n",
       "           0.0453, -0.0786,  0.1256,  0.0574, -0.0214, -0.0883,  0.0096,\n",
       "          -0.0445, -0.0450,  0.0027, -0.0556,  0.0245, -0.0478,  0.0320,\n",
       "           0.0409,  0.0767,  0.0753, -0.0314,  0.0079,  0.0222, -0.0637,\n",
       "           0.0119,  0.0674,  0.1120,  0.0800, -0.0013,  0.1177,  0.0449,\n",
       "          -0.0537,  0.1077, -0.0655,  0.1221, -0.0133,  0.0992,  0.0880,\n",
       "           0.0320,  0.1215,  0.0579, -0.0363,  0.1086,  0.1140,  0.0110,\n",
       "          -0.0591,  0.0237,  0.1069, -0.0633,  0.0646,  0.0378,  0.0456,\n",
       "          -0.0276, -0.0110,  0.0550, -0.0318,  0.0283,  0.0591, -0.0607,\n",
       "           0.0092,  0.0001, -0.0929, -0.0017,  0.0983, -0.0185, -0.0575,\n",
       "          -0.0293, -0.0116],\n",
       "         [ 0.0080,  0.0973, -0.0807,  0.0031,  0.0436, -0.0924,  0.0449,\n",
       "          -0.0494, -0.0894,  0.0833,  0.0981,  0.0458,  0.0297,  0.1057,\n",
       "          -0.0825,  0.1005, -0.0596,  0.0682,  0.0692,  0.0849, -0.0358,\n",
       "           0.0019,  0.0758,  0.0750, -0.0743, -0.0241, -0.0357, -0.0854,\n",
       "           0.0113, -0.0642,  0.0255, -0.0440,  0.1014, -0.0346,  0.0724,\n",
       "           0.0407,  0.0259, -0.0265,  0.1020, -0.0692,  0.0975,  0.0761,\n",
       "           0.0546,  0.1038, -0.0091,  0.0521, -0.0683,  0.0586, -0.0345,\n",
       "          -0.0035, -0.0139,  0.0557,  0.1202,  0.0616,  0.1143, -0.0415,\n",
       "           0.0183, -0.0498, -0.0615,  0.0874, -0.0409,  0.0262,  0.0716,\n",
       "          -0.0416, -0.0222,  0.1059,  0.0083, -0.0345, -0.0348, -0.0012,\n",
       "           0.0166,  0.0865,  0.0710,  0.0772,  0.0993, -0.0351, -0.0454,\n",
       "          -0.0304,  0.0244,  0.0858,  0.0912, -0.0487,  0.0324, -0.0396,\n",
       "          -0.0210,  0.0731, -0.0439,  0.1025, -0.0237, -0.0579, -0.0547,\n",
       "           0.0050, -0.0447, -0.0200, -0.0527, -0.0666, -0.0174,  0.0945,\n",
       "           0.0875,  0.0951],\n",
       "         [ 0.0046,  0.0069,  0.0797,  0.0987,  0.0734, -0.0811, -0.0298,\n",
       "           0.1166, -0.0583, -0.0615, -0.0349, -0.0743, -0.0843,  0.0662,\n",
       "           0.0075,  0.0189,  0.0058,  0.1187, -0.0700,  0.0835, -0.0805,\n",
       "          -0.0029,  0.0300,  0.0601,  0.0856,  0.0807,  0.0998,  0.0082,\n",
       "           0.0577,  0.0297, -0.0482,  0.1244,  0.0712, -0.0337, -0.0045,\n",
       "          -0.0144,  0.0788,  0.0041,  0.0797,  0.0703,  0.0072,  0.0779,\n",
       "           0.0403,  0.0147,  0.0265,  0.0734, -0.0349,  0.0963, -0.0022,\n",
       "          -0.0650, -0.0786,  0.0350,  0.1258, -0.0103, -0.0222, -0.0738,\n",
       "           0.1094,  0.0902,  0.0418,  0.0088,  0.1044,  0.0148,  0.1167,\n",
       "          -0.0783, -0.0249, -0.0328,  0.0030,  0.1272, -0.0689, -0.0209,\n",
       "          -0.0372, -0.0254,  0.0549,  0.0274,  0.0896,  0.0683,  0.0166,\n",
       "           0.0454, -0.0537, -0.0356,  0.0093,  0.0291,  0.1219,  0.0545,\n",
       "           0.0279,  0.0090,  0.0450, -0.0648, -0.0115,  0.0846, -0.0879,\n",
       "           0.0180,  0.0084,  0.0071, -0.0725, -0.0554, -0.0581,  0.0488,\n",
       "           0.0295, -0.0123],\n",
       "         [-0.0459,  0.0228, -0.0826, -0.0082,  0.0178,  0.0408, -0.0388,\n",
       "           0.1090,  0.0115,  0.0978, -0.0704,  0.0093,  0.0260,  0.0812,\n",
       "          -0.0364, -0.0377, -0.0614,  0.1113, -0.0014,  0.0436,  0.0179,\n",
       "          -0.0587,  0.0999, -0.0014,  0.0483,  0.1124, -0.0454, -0.0491,\n",
       "           0.0110,  0.0042, -0.0812, -0.0223,  0.1260,  0.0045, -0.0440,\n",
       "           0.0424, -0.0252,  0.0879,  0.0791, -0.0104, -0.0577,  0.0979,\n",
       "           0.1284,  0.0090, -0.0869, -0.0676,  0.0056,  0.0279, -0.0284,\n",
       "           0.0713, -0.0766, -0.0300,  0.1186, -0.0738, -0.0575, -0.0177,\n",
       "           0.0422,  0.0177, -0.0031,  0.0057, -0.0199, -0.0656, -0.0032,\n",
       "          -0.0427,  0.0279,  0.0903,  0.0845,  0.0269,  0.1000, -0.0579,\n",
       "          -0.0258,  0.0293, -0.0150,  0.0205, -0.0505,  0.0942, -0.0278,\n",
       "           0.0818, -0.0368, -0.0571, -0.0416,  0.0610, -0.0225, -0.0530,\n",
       "           0.0311,  0.0093,  0.0640,  0.1167,  0.0216, -0.0978,  0.1052,\n",
       "           0.1278, -0.0930,  0.0316, -0.0079, -0.0137,  0.0380,  0.0302,\n",
       "           0.0157,  0.0758]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([ 6.0421,  6.0577,  6.0625,  6.0579,  6.0671,  6.0306], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[ 8.4184e-03,  2.4707e-02, -3.7126e-02,  5.8058e-02,  2.7046e-02,\n",
       "          -2.6078e-02,  8.8114e-02,  6.2807e-02,  1.6126e-02,  6.6117e-02,\n",
       "           6.7909e-02, -6.4129e-02, -5.1941e-02,  4.5311e-02,  4.2512e-02,\n",
       "           3.3032e-02,  5.3452e-02,  4.1066e-03, -9.2574e-02, -5.5545e-02,\n",
       "           1.0248e-01, -7.8986e-02, -5.4752e-02, -2.6106e-02,  6.9860e-02,\n",
       "           9.1357e-02, -3.9922e-03,  2.1906e-02,  8.8179e-02, -7.8045e-03,\n",
       "           1.0365e-03, -3.8138e-02,  7.1561e-02,  1.0981e-02,  1.0428e-01,\n",
       "          -6.2825e-02,  3.1815e-02, -1.4631e-02, -1.3902e-02, -3.5136e-03,\n",
       "          -2.5820e-02,  1.0083e-01,  1.2375e-01,  3.2837e-03, -3.3880e-02,\n",
       "          -1.2902e-02,  9.3917e-02, -4.2529e-02, -9.0384e-02, -4.1558e-02,\n",
       "          -9.2292e-02,  9.5031e-02,  6.4370e-02, -2.7641e-02, -1.8855e-02,\n",
       "           7.2670e-02, -6.2388e-02,  1.1647e-02,  1.0686e-02,  1.5856e-02,\n",
       "           4.9840e-02, -4.9673e-02,  1.1675e-01, -7.8873e-02,  1.6697e-03,\n",
       "           2.5932e-02, -4.3680e-02,  9.1883e-02, -3.8589e-02,  1.1144e-01,\n",
       "           6.7456e-03,  1.2871e-01,  8.9055e-02, -2.3469e-02, -1.9045e-03,\n",
       "           1.0048e-01, -3.4330e-02, -4.8216e-02, -4.0293e-02,  6.1831e-02,\n",
       "          -8.6964e-02,  2.5355e-02,  6.2207e-02,  5.3927e-03, -3.4016e-02,\n",
       "          -9.2566e-02,  1.5536e-02, -5.7825e-03,  9.1354e-02, -3.2687e-03,\n",
       "          -5.5060e-02,  7.4001e-02,  8.3064e-02, -1.5229e-02,  8.1444e-02,\n",
       "           9.9316e-02,  1.1299e-02,  1.2643e-01, -9.5332e-03, -1.4445e-02],\n",
       "         [-5.5272e-02, -4.5848e-02, -1.1472e-03, -2.9197e-02, -6.6859e-02,\n",
       "          -3.4922e-02,  6.0756e-02,  2.0365e-02,  5.6975e-02,  2.5960e-02,\n",
       "          -2.0840e-02,  7.7962e-02, -2.8865e-02,  1.0995e-01,  4.5833e-02,\n",
       "           7.1913e-03,  4.7697e-02,  1.0355e-01,  5.0892e-02,  9.3829e-02,\n",
       "          -1.6232e-02,  3.2148e-02,  3.2241e-02,  9.0767e-02,  5.8280e-02,\n",
       "          -7.2195e-02, -3.5448e-02, -8.3145e-02,  1.5016e-02, -1.0684e-02,\n",
       "           7.5009e-02,  4.0062e-02,  1.2184e-01,  7.0773e-02, -8.3065e-02,\n",
       "           6.7716e-02, -3.4378e-02,  9.6409e-02, -3.7332e-02,  2.5135e-02,\n",
       "          -3.2096e-02, -3.6170e-02,  2.9781e-02, -2.5826e-03,  9.3618e-02,\n",
       "          -8.9485e-02, -7.6336e-02, -1.3591e-02, -7.3027e-02,  9.8436e-03,\n",
       "          -5.2325e-02,  3.2685e-02,  1.2647e-01,  4.9906e-02,  3.8379e-03,\n",
       "           3.0991e-02,  1.0979e-01, -3.0091e-03,  1.2399e-01,  6.1167e-03,\n",
       "          -4.7617e-02,  3.9521e-02,  1.0523e-01, -2.5033e-02,  8.4840e-02,\n",
       "           5.6047e-02,  1.2029e-01, -4.0723e-02,  3.2424e-02, -2.1364e-02,\n",
       "           9.0072e-02,  1.3197e-03,  4.0031e-02, -1.2399e-02, -2.0990e-02,\n",
       "           1.1061e-01,  8.1925e-02,  1.2865e-03, -4.4035e-02,  5.6432e-02,\n",
       "           2.8434e-02,  4.5632e-02,  9.5187e-02, -6.1354e-02, -1.3795e-03,\n",
       "          -7.7586e-02, -2.5548e-02,  2.4766e-02, -3.2836e-02,  9.4052e-02,\n",
       "           7.1816e-02,  3.0196e-02,  6.8523e-02,  9.0851e-02, -5.0961e-02,\n",
       "           1.1226e-01, -7.3235e-02, -2.9448e-02, -8.6633e-02,  4.0014e-02],\n",
       "         [ 3.5699e-02, -1.4030e-02, -4.5574e-02,  7.5270e-02, -3.9860e-02,\n",
       "           5.7681e-02,  8.4842e-02, -1.1705e-02,  4.3555e-02, -8.9223e-02,\n",
       "           2.2758e-02,  2.7872e-02, -1.9887e-02,  6.8207e-02, -1.9063e-02,\n",
       "           1.9371e-02,  1.7438e-02,  1.1759e-01, -7.9760e-02,  1.7130e-02,\n",
       "           1.0366e-01, -1.3021e-02,  1.5621e-02,  1.1203e-01,  4.5391e-02,\n",
       "          -3.0359e-02, -1.9805e-03,  8.7710e-02,  3.0964e-02,  3.6277e-02,\n",
       "           6.8255e-02,  6.9865e-02,  6.1624e-02,  1.2771e-01,  4.4184e-02,\n",
       "          -4.7522e-02,  2.3503e-04, -2.4068e-02,  3.4381e-02,  2.6156e-02,\n",
       "          -2.8573e-02, -3.7763e-02,  8.4094e-02,  7.8293e-02,  1.4660e-02,\n",
       "          -6.6570e-02, -2.0754e-02, -3.1535e-02,  1.6435e-02, -1.4796e-03,\n",
       "          -8.6203e-02, -4.0253e-02, -7.1928e-02,  9.0838e-02, -1.7805e-03,\n",
       "          -5.9967e-02,  1.1257e-02,  8.6010e-02,  1.1243e-01,  2.7515e-02,\n",
       "          -7.0203e-02,  3.6713e-02,  1.0963e-01,  2.6866e-03,  2.6549e-02,\n",
       "           2.7478e-02, -5.3939e-02,  4.2256e-02, -1.0167e-02,  2.2826e-02,\n",
       "           7.2907e-02,  1.1056e-01,  1.7583e-02,  5.4884e-02,  1.1130e-02,\n",
       "           5.3244e-02,  6.9130e-02, -5.9314e-02,  1.0277e-01,  1.1892e-01,\n",
       "          -4.4547e-03,  1.0291e-01,  8.8863e-05, -5.9984e-02,  1.6690e-02,\n",
       "           5.5211e-02, -4.9815e-02,  3.8643e-02, -1.4482e-02, -1.6559e-02,\n",
       "           6.1841e-02,  8.6234e-02,  4.1866e-02,  1.9238e-02,  3.3747e-02,\n",
       "          -2.7421e-02,  3.1715e-02,  1.1585e-01, -6.7628e-02,  1.1113e-01],\n",
       "         [ 1.0185e-01,  5.5288e-02, -6.7356e-02,  7.8236e-03,  7.8289e-02,\n",
       "          -1.4257e-02,  5.9381e-02, -1.7861e-02,  2.5582e-02, -2.0179e-02,\n",
       "          -8.6285e-02, -7.7614e-02, -1.1863e-02, -6.4728e-02, -4.2186e-02,\n",
       "           6.9013e-02,  1.0093e-01,  5.0418e-02, -7.6360e-02,  8.3245e-03,\n",
       "          -3.9644e-02, -1.2431e-02,  1.2062e-01, -2.8896e-02, -6.6228e-02,\n",
       "           8.3425e-02,  4.6206e-02, -2.8600e-02,  2.6639e-02,  9.4045e-02,\n",
       "          -6.9441e-02,  1.2346e-01,  1.0706e-01,  1.2274e-01, -6.1006e-02,\n",
       "           5.1471e-02, -3.4719e-02, -1.5083e-02,  5.0230e-03,  7.9808e-02,\n",
       "          -2.6886e-02,  1.4023e-02,  4.8080e-02, -4.4521e-02, -3.7670e-02,\n",
       "           9.9247e-02, -1.2132e-02, -2.5601e-03,  1.9980e-02,  5.2605e-02,\n",
       "          -6.1913e-02,  7.6491e-02, -2.7957e-03,  2.4588e-02, -2.0460e-02,\n",
       "          -9.2779e-02, -6.6633e-02,  1.0757e-01,  1.2058e-01, -1.3404e-02,\n",
       "          -3.1591e-02, -1.9451e-02,  1.8529e-03,  8.2943e-02, -2.5028e-02,\n",
       "          -5.1656e-02,  5.2656e-02,  1.1192e-01, -6.3345e-02,  1.2250e-01,\n",
       "           1.0028e-01,  1.1097e-02,  7.4977e-02, -4.0682e-02,  8.9081e-02,\n",
       "          -2.2110e-02,  5.5994e-02, -1.8109e-02, -9.5297e-03,  8.6688e-02,\n",
       "          -6.0249e-02,  6.9097e-02, -8.9356e-03, -8.5332e-02,  2.9495e-02,\n",
       "          -6.5903e-03,  3.5707e-02,  3.9188e-02,  4.6471e-02,  7.2941e-02,\n",
       "          -8.2027e-02,  5.0616e-02, -5.9009e-06, -8.5445e-03,  1.0126e-01,\n",
       "           1.1821e-01, -5.9305e-02,  3.3165e-02,  2.6876e-02,  2.0927e-02],\n",
       "         [-2.8909e-02,  1.5130e-02,  1.7866e-02, -5.7671e-02, -7.2273e-02,\n",
       "          -6.0605e-04,  7.6447e-02,  8.3190e-02, -7.5839e-02,  2.1707e-02,\n",
       "           8.3060e-02,  1.5441e-02,  6.0446e-02,  1.1788e-01, -6.2598e-02,\n",
       "           8.5715e-02, -3.1609e-02,  1.6826e-02, -3.6268e-02,  1.1341e-01,\n",
       "           1.3593e-02,  4.2270e-02,  5.8017e-02,  1.2460e-01, -4.1899e-02,\n",
       "           3.1660e-02, -1.7799e-02,  6.8244e-02,  1.1738e-01,  4.1565e-02,\n",
       "           9.6777e-02,  8.3538e-02,  3.6182e-02,  9.6062e-02, -7.8630e-03,\n",
       "          -2.6270e-02,  6.7760e-02,  1.2743e-02,  4.5398e-03, -6.5451e-02,\n",
       "           7.1461e-02,  9.2838e-03, -3.8279e-02, -2.9545e-02, -5.6763e-02,\n",
       "           9.3445e-02, -8.5025e-02,  8.0929e-03,  8.0205e-02, -3.8138e-02,\n",
       "           8.3341e-02, -1.9916e-02,  8.4417e-02,  2.4037e-02,  8.7560e-02,\n",
       "           5.6605e-02,  3.2575e-02,  7.2425e-02,  4.6480e-02,  1.0496e-01,\n",
       "          -4.4596e-02,  9.6915e-02,  5.4990e-02, -8.3301e-02, -2.4427e-02,\n",
       "           1.2130e-01, -6.3244e-02,  4.4375e-02,  5.1050e-02,  1.1747e-01,\n",
       "          -1.1015e-02,  5.2467e-02, -7.2229e-02, -7.3597e-02,  3.3977e-03,\n",
       "          -3.4818e-02, -4.7975e-02,  1.5033e-03,  5.0871e-02,  1.2463e-01,\n",
       "           6.4255e-02, -5.4472e-02,  2.9614e-02, -4.3988e-02, -4.7677e-02,\n",
       "           5.8606e-02,  1.1774e-01,  4.8050e-02,  8.5589e-02, -2.1642e-02,\n",
       "           1.9954e-02,  5.2746e-03,  7.9041e-02, -8.2605e-03, -2.8578e-02,\n",
       "           9.3012e-02,  3.1548e-02,  9.3960e-02,  4.1915e-02,  8.1749e-02],\n",
       "         [ 3.4367e-02,  5.5637e-02,  2.5673e-02,  5.8943e-02, -4.0259e-02,\n",
       "           9.1023e-03, -4.6704e-02, -6.8366e-02, -8.5043e-02, -5.3086e-02,\n",
       "          -9.7086e-02,  8.2480e-02, -6.8488e-02,  9.0226e-02, -2.7855e-02,\n",
       "          -2.7228e-02,  8.9590e-02,  6.9713e-03,  5.9714e-02,  9.1071e-02,\n",
       "          -1.9903e-03, -7.5975e-02,  1.1335e-01,  1.1008e-01,  1.7401e-02,\n",
       "           9.2312e-02,  1.7538e-02,  1.5734e-02,  5.1710e-02,  1.5336e-02,\n",
       "           3.9298e-02,  1.7500e-02,  1.1341e-01, -5.6683e-02, -2.5537e-02,\n",
       "           1.0528e-01,  2.2958e-02,  1.1888e-01, -2.1621e-02, -2.3465e-02,\n",
       "          -4.3709e-02, -6.7195e-02,  9.5927e-02,  1.2628e-01,  4.7778e-02,\n",
       "          -4.6040e-02, -6.9034e-03,  4.8657e-02,  3.4154e-02, -2.1984e-02,\n",
       "           6.7778e-02, -3.4626e-03,  8.9565e-02,  1.8495e-02,  2.4226e-03,\n",
       "          -8.8356e-02,  6.8137e-02,  2.0399e-02,  5.8282e-02, -4.3148e-02,\n",
       "          -8.5319e-02, -5.4567e-02,  8.2762e-02,  6.6627e-02,  7.4112e-02,\n",
       "           7.6056e-02,  1.2355e-01,  9.5393e-03,  1.0973e-01,  4.2799e-03,\n",
       "           1.2673e-02,  2.9474e-02, -7.1662e-02,  3.5825e-02, -2.2450e-02,\n",
       "           1.1495e-01, -1.5330e-03, -7.5895e-02,  1.1770e-01, -2.7949e-02,\n",
       "           7.7709e-02,  5.3070e-02,  6.3348e-02,  5.1943e-02, -4.2287e-04,\n",
       "          -8.5764e-02,  4.9930e-03,  7.1993e-02,  1.7088e-02,  8.2341e-02,\n",
       "          -8.4293e-02,  1.0929e-01, -3.8026e-02, -8.9832e-02, -2.3994e-02,\n",
       "           6.6473e-02,  6.4061e-02,  3.8884e-02, -9.9856e-03,  4.7005e-02]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([ 6.0571,  6.0733,  6.0313,  6.0615,  6.0569,  6.0654], device='cuda:0')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Agents[0].parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH+OuaJKdVSgeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0b88e781d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "epoch_list = np.arange(len(r_list[0]))*ep_record\n",
    "plt.plot(epoch_list,r_list[0], label='Agent 1')\n",
    "plt.plot(epoch_list,r_list[1], label='Agent 2')\n",
    "plt.ylabel('Average reward in epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.savefig('N_ep='+str(N_ep)+'_seed='+str(num_seed)+'.png')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
