{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grid Search Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Network\n",
    "import torch\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "# cuda\n",
    "use_cuda = 0\n",
    "\n",
    "# Random seeds for testing\n",
    "num_seed = 15\n",
    "torch.manual_seed(num_seed)\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    torch.cuda.manual_seed(num_seed)\n",
    "np.random.seed(num_seed)\n",
    "\n",
    "# Utility functions\n",
    "from utility import truncated_poisson_sampling, create_item_pool, create_agent_utility, rewards_func, rewards_func_prosocial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start ----------------\n",
      "tensor(-737.3980)\n",
      "tensor(-832.3733)\n",
      "tensor(-278.1211)\n",
      "tensor(-238.3404)\n",
      "tensor(-179.3310)\n",
      "tensor(-675.9233)\n",
      "tensor(-556.4774)\n",
      "tensor(-278.0686)\n",
      "tensor(-178.2243)\n",
      "tensor(-119.3066)\n",
      "tensor(-78.9677)\n",
      "Runtime for episodes 0-1: 0.665816068649292s\n",
      "tensor(-636.2114)\n",
      "tensor(-911.5576)\n",
      "tensor(-357.7677)\n",
      "tensor(-177.3233)\n",
      "tensor(-59.3589)\n",
      "tensor(-78.9061)\n",
      "tensor(-99.8142)\n",
      "Runtime for episodes 1-2: 0.42109107971191406s\n",
      "tensor(-772.3712)\n",
      "tensor(-712.0836)\n",
      "tensor(-277.9286)\n",
      "tensor(-118.7993)\n",
      "tensor(-117.6234)\n",
      "tensor(-77.6907)\n",
      "Runtime for episodes 2-3: 0.3400909900665283s\n",
      "tensor(-875.2296)\n",
      "tensor(-791.2794)\n",
      "tensor(-436.2683)\n",
      "tensor(-177.9840)\n",
      "tensor(-79.0604)\n",
      "Runtime for episodes 3-4: 0.3653700351715088s\n",
      "tensor(-773.4790)\n",
      "tensor(-833.1249)\n",
      "tensor(-356.0059)\n",
      "tensor(-179.5616)\n",
      "tensor(-58.5135)\n",
      "tensor(-98.7395)\n",
      "Runtime for episodes 4-5: 0.40854406356811523s\n",
      "tensor(-792.7980)\n",
      "tensor(-865.9956)\n",
      "tensor(-516.3719)\n",
      "tensor(-296.2022)\n",
      "tensor(-179.4833)\n",
      "tensor(-158.5286)\n",
      "Runtime for episodes 5-6: 0.46125078201293945s\n",
      "tensor(-815.8885)\n",
      "tensor(-903.7821)\n",
      "tensor(-357.5557)\n",
      "tensor(-236.8138)\n",
      "tensor(-120.5760)\n",
      "Runtime for episodes 6-7: 0.33949804306030273s\n",
      "tensor(-910.5899)\n",
      "tensor(-863.6086)\n",
      "tensor(-755.8368)\n",
      "tensor(-61.1366)\n",
      "tensor(-119.3102)\n",
      "tensor(-78.7413)\n",
      "Runtime for episodes 7-8: 0.38169193267822266s\n",
      "tensor(-950.7403)\n",
      "tensor(-861.8017)\n",
      "tensor(-633.7195)\n",
      "tensor(-176.6241)\n",
      "tensor(-297.8519)\n",
      "tensor(-99.0974)\n",
      "Runtime for episodes 8-9: 0.49926304817199707s\n",
      "End ------------------\n",
      "Total runtime: 3.882856845855713s\n",
      "Start ----------------\n",
      "tensor(-755.8226)\n",
      "tensor(-672.2106)\n",
      "tensor(-160.2810)\n",
      "tensor(-237.4157)\n",
      "tensor(-238.0626)\n",
      "tensor(-79.2156)\n",
      "tensor(-99.6992)\n",
      "tensor(-697.7330)\n",
      "tensor(-556.8120)\n",
      "tensor(-474.9171)\n",
      "tensor(-297.7273)\n",
      "tensor(-118.4027)\n",
      "tensor(-78.2579)\n",
      "Runtime for episodes 0-1: 0.7566142082214355s\n",
      "tensor(-676.0695)\n",
      "tensor(-753.1260)\n",
      "tensor(-356.8250)\n",
      "tensor(-118.0660)\n",
      "tensor(-119.3769)\n",
      "tensor(-79.7790)\n",
      "Runtime for episodes 1-2: 0.3641188144683838s\n",
      "tensor(-694.9780)\n",
      "tensor(-594.5283)\n",
      "tensor(-515.5261)\n",
      "tensor(-237.4183)\n",
      "tensor(-58.8517)\n",
      "tensor(-79.7777)\n",
      "Runtime for episodes 2-3: 0.3567671775817871s\n",
      "tensor(-868.7581)\n",
      "tensor(-824.2319)\n",
      "tensor(-358.9812)\n",
      "tensor(-413.6278)\n",
      "tensor(-118.5978)\n",
      "tensor(-78.5891)\n",
      "Runtime for episodes 3-4: 0.3862428665161133s\n",
      "tensor(-813.0813)\n",
      "tensor(-514.4294)\n",
      "tensor(-635.5482)\n",
      "tensor(-178.0652)\n",
      "tensor(-178.8635)\n",
      "tensor(-157.7193)\n",
      "Runtime for episodes 4-5: 0.41681623458862305s\n",
      "tensor(-894.9459)\n",
      "tensor(-553.8162)\n",
      "tensor(-555.3748)\n",
      "tensor(-823.6210)\n",
      "tensor(-59.1676)\n",
      "tensor(-80.1882)\n",
      "tensor(-78.7495)\n",
      "tensor(-97.9163)\n",
      "Runtime for episodes 5-6: 0.4764847755432129s\n",
      "tensor(-732.3738)\n",
      "tensor(-904.0295)\n",
      "tensor(-752.0480)\n",
      "tensor(-59.1150)\n",
      "tensor(-118.0812)\n",
      "tensor(-78.4357)\n",
      "tensor(-79.9326)\n",
      "Runtime for episodes 6-7: 0.4906899929046631s\n",
      "tensor(-1049.8069)\n",
      "tensor(-789.7328)\n",
      "tensor(-397.4525)\n",
      "tensor(-235.4225)\n",
      "tensor(-77.4377)\n",
      "Runtime for episodes 7-8: 0.49534106254577637s\n",
      "tensor(-988.4778)\n",
      "tensor(-785.4548)\n",
      "tensor(-473.4484)\n",
      "tensor(-176.8673)\n",
      "tensor(-59.7645)\n",
      "Runtime for episodes 8-9: 0.4350719451904297s\n",
      "End ------------------\n",
      "Total runtime: 4.178504228591919s\n",
      "Start ----------------\n",
      "tensor(-495.1635)\n",
      "tensor(-556.8148)\n",
      "tensor(-477.0294)\n",
      "tensor(-119.0995)\n",
      "tensor(-158.5647)\n",
      "tensor(-691.9576)\n",
      "tensor(-753.0024)\n",
      "tensor(-477.4930)\n",
      "tensor(-179.0338)\n",
      "tensor(-59.7080)\n",
      "Runtime for episodes 0-1: 0.8649048805236816s\n",
      "tensor(-811.0562)\n",
      "tensor(-632.2816)\n",
      "tensor(-237.5743)\n",
      "tensor(-177.7791)\n",
      "tensor(-58.6451)\n",
      "Runtime for episodes 1-2: 1.0445871353149414s\n",
      "tensor(-716.1974)\n",
      "tensor(-593.8935)\n",
      "tensor(-596.9001)\n",
      "tensor(-237.1851)\n",
      "tensor(-59.6409)\n",
      "Runtime for episodes 2-3: 0.630964994430542s\n",
      "tensor(-828.4752)\n",
      "tensor(-749.1268)\n",
      "tensor(-356.3175)\n",
      "tensor(-118.7451)\n",
      "tensor(-58.6194)\n",
      "tensor(-158.2999)\n",
      "Runtime for episodes 3-4: 0.6240298748016357s\n",
      "tensor(-773.0211)\n",
      "tensor(-753.0241)\n",
      "tensor(-397.2592)\n",
      "tensor(-118.3699)\n",
      "tensor(-179.4590)\n",
      "tensor(-80.2290)\n",
      "Runtime for episodes 4-5: 0.42726922035217285s\n",
      "tensor(-871.8479)\n",
      "tensor(-670.2084)\n",
      "tensor(-237.7516)\n",
      "tensor(-296.7145)\n",
      "tensor(-60.0481)\n",
      "tensor(-78.4551)\n",
      "Runtime for episodes 5-6: 0.46901893615722656s\n",
      "tensor(-1028.3323)\n",
      "tensor(-711.2190)\n",
      "tensor(-479.1047)\n",
      "tensor(-237.9370)\n",
      "tensor(-119.1806)\n",
      "tensor(-77.8052)\n",
      "Runtime for episodes 6-7: 0.42679905891418457s\n",
      "tensor(-1085.2181)\n",
      "tensor(-704.6095)\n",
      "tensor(-397.0808)\n",
      "tensor(-118.6251)\n",
      "tensor(-60.3656)\n",
      "Runtime for episodes 7-8: 0.40224480628967285s\n",
      "tensor(-985.3728)\n",
      "tensor(-699.8158)\n",
      "tensor(-629.6843)\n",
      "tensor(-58.6175)\n",
      "tensor(-158.1442)\n",
      "Runtime for episodes 8-9: 0.5606799125671387s\n",
      "End ------------------\n",
      "Total runtime: 5.450795888900757s\n"
     ]
    }
   ],
   "source": [
    "# Game setup\n",
    "\n",
    "num_agents = 2         # Number of agents playing the game\n",
    "num_types = 3          # Number of item types\n",
    "max_item = 5           # Maximum number of each item in a pool\n",
    "max_utility = 5       # Maximum utility value for agents\n",
    "\n",
    "# Turn sampling\n",
    "lam = 7                # Poisson parameter\n",
    "max_N = 10             # Maximum number of turns\n",
    "min_N = 4              # Minimum number of turns\n",
    "\n",
    "# Linguistic channel\n",
    "num_vocab = 10         # Symbol vocabulary size for linguistic channel\n",
    "len_message = 6        # Linguistic message length\n",
    "\n",
    "# Training\n",
    "alpha = 0.001          # learning rate\n",
    "N_ep = 100000          # Number of episodes\n",
    "num_games = 128        # Number of games per episode (batch size)\n",
    "\n",
    "# Appendix\n",
    "lambda1 = 0.05        # Entropy regularizer for pi_term\n",
    "lambda2 = 0.005       # Entropy regularizer for pi_utt\n",
    "lambda3 = 0.05        # Entropy regularizer for pi_prop\n",
    "smoothing_const = 0.7  # Smoothing constant for the exponential moving average baseline\n",
    "\n",
    "# Reward Scheme\n",
    "social  = 0            # 0 = selfish | 1 = prosocial   \n",
    "\n",
    "# Channels \n",
    "enable_message = 1     # 0 = Off | 1 = On\n",
    "enable_proposal = 1\n",
    "\n",
    "# Miscellaneous\n",
    "ep_time = int(max(1,N_ep/10))         # Print time every ep_time episodes\n",
    "ep_record = int(max(1,N_ep/1000))        # Record training curve every ep_record episodes\n",
    "save_plot = False\n",
    "\n",
    "ict_term = 4        # Iterator for gird search - change to affect range of search \n",
    "ict_utt = 2\n",
    "ict_prop = 2\n",
    "\n",
    "TR = np.zeros((ict_term-1,ict_utt-1,ict_prop-1))\n",
    "\n",
    "for i_term in range(1,ict_term,1):\n",
    "    lambda1 = 0.001*(i_term+5)\n",
    "    \n",
    "    for i_utt in range(1,ict_utt,1):\n",
    "        lambda2 = 0.005 \n",
    "        \n",
    "        for i_prop in range(1,ict_prop,1):\n",
    "            lambda3 = 0.005\n",
    "\n",
    "            #----------Network----------\n",
    "\n",
    "\n",
    "            class combined_policy(nn.Module):\n",
    "                def __init__(self, embedding_dim = 100, batch_size = 128, num_layers = 1, bias = True, batch_first = False, dropout = 0, bidirectional = False):\n",
    "                    super(combined_policy, self).__init__()\n",
    "                    # Save variables\n",
    "                    self.embedding_dim = embedding_dim # Hidden layer dimensions\n",
    "                    self.batch_size = batch_size       # Batch size (updated every forward pass)\n",
    "                    self.log_p = torch.zeros([batch_size,1], requires_grad=True)                     # Store policy log likelihood for REINFORCE\n",
    "                    if torch.cuda.is_available() and use_cuda:\n",
    "                        self.log_p = self.log_p.cuda()\n",
    "\n",
    "                    # Encoding -------------------------------------------------------------\n",
    "\n",
    "                    # Numerical encoder\n",
    "                    self.encoder1 = nn.Embedding(max_utility+1, embedding_dim)\n",
    "                    # Linguistic encoder\n",
    "                    self.encoder2 = nn.Embedding(num_vocab, embedding_dim)\n",
    "\n",
    "                    self.encoder3 = nn.Embedding(num_vocab, embedding_dim)\n",
    "                    # Item context LSTM\n",
    "                    self.lstm1 = nn.LSTMCell(\n",
    "                        input_size=embedding_dim,\n",
    "                        hidden_size=embedding_dim\n",
    "                    )\n",
    "\n",
    "                    # Linguistic LSTM\n",
    "                    self.lstm2 = nn.LSTMCell(\n",
    "                        input_size=embedding_dim,\n",
    "                        hidden_size=embedding_dim\n",
    "                    )\n",
    "                    # Proposal LSTM\n",
    "                    self.lstm3 = nn.LSTMCell(\n",
    "                        input_size=embedding_dim,\n",
    "                        hidden_size=embedding_dim\n",
    "                    )\n",
    "\n",
    "                    # Outputs of the 3 LSTMS get concatenated together\n",
    "\n",
    "                    # Feed-forward\n",
    "                    if enable_message + enable_proposal == 2:\n",
    "                        self.ff = nn.Linear(3*embedding_dim, embedding_dim)\n",
    "                    elif enable_message + enable_proposal == 1:\n",
    "                        self.ff = nn.Linear(2*embedding_dim, embedding_dim)\n",
    "                    elif enable_message + enable_proposal == 0:\n",
    "                        self.ff = nn.Linear(1*embedding_dim, embedding_dim)\n",
    "\n",
    "                    # Output of feed-forward is the input for the policy networks\n",
    "\n",
    "                    # Policy ---------------------------------------------------------------\n",
    "\n",
    "                    # Termination policy\n",
    "                    self.policy_term = nn.Linear(embedding_dim, 1)\n",
    "                    # Linguistic policy\n",
    "                    self.policy_ling = nn.LSTMCell(\n",
    "                        input_size=embedding_dim,\n",
    "                        hidden_size=embedding_dim\n",
    "                    )\n",
    "                    self.ff_ling = nn.Linear(embedding_dim, num_vocab)\n",
    "                    # Proposal policies\n",
    "                    self.policy_prop = nn.ModuleList([nn.Linear(embedding_dim, max_item+1) for i in range(num_types)])\n",
    "\n",
    "                def forward(self, x, batch_size=128):\n",
    "                    # Inputs --------------------------------------------------------------------\n",
    "                    # x = list of three elements consisting of:\n",
    "                    #   1. item context (longtensor of shape batch_size x (2*num_types))\n",
    "                    #   2. previous linguistic message (longtensor of shape batch_size x len_message)\n",
    "                    #   3. previous proposal (longtensor of shape batch_size x num_types)\n",
    "                    # test = whether training or testing (testing selects actions greedily)\n",
    "                    # batch_size = batch size\n",
    "                    # Outputs -------------------------------------------------------------------\n",
    "                    # term = binary variable where 1 indicates proposal accepted => game finished (longtensor of shape batch_size x 1)\n",
    "                    # message = crafted linguistic message (longtensor of shape batch_size x len_message)\n",
    "                    # prop = crafted proposal (longtensor of shape batch_size x num_types)\n",
    "                    # entropy_loss = Number containing the sum of policy entropies (should be total entropy by additivity)\n",
    "\n",
    "                    # Update batch_size variable (changes throughout training due to sieving (see survivors below))\n",
    "                    self.batch_size = batch_size\n",
    "\n",
    "                    # Extract inputs ------------------------------------------------------------\n",
    "\n",
    "                    # Item context\n",
    "                    x1 = x[0]\n",
    "                    # Previous linguistic message\n",
    "                    x2 = x[1]\n",
    "                    # Previous proposal\n",
    "                    x3 = x[2]  \n",
    "\n",
    "                    # Encoding ------------------------------------------------------------------\n",
    "\n",
    "                    # Initial embedding\n",
    "                    x1 = self.encoder1(x1)\n",
    "                    x2 = self.encoder2(x2)\n",
    "                    x3 = self.encoder1(x3) # Same encoder as item context       \n",
    "\n",
    "                    # LSTM for item context\n",
    "                    h1 = torch.zeros(self.batch_size,self.embedding_dim) # Initial hidden\n",
    "                    c1 = torch.zeros(self.batch_size,self.embedding_dim) # Initial cell\n",
    "                    if torch.cuda.is_available() and use_cuda:\n",
    "                        h1 = h1.cuda()\n",
    "                        c1 = c1.cuda()\n",
    "                    for i in range(x1.size()[1]):\n",
    "                        (h1,c1) = self.lstm1(x1[:,i].view(self.batch_size,self.embedding_dim),(h1,c1))\n",
    "                    x1_encoded = h1\n",
    "\n",
    "                    # LSTM for linguistic\n",
    "                    h2 = torch.zeros(self.batch_size,self.embedding_dim) # Initial hidden\n",
    "                    c2 = torch.zeros(self.batch_size,self.embedding_dim) # Initial cell\n",
    "                    if torch.cuda.is_available() and use_cuda:\n",
    "                        h2 = h2.cuda()\n",
    "                        c2 = c2.cuda()\n",
    "\n",
    "                    for i in range(x2.size()[1]):\n",
    "                        (h2,c2) = self.lstm2(x2[:,i].view(self.batch_size,self.embedding_dim),(h2,c2))\n",
    "                    x2_encoded = h2\n",
    "\n",
    "                    # LSTM for proposal\n",
    "                    h3 = torch.zeros(self.batch_size,self.embedding_dim) # Initial hidden\n",
    "                    c3 = torch.zeros(self.batch_size,self.embedding_dim) # Initial cell\n",
    "                    if torch.cuda.is_available() and use_cuda:\n",
    "                        h3 = h3.cuda()\n",
    "                        c3 = c3.cuda()\n",
    "\n",
    "                    for i in range(x3.size()[1]):\n",
    "                        (h3,c3) = self.lstm3(x3[:,i].view(self.batch_size,self.embedding_dim),(h3,c3))\n",
    "                    x3_encoded = h3\n",
    "\n",
    "                    # Concatenate side-by-side based on what channels are open\n",
    "                    if enable_message == 1 and enable_proposal == 1:\n",
    "                        h = torch.cat([x1_encoded,x2_encoded,x3_encoded],1).view(self.batch_size,-1)\n",
    "                    elif enable_message == 1 and enable_proposal == 0:\n",
    "                        h = torch.cat([x1_encoded,x2_encoded],1).view(self.batch_size,-1)\n",
    "                    elif enable_message == 0 and enable_proposal == 1:\n",
    "                        h = torch.cat([x1_encoded,x3_encoded],1).view(self.batch_size,-1)\n",
    "                    elif enable_message == 0 and enable_proposal == 0:\n",
    "                        h = x1_encoded\n",
    "\n",
    "                    # Feedforward\n",
    "                    h = self.ff(h)\n",
    "                    h = F.relu(h) # Hidden layer input for policy networks\n",
    "\n",
    "                    # Policy ------------------------------------------------------------------\n",
    "\n",
    "                    # Termination -----------------------------------------------\n",
    "                    p_term = F.sigmoid(self.policy_term(h)).float()\n",
    "\n",
    "\n",
    "                    # Entropy\n",
    "                    one_tensor = torch.ones(self.batch_size,1)\n",
    "                    if torch.cuda.is_available() and use_cuda:\n",
    "                        one_tensor = one_tensor.cuda()\n",
    "                    entropy_term = -(p_term * (p_term+1e-8).log()) - (one_tensor-p_term) * ((one_tensor-p_term)+1e-8).log()\n",
    "                    # Sample\n",
    "                    term = torch.bernoulli(p_term).long()\n",
    "\n",
    "                    # log p for REINFORCE\n",
    "                    log_p_term = torch.zeros(self.batch_size,1)\n",
    "                    if torch.cuda.is_available() and use_cuda:\n",
    "                        log_p_term = log_p_term.cuda()\n",
    "\n",
    "                    log_p_term = ((term.float() * p_term) + ((one_tensor-term.float()) * (one_tensor-p_term))+1e-8).log()\n",
    "                    # Linguistic construction ----------------------------------\n",
    "                    if enable_message == 1:\n",
    "                        h_ling = h.clone().view(self.batch_size,self.embedding_dim) # Initial hidden state\n",
    "                        c_ling = torch.zeros(self.batch_size,self.embedding_dim) # Initial cell state\n",
    "                        letter = torch.zeros(self.batch_size,1).long() # Initial letter (dummy)\n",
    "                        entropy_letter = torch.zeros([self.batch_size,len_message])\n",
    "                        if torch.cuda.is_available() and use_cuda:\n",
    "                            c_ling = c_ling.cuda()\n",
    "                            letter = letter.cuda()\n",
    "                            entropy_letter = entropy_letter.cuda()\n",
    "\n",
    "                        # log p for REINFORCE \n",
    "                        log_p_letter = torch.zeros([self.batch_size,1])\n",
    "                        if torch.cuda.is_available() and use_cuda:\n",
    "                            log_p_letter = log_p_letter.cuda()\n",
    "\n",
    "                        message = torch.zeros(self.batch_size,len_message) # Message\n",
    "                        if torch.cuda.is_available() and use_cuda:\n",
    "                            message = message.cuda()\n",
    "                        for i in range(len_message):\n",
    "                            embedded_letter = self.encoder3(letter)\n",
    "\n",
    "                            h_ling,c_ling = self.policy_ling(embedded_letter.view(self.batch_size,self.embedding_dim),(h_ling,c_ling))\n",
    "                            logit = self.ff_ling(h_ling.view(self.batch_size,self.embedding_dim))\n",
    "                            p_letter = F.softmax(logit,dim=1).float()\n",
    "\n",
    "                            entropy_letter[:,i] = -torch.sum(p_letter*(p_letter+1e-8).log(),1)\n",
    "                            letter = torch.multinomial(p_letter,1).long()\n",
    "\n",
    "                            # Gather the probabilities for the letters we've picked\n",
    "                            probs = torch.gather(p_letter, 1, letter)\n",
    "                            log_p_letter = log_p_letter + (probs+1e-8).log()\n",
    "\n",
    "                            message[:,i] = letter.squeeze()\n",
    "\n",
    "                        message = message.long()\n",
    "                        entropy_letter = torch.sum(entropy_letter,1,keepdim=True)\n",
    "\n",
    "                    else:\n",
    "                        message = torch.zeros(self.batch_size,len_message).long()\n",
    "                        entropy_letter = torch.zeros(self.batch_size,1).float()\n",
    "                        log_p_letter = torch.zeros(self.batch_size,1).float()\n",
    "\n",
    "                    #print(message)\n",
    "\n",
    "                    # Proposal ----------------------------------------------\n",
    "                    if enable_proposal == 1:\n",
    "                        p_prop = []\n",
    "                        prop = []\n",
    "                        entropy_prop_list = [0,0,0]\n",
    "\n",
    "                        # log p for REINFORCE \n",
    "                        log_p_prop = torch.zeros([self.batch_size,1])\n",
    "                        if torch.cuda.is_available() and use_cuda:\n",
    "                            log_p_prop = log_p_prop.cuda()\n",
    "\n",
    "                        for i in range(num_types):\n",
    "                            p_prop.append(F.softmax(self.policy_prop[i](h),dim=1))\n",
    "\n",
    "                            entropy_prop_list[i] = -torch.sum(p_prop[i]*(p_prop[i]+1e-8).log(),1,keepdim=True)\n",
    "\n",
    "                            p_prop[i] = p_prop[i].view(self.batch_size,max_item+1)\n",
    "\n",
    "                            # Sample\n",
    "                            prop.append(torch.multinomial(p_prop[i],1))\n",
    "\n",
    "                            # Gather the probabilities for the letters we've picked\n",
    "                            probs = torch.gather(p_prop[i], 1, prop[i].view(self.batch_size,1))\n",
    "                            log_p_prop = log_p_prop + (probs+1e-8).log()\n",
    "\n",
    "                        prop = torch.stack(prop).transpose(0,1)\n",
    "                        entropy_prop = torch.sum(torch.cat(entropy_prop_list,1),1,keepdim=True)\n",
    "\n",
    "                    else:\n",
    "                        prop = torch.zeros(self.batch_size,num_types,1).long()\n",
    "                        entropy_prop = torch.zeros(self.batch_size,1).float()\n",
    "                        log_p_prop = torch.zeros(self.batch_size,1).float()\n",
    "\n",
    "                    entropy_loss = -(lambda1*entropy_term + lambda3*entropy_prop + lambda2*entropy_letter)\n",
    "                    entropy_loss = entropy_loss.sum()\n",
    "                    self.log_p = self.log_p + log_p_term + log_p_letter + log_p_prop\n",
    "                    return (term,message,prop, entropy_loss)\n",
    "\n",
    "            #----------Training----------\n",
    "\n",
    "            # Initialize agents\n",
    "            Agents = []\n",
    "            for i in range(num_agents):\n",
    "                Agents.append(combined_policy())\n",
    "                if torch.cuda.is_available() and use_cuda:\n",
    "                    Agents[i] = Agents[i].cuda()\n",
    "\n",
    "            baselines = [0 for _ in range(num_agents+1)] # Baselines for reward calculation\n",
    "\n",
    "            # Initialize optimizers for learning\n",
    "            optimizers = []\n",
    "            for i in range(num_agents):\n",
    "                optimizers.append(optim.Adam(Agents[i].parameters()))\n",
    "\n",
    "            # Recording train reward (see end of episode)\n",
    "            r_list = []\n",
    "            for i in range(num_agents):\n",
    "                r_list.append([])\n",
    "\n",
    "            total_r_list = []\n",
    "\n",
    "            print('Start ----------------')\n",
    "            time_start = time.time()\n",
    "            time_p1 = time.time()\n",
    "            # Loop over episodes\n",
    "            for i_ep in range(N_ep):\n",
    "                #print(i_ep, '-----------------------------------------------------')\n",
    "                # Setting up games -----------------------------------------------------------------------\n",
    "\n",
    "                # Game setup\n",
    "\n",
    "                # Truncated Poisson sampling for number of turns in each game\n",
    "                N = truncated_poisson_sampling(lam, min_N, max_N, num_games)\n",
    "\n",
    "                # Item pools for each game\n",
    "                pool = create_item_pool(num_types, max_item, num_games)\n",
    "\n",
    "                # Item contexts for each game\n",
    "                item_contexts = [] # Each agent has different utilities (but same pool)\n",
    "                for i in range(num_agents):\n",
    "                    utility = create_agent_utility(num_types, max_utility, num_games)\n",
    "                    item_contexts.append(torch.cat([pool, utility],1))\n",
    "\n",
    "                # For getting rid of finished games\n",
    "                survivors = torch.ones(num_games).nonzero()               # Keeps track of ongoing games; everyone alive initially\n",
    "                num_alive = len(survivors)                                # Actual batch size for each turn (initially num_games)\n",
    "\n",
    "                # Initial inputs to the network\n",
    "                prev_messages = torch.zeros(num_games, len_message).long() # Previous linguistic message for each game\n",
    "                prev_proposals = torch.zeros(num_games, num_types).long()  # Previous proposal for each game\n",
    "\n",
    "                # For keeping track of sum of all rewards in the episode (used to calculate mean)\n",
    "                reward_sums = torch.zeros(2)\n",
    "                total_reward_sums = 0\n",
    "\n",
    "                # Initialize loss\n",
    "                losses = []\n",
    "                for j in range(num_agents):\n",
    "                    losses.append(torch.zeros([],requires_grad=True))\n",
    "\n",
    "                # Initialize log_p for REINFORCE\n",
    "                for j in range(num_agents):\n",
    "                    Agents[j].log_p = torch.zeros([num_alive,1], requires_grad = True)\n",
    "\n",
    "                # cuda stuff\n",
    "                if torch.cuda.is_available() and use_cuda:\n",
    "                    N = N.cuda()\n",
    "                    pool = pool.cuda()\n",
    "\n",
    "                    for j in range(num_agents):\n",
    "                        item_contexts[j] = item_contexts[j].cuda()\n",
    "                        Agents[j].log_p = Agents[j].log_p.cuda()\n",
    "                        losses[j] = losses[j].cuda()\n",
    "\n",
    "                    survivors = survivors.cuda()\n",
    "                    prev_messages = prev_messages.cuda()\n",
    "                    prev_proposals = prev_proposals.cuda()\n",
    "\n",
    "                    reward_sums = reward_sums.cuda()\n",
    "\n",
    "                # Play the games -------------------------------------------------------------------------\n",
    "                for i_turn in range(max_N): # Loop through maximum possible number of turns for all games\n",
    "\n",
    "                    utility_max = []\n",
    "                    reward_losses = []\n",
    "                    entropy_losses = []\n",
    "                    for j in range(num_agents):\n",
    "                        # Losses for each agent\n",
    "                        reward_losses.append(torch.zeros([],requires_grad=True))\n",
    "                        entropy_losses.append(torch.zeros([],requires_grad=True))\n",
    "\n",
    "                        if torch.cuda.is_available() and use_cuda:\n",
    "                            reward_losses[j] = reward_losses[j].cuda()\n",
    "                            entropy_losses[j] = entropy_losses[j].cuda()\n",
    "\n",
    "\n",
    "                    # Agent IDs\n",
    "                    id_1 = i_turn % 2    # Current player\n",
    "                    id_2 = int(not id_1) # Other player\n",
    "\n",
    "                    # Remove finished games (batch size decreases)\n",
    "                    N = N[survivors].view(num_alive, 1)\n",
    "                    pool = pool[survivors].view(num_alive, num_types)\n",
    "                    prev_messages = prev_messages[survivors].view(num_alive, len_message)\n",
    "                    prev_proposals = prev_proposals[survivors].view(num_alive, num_types)\n",
    "                    if torch.cuda.is_available() and use_cuda: # Necessary?\n",
    "                        N = N.cuda()\n",
    "                        pool = pool.cuda()\n",
    "                        prev_messages = prev_messages.cuda()\n",
    "                        prev_proposals = prev_proposals.cuda()\n",
    "                    for j in range(num_agents):\n",
    "                        item_contexts[j] = item_contexts[j][survivors].view(num_alive,num_types*2)\n",
    "                        Agents[j].log_p = Agents[j].log_p[survivors].view(num_alive,1)\n",
    "                        if torch.cuda.is_available() and use_cuda:\n",
    "                            item_contexts[j] = item_contexts[j].cuda() # Necessaire?\n",
    "\n",
    "                    # Agent currently playing\n",
    "                    Agent = Agents[id_1]             \n",
    "                    item_context = item_contexts[id_1]\n",
    "\n",
    "                    # Play the game -------------------------------------------------------------\n",
    "                    term, prev_messages, proposals, entropy_loss = Agent([item_context, prev_messages, prev_proposals], num_alive)\n",
    "                    entropy_losses[id_1] = entropy_loss\n",
    "\n",
    "                    # Compute reward loss (assumes 2 agents) ------------------------------------\n",
    "\n",
    "                    # Games terminated by the current agent (previous proposal accepted)\n",
    "                    finishers = term.squeeze().nonzero()          # squeeze is for getting rid of extra useless dimension that pops up for some reason\n",
    "                    num_finishers = len(finishers)\n",
    "                    losses[id_1] = losses[id_1] + entropy_losses[id_1]\n",
    "                    losses[id_2] = losses[id_2] + entropy_losses[id_2]\n",
    "                    # On the first turn there is no prev. proposal so terminating gives zero reward\n",
    "                    if num_finishers != 0 and i_turn != 0:\n",
    "                        pool_12 = pool[finishers].view(num_finishers,num_types)\n",
    "\n",
    "                        share_2 = prev_proposals[finishers].view(num_finishers,num_types) # Share of other (previous proposal) \n",
    "                        share_1 = pool_12 - share_2 # Share of this agent (remainder)\n",
    "\n",
    "                        # Zero reward if proposal exceeds pool\n",
    "                        invalid_batches = torch.sum(share_2>pool_12,1)>0\n",
    "                        share_2[invalid_batches] = 0\n",
    "                        share_1[invalid_batches] = 0\n",
    "\n",
    "                        utility_1 = item_contexts[id_1][:,num_types:] # Recall that item context is a concatenation of pool and utility\n",
    "                        utility_1 = utility_1[finishers].view(num_finishers,num_types)\n",
    "                        utility_2 = item_contexts[id_2][:,num_types:]\n",
    "                        utility_2 = utility_2[finishers].view(num_finishers,num_types)\n",
    "\n",
    "                        utility_max = np.maximum(utility_1,utility_2)\n",
    "\n",
    "                        log_p_1 = Agents[id_1].log_p[finishers].view(num_finishers,1)\n",
    "                        log_p_2 = Agents[id_2].log_p[finishers].view(num_finishers,1)\n",
    "\n",
    "                        # Calculate reward and reward losses\n",
    "                        r1, rl1 = rewards_func_prosocial(share_1, share_2, utility_1, utility_2, pool_12, log_p_1, baselines[-1], utility_max)\n",
    "                        r2, rl2 = rewards_func_prosocial(share_1, share_2, utility_1, utility_2, pool_12, log_p_2, baselines[-1], utility_max)\n",
    "\n",
    "                        # Calculate total reward\n",
    "                        total_reward = r1\n",
    "                        total_reward_sums = total_reward_sums +  total_reward.sum()\n",
    "\n",
    "                        # Calculate social reward \n",
    "                        if social == 0:\n",
    "                            r1, rl1 = rewards_func(share_1, utility_1, pool_12, log_p_1, baselines[id_1])\n",
    "                            r2, rl2 = rewards_func(share_2, utility_2, pool_12, log_p_2, baselines[id_2])\n",
    "\n",
    "                        # Add rewards and reward losses\n",
    "                        reward_losses[id_1] = rl1\n",
    "                        reward_losses[id_2] = rl2\n",
    "\n",
    "                        # Summing over all finished games\n",
    "                        reward_sums[id_1] = reward_sums[id_1] + r1.sum()\n",
    "                        reward_sums[id_2] = reward_sums[id_2] + r2.sum()\n",
    "\n",
    "                        # Accumulate reward loss\n",
    "                        losses[id_1] += rl1\n",
    "                        losses[id_2] += rl2\n",
    "\n",
    "                    prev_proposals = proposals # Don't need previous proposals anymore so update it\n",
    "\n",
    "\n",
    "                    # Wrapping up the end of turn ------------------------------------------------\n",
    "                    # Remove finished games\n",
    "                    # In term and term_N, element = 1 means die\n",
    "                    term_N = (N <= (i_turn+1)).view(num_alive,1).long() # Last turn reached; i_turn + 1 since i_turn starts counting from 0\n",
    "                    # In survivors, element = 1 means live\n",
    "                    survivors = (term+term_N) == 0\n",
    "\n",
    "                    # Check if everyone's dead\n",
    "                    if survivors.sum() == 0: # If all games over, break episode\n",
    "                        break;\n",
    "\n",
    "                    # Reshape\n",
    "                    survivors = ((term+term_N) == 0).nonzero()[:,0].view(-1,1)\n",
    "                    num_alive = len(survivors) # Number of survivors\n",
    "\n",
    "                # End of episode\n",
    "\n",
    "                # Gradient descent\n",
    "                for i in range(num_agents):\n",
    "                    # optimize\n",
    "                    optimizers[i].zero_grad()\n",
    "                    losses[i].backward()\n",
    "                    #print(Agents[i].policy_term.weight.grad.sum())\n",
    "                    optimizers[i].step()\n",
    "\n",
    "                for j in range(num_agents):\n",
    "                    r_mean = reward_sums[j]/num_games # Overall episode batch-averaged reward\n",
    "\n",
    "                    # Update baseline with batch-averaged reward\n",
    "                    baselines[j] = smoothing_const * baselines[j] + (1-smoothing_const)*r_mean\n",
    "\n",
    "                    # Record batch-averaged reward\n",
    "                    if (i_ep % ep_record == 0):\n",
    "                        r_list[j].append(r_mean)\n",
    "\n",
    "                #Calculate total reward\n",
    "                total_r_mean = total_reward_sums/num_games\n",
    "                if (i_ep % ep_record == 0):\n",
    "                    total_r_list.append(total_r_mean)\n",
    "\n",
    "                # Update prosocial baseline\n",
    "                baselines[-1] = smoothing_const * baselines[-1] + (1-smoothing_const)*r_mean\n",
    "\n",
    "                # Record partial runtime\n",
    "                if (i_ep % ep_time == 0) and (i_ep != 0):\n",
    "                    time_p2 = time.time()\n",
    "                    print('Runtime for episodes ' + str(i_ep-ep_time) + '-' + str(i_ep) + ': ' + str(time_p2 - time_p1) + 's')\n",
    "                    time_p1 = time_p2\n",
    "\n",
    "            print('End ------------------')\n",
    "            time_finish = time.time()\n",
    "            print('Total runtime: ' + str(time_finish-time_start) + 's')\n",
    "            TR[i_term-1,i_utt-1,i_prop-1] = total_r_list[-1]\n",
    "            # Save trained models\n",
    "\n",
    "            #for i in range(num_agents):\n",
    "            #    torch.save(Agents[0].state_dict(),'saved_model_agent_' + str(i) + '.pt')\n",
    "\n",
    "            # Template for loading\n",
    "\n",
    "            #Agents[0].load_state_dict(torch.load('saved_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "ict_utt = 2\n",
    "for i_utt in range(1,ict_utt,1):\n",
    "    print(i_utt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.13481206]],\n",
       "\n",
       "       [[ 0.08382601]],\n",
       "\n",
       "       [[ 0.0932066 ]]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
