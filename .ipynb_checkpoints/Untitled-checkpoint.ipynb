{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3f6a25617a54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;31m# Network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mautograd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Network\n",
    "import torch\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Optimizer\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Game setup\n",
    "num_types = 3    # Number of item types\n",
    "max_item = 5     # Maximum number of each item in a pool\n",
    "max_utility = 10 # Maximum utility value for agents\n",
    "\n",
    "# Linguistic channel\n",
    "num_vocab = 10   # Symbol vocabulary size for linguistic channel\n",
    "len_message = 6  # Linguistic message length\n",
    "\n",
    "# Appendix\n",
    "lambda1 = 0.05  # Entropy regularizer for pi_term, pi_prop\n",
    "lambda2 = 0.001 # Entropy regularizer for pi_utt\n",
    "smoothing_const = 0.7 # Smoothing constant for the exponential moving average baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample an item pool for a game\n",
    "def create_item_pool(num_types, max_item):\n",
    "    # Possible to have zero items?\n",
    "    pool = np.random.randint(0, max_item+1, num_types)\n",
    "    return pool\n",
    "        \n",
    "# Sample agent utility\n",
    "def create_agent_utility(num_types, max_utility):\n",
    "    utility = np.zeros(num_types) # Initialize zero vector\n",
    "    \n",
    "    while np.sum(utility) == 0:   # At least one item has non-zero utility\n",
    "        utility = np.random.randint(0, max_utility+1, num_types)\n",
    "    return utility\n",
    "\n",
    "# Calculate reward\n",
    "def reward(share, utility):\n",
    "    return np.dot(utility, share)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-94284049a91f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mclass\u001b[0m \u001b[0mcombined_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbidirectional\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombined_policy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[1;31m# Save variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class combined_policy(nn.Module):\n",
    "    def __init__(self, embedding_dim = 100, num_layers = 1, bias = True, batch_first = True, dropout = 0, bidirectional = False):\n",
    "        super(combined_policy, self).__init__()\n",
    "        # Save variables\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Numerical encoder\n",
    "        self.encoder1 = nn.Embedding(max_utility, embedding_dim)\n",
    "        # Linguistic encoder\n",
    "        self.encoder2 = nn.Embedding(num_vocab, embedding_dim)\n",
    "        \n",
    "        # Item context LSTM\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        # Linguistic LSTM\n",
    "        self.lstm2 = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        # Proposal LSTM\n",
    "        self.lstm3 = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        \n",
    "        # Feed-forward\n",
    "        self.ff = nn.Linear(embedding_dim, embedding_dim)\n",
    "        \n",
    "        # Termination policy\n",
    "        self.policy_term = nn.Linear(embedding_dim, 1)\n",
    "        # Linguistic policy\n",
    "        self.policy_ling = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        self.ff_ling = nn.Linear(embedding_dim, num_vocab)\n",
    "        # Proposal policies\n",
    "        self.policy_prop = []\n",
    "        for i in range(num_types):\n",
    "            ff = nn.Linear(embedding_dim, 1)\n",
    "            self.policy_prop.append(ff)\n",
    "        \n",
    "    def forward(self, x, test):\n",
    "        # Item context\n",
    "        x1 = x[0]\n",
    "        # Previous linguistic message\n",
    "        x2 = x[1]\n",
    "        # Previous proposal\n",
    "        x3 = x[2]\n",
    "        \n",
    "        # Initial embedding\n",
    "        x1 = self.encoder1(x1).view(x1.size()[1],1,-1)\n",
    "        x2 = self.encoder2(x2).view(x2.size()[1],1,-1)\n",
    "        x3 = self.encoder1(x3).view(x3.size()[1],1,-1)\n",
    "        \n",
    "        # LSTM for item context\n",
    "        h = torch.zeros(1,1,self.embedding_dim) # Initial hidden\n",
    "        c = torch.zeros(1,1,self.embedding_dim) # Initial cell\n",
    "\n",
    "        for i in range(x1.size()[0]):\n",
    "            _, (h,c) = self.lstm1(x1[i].view(1,1,-1),(h,c))\n",
    "        x1 = h\n",
    "        \n",
    "        # LSTM for linguistic\n",
    "        h = torch.zeros(1,1,self.embedding_dim) # Initial hidden\n",
    "        c = torch.zeros(1,1,self.embedding_dim) # Initial cell\n",
    "\n",
    "        for i in range(x2.size()[0]):\n",
    "            _, (h,c) = self.lstm2(x2[i].view(1,1,-1),(h,c))\n",
    "        x2 = h\n",
    "        \n",
    "        # LSTM for proposal\n",
    "        h = torch.zeros(1,1,self.embedding_dim) # Initial hidden\n",
    "        c = torch.zeros(1,1,self.embedding_dim) # Initial cell\n",
    "\n",
    "        for i in range(x3.size()[0]):\n",
    "            _, (h,c) = self.lstm2(x3[i].view(1,1,-1),(h,c))\n",
    "        x3 = h\n",
    "        \n",
    "        # Concatenate side-by-side\n",
    "        x = torch.cat([x1,x2,x3],1)\n",
    "        \n",
    "        # Feedforward\n",
    "        h = self.ff(x)\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        # Termination\n",
    "        p_term = F.sigmoid(self.policy_term(h))\n",
    "        \n",
    "        if test:\n",
    "            # Greedy\n",
    "            term = np.round(p_term).long()\n",
    "        else:\n",
    "            # Sample\n",
    "            term = torch.bernoulli(p_term).long()\n",
    "        \n",
    "        # Linguistic construction\n",
    "        h = torch.zeros(1,1,self.embedding_dim) # Initial hidden state\n",
    "        c = torch.zeros(1,1,self.embedding_dim) # Initial cell state\n",
    "        letter = torch.zeros(1,1).long() # Initial letter (dummy)\n",
    "        \n",
    "        message = [] # Message\n",
    "        for i in range(len_message):\n",
    "            embedded_letter = self.encoder2(letter)\n",
    "            \n",
    "            out, (h,c) = self.policy_ling(embedded_letter,(h,c))\n",
    "            logit = self.ff_ling(h)\n",
    "            prob_letter = F.softmax(logit,dim=2)\n",
    "            \n",
    "            if test:\n",
    "                # Greedy\n",
    "                letter = prob_letter.data.max(1)[1]\n",
    "            else:\n",
    "                # Sample\n",
    "                letter = torch.polynomial(prob_letter,1)\n",
    "            \n",
    "            message.append(letter)\n",
    "            \n",
    "        message = torch.tensor(message).view(1,num_types)\n",
    "\n",
    "        # Proposal\n",
    "        p_prop = torch.zeros(num_types)\n",
    "        prop = torch.zeros([1,num_types])\n",
    "        \n",
    "        for i in range(num_types):\n",
    "            p_prop[i] = F.sigmoid(self.policy_prop[i](h))\n",
    "            \n",
    "            if test:\n",
    "                # Greedy\n",
    "                prop[0][i] = p_prop[i].data.max(1)[1]\n",
    "            else:\n",
    "                # Sample\n",
    "                prop[0][i] = torch.polynomial(p_prop,1)\n",
    "            \n",
    "        return (term,message,prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = combined_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros([1,6]).long()\n",
    "y = torch.zeros([1,6]).long()\n",
    "z = torch.zeros([1,3]).long()\n",
    "\n",
    "x.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 1)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net([x,y,z],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5,  1,  4,  0,  3,  2,  5,  5,  5,  5,  4,  4])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(np.random.choice(6,(4,3),replace=True)).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Game\n",
    "games = 128;\n",
    "\n",
    "# Set up truncated poisson - N is the number of steps this game\n",
    "lam = 7;\n",
    "max_N = 10;\n",
    "min_N = 4;\n",
    "s = np.random.poisson(lam,games);\n",
    "s = np.minimum(s,max_N);\n",
    "s = np.maximum(s,min_N)\n",
    "N = np.random.choice(s,1);\n",
    "\n",
    "#Initialization\n",
    "pool = create_item_pool(num_types, max_item);\n",
    "utility = create_agent_utility(num_types, max_utility);\n",
    "item_context = np.concatenate((pool,utility),0);\n",
    "item_context = torch.from_numpy(item_context).long();\n",
    "\n",
    "A1 = combined_policy();\n",
    "A2 = combined_policy();\n",
    "\n",
    "A1_e =  0.0;\n",
    "A1_m = torch.zeros([1,6]).long();\n",
    "A1_p = torch.zeros([1,3]).long();\n",
    "A2_e =  0.0;\n",
    "A2_m = torch.zeros([1,6]).long();\n",
    "A2_p = torch.zeros([1,3]).long();\n",
    "\n",
    "#Game\n",
    "for i in range(N[0]):\n",
    "    if i%2 == 0:       \n",
    "        if A1_e == 0:\n",
    "            A1_e,A1_m,A1_p = A1([A2_m,A2_m,A2_p], True);\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        if A2_e == 0:\n",
    "            A2_e,A2_m,A2_p = A2([A1_m,A1_m,A1_p], True);\n",
    "        else:\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = create_item_pool(num_types, max_item);\n",
    "utility = create_agent_utility(num_types, max_utility);\n",
    "\n",
    "item_context = np.concatenate((pool,utility),0);\n",
    "item_context = torch.from_numpy(item_context).long();\n",
    "\n",
    "type(item_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, tensor([[ 0,  0,  0,  0,  0,  0]]), tensor([[ 0,  0,  0]]))"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Agent2([A1_m,A1_m,A1_p], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
