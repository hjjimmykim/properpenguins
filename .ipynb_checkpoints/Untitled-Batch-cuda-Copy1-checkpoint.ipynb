{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "All the logarithms used are base 2. <br>\n",
    "Assumes 2 self-interested agents alternating turns. <br>\n",
    "Baseline (1 for each agent) gets updated after each episode ends (see corpses). <br>\n",
    "Rewards only possible at the end of each game. <br>\n",
    "Uses same (numerical) encoder for both item context and proposal. Reference code uses 3 distinct ones. It also has max_utility = num_types instead of 10 for us.<br>\n",
    "Check how message policy works again; paper seemed to imply that each output of the lstm is a letter. (we take the hidden output and make a probability over letters out of it).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Network\n",
    "import torch\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "# cuda\n",
    "use_cuda = 1\n",
    "\n",
    "# Random seeds for testing\n",
    "num_seed = 10**5\n",
    "torch.manual_seed(num_seed)\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    torch.cuda.manual_seed(num_seed)\n",
    "np.random.seed(num_seed)\n",
    "\n",
    "# Utility functions\n",
    "from utility import truncated_poisson_sampling, create_item_pool, create_agent_utility, rewards_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Game setup\n",
    "num_agents = 2         # Number of agents playing the game\n",
    "num_types = 3          # Number of item types\n",
    "max_item = 5           # Maximum number of each item in a pool\n",
    "max_utility = 5       # Maximum utility value for agents\n",
    "\n",
    "# Turn sampling\n",
    "lam = 7                # Poisson parameter\n",
    "max_N = 10             # Maximum number of turns\n",
    "min_N = 4              # Minimum number of turns\n",
    "\n",
    "# Linguistic channel\n",
    "num_vocab = 10         # Symbol vocabulary size for linguistic channel\n",
    "len_message = 6        # Linguistic message length\n",
    "\n",
    "# Training\n",
    "alpha = 0.001          # learning rate\n",
    "N_ep = 100              # Number of episodes\n",
    "num_games = 128        # Number of games per episode (batch size)\n",
    "\n",
    "# Appendix\n",
    "lambda1 = 0.05         # Entropy regularizer for pi_term\n",
    "lambda2 = 0.0001        # Entropy regularizer for pi_utt\n",
    "lambda3 = 0.005        # Entropy regularizer for pi_prop\n",
    "smoothing_const = 0.7  # Smoothing constant for the exponential moving average baseline\n",
    "\n",
    "# Miscellaneous\n",
    "ep_time = 5         # Print time every ep_time episodes\n",
    "ep_record = 1        # Record training curve every ep_record episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class combined_policy(nn.Module):\n",
    "    def __init__(self, embedding_dim = 100, batch_size = 128, num_layers = 1, bias = True, batch_first = False, dropout = 0, bidirectional = False):\n",
    "        super(combined_policy, self).__init__()\n",
    "        # Save variables\n",
    "        self.embedding_dim = embedding_dim # Hidden layer dimensions\n",
    "        self.batch_size = batch_size       # Batch size (updated every forward pass)\n",
    "        self.log_p = torch.zeros([batch_size,1], requires_grad=True)                     # Store policy log likelihood for REINFORCE\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            self.log_p = self.log_p.cuda()\n",
    "        \n",
    "        # Encoding -------------------------------------------------------------\n",
    "        \n",
    "        # Numerical encoder\n",
    "        self.encoder1 = nn.Embedding(max_utility+1, embedding_dim)\n",
    "        # Linguistic encoder\n",
    "        self.encoder2 = nn.Embedding(num_vocab+1, embedding_dim)\n",
    "        \n",
    "        # Item context LSTM\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        # Linguistic LSTM\n",
    "        self.lstm2 = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        # Proposal LSTM\n",
    "        self.lstm3 = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        \n",
    "        # Outputs of the 3 LSTMS get concatenated together\n",
    "        \n",
    "        # Feed-forward\n",
    "        self.ff = nn.Linear(3*embedding_dim, embedding_dim)\n",
    "        \n",
    "        # Output of feed-forward is the input for the policy networks\n",
    "        \n",
    "        # Policy ---------------------------------------------------------------\n",
    "        \n",
    "        # Termination policy\n",
    "        self.policy_term = nn.Linear(embedding_dim, 1)\n",
    "        # Linguistic policy\n",
    "        self.policy_ling = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        self.ff_ling = nn.Linear(embedding_dim, num_vocab)\n",
    "        # Proposal policies\n",
    "        self.policy_prop = nn.ModuleList([nn.Linear(embedding_dim, max_item+1) for i in range(num_types)])\n",
    "        \n",
    "    def forward(self, x, test, batch_size=128):\n",
    "        # Inputs --------------------------------------------------------------------\n",
    "        # x = list of three elements consisting of:\n",
    "        #   1. item context (longtensor of shape batch_size x (2*num_types))\n",
    "        #   2. previous linguistic message (longtensor of shape batch_size x len_message)\n",
    "        #   3. previous proposal (longtensor of shape batch_size x num_types)\n",
    "        # test = whether training or testing (testing selects actions greedily)\n",
    "        # batch_size = batch size\n",
    "        # Outputs -------------------------------------------------------------------\n",
    "        # term = binary variable where 1 indicates proposal accepted => game finished (longtensor of shape batch_size x 1)\n",
    "        # message = crafted linguistic message (longtensor of shape batch_size x len_message)\n",
    "        # prop = crafted proposal (longtensor of shape batch_size x num_types)\n",
    "        # entropy_loss = Number containing the sum of policy entropies (should be total entropy by additivity)\n",
    "        \n",
    "        # Update batch_size variable (changes throughout training due to sieving (see survivors below))\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Extract inputs ------------------------------------------------------------\n",
    "        \n",
    "        # Item context\n",
    "        x1 = x[0]\n",
    "        # Previous linguistic message\n",
    "        x2 = x[1]\n",
    "        # Previous proposal\n",
    "        x3 = x[2]  \n",
    "\n",
    "        # Encoding ------------------------------------------------------------------\n",
    "\n",
    "        # Initial embedding\n",
    "        x1 = self.encoder1(x1).transpose(0,1)\n",
    "        x2 = self.encoder2(x2).transpose(0,1)\n",
    "        x3 = self.encoder1(x3).transpose(0,1) # Same encoder as item context\n",
    "        \n",
    "            \n",
    "        # LSTM for item context\n",
    "        h1 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial hidden\n",
    "        c1 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial cell\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            h1 = h1.cuda()\n",
    "            c1 = c1.cuda()\n",
    "\n",
    "        for i in range(x1.size()[0]):\n",
    "            _, (h1,c1) = self.lstm1(x1[i].view(1,self.batch_size,self.embedding_dim),(h1,c1))\n",
    "        x1_encoded = h1\n",
    "        \n",
    "        # LSTM for linguistic\n",
    "        h2 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial hidden\n",
    "        c2 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial cell\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            h2 = h2.cuda()\n",
    "            c2 = c2.cuda()\n",
    "\n",
    "        for i in range(x2.size()[0]):\n",
    "            _, (h2,c2) = self.lstm2(x2[i].view(1,self.batch_size,self.embedding_dim),(h2,c2))\n",
    "        x2_encoded = h2\n",
    "        \n",
    "        # LSTM for proposal\n",
    "        h3 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial hidden\n",
    "        c3 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial cell\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            h3 = h3.cuda()\n",
    "            c3 = c3.cuda()\n",
    "\n",
    "        for i in range(x3.size()[0]):\n",
    "            _, (h3,c3) = self.lstm2(x3[i].view(1,self.batch_size,self.embedding_dim),(h3,c3))\n",
    "        x3_encoded = h3\n",
    "\n",
    "        # Concatenate side-by-side\n",
    "        h = torch.cat([x1_encoded,x2_encoded,x3_encoded],2).squeeze()\n",
    "\n",
    "        # Feedforward\n",
    "        h = self.ff(h)\n",
    "        h = F.relu(h) # Hidden layer input for policy networks\n",
    "        \n",
    "        # Policy ------------------------------------------------------------------\n",
    "\n",
    "        # Termination -----------------------------------------------\n",
    "        p_term = F.sigmoid(self.policy_term(h)).float()\n",
    "\n",
    "        # Entropy\n",
    "        one_tensor = torch.ones(self.batch_size,1)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            one_tensor = one_tensor.cuda()\n",
    "        entropy_term_tensor = -(p_term * (p_term+1e-8).log()) - ((one_tensor-p_term) * (one_tensor-p_term+1e-8).log())\n",
    "        entropy_term = torch.sum(entropy_term_tensor)\n",
    "        #print(entropy_term)\n",
    "        \n",
    "        if test:\n",
    "            # Greedy\n",
    "            term = torch.round(p_term).long()\n",
    "        else:\n",
    "            # Sample\n",
    "            term = torch.bernoulli(p_term).long()\n",
    "            \n",
    "        # log p for REINFORCE\n",
    "        log_p_term = torch.zeros(self.batch_size,1)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            log_p_term = log_p_term.cuda()\n",
    "\n",
    "        log_p_term = ((term.float() * p_term) + ((one_tensor-term.float()) * (one_tensor-p_term))+1e-8).log()\n",
    "\n",
    "        # Linguistic construction ----------------------------------\n",
    "        h_ling = h.clone().view(1,self.batch_size,self.embedding_dim) # Initial hidden state\n",
    "        c_ling = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial cell state\n",
    "        letter = torch.zeros(self.batch_size,1).long() # Initial letter (dummy)\n",
    "        entropy_letter = torch.zeros([self.batch_size,len_message])\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            c_ling = c_ling.cuda()\n",
    "            letter = letter.cuda()\n",
    "            entropy_letter = entropy_letter.cuda()\n",
    "        \n",
    "        # log p for REINFORCE \n",
    "        log_p_letter = torch.zeros([self.batch_size,1])\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            log_p_letter = log_p_letter.cuda()\n",
    "\n",
    "        message = torch.zeros(self.batch_size,len_message) # Message\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            message = message.cuda()\n",
    "        for i in range(len_message):\n",
    "            embedded_letter = self.encoder2(letter)\n",
    "\n",
    "            _, (h_ling,c_ling) = self.policy_ling(embedded_letter.view(1,self.batch_size,self.embedding_dim),(h_ling,c_ling))\n",
    "            logit = self.ff_ling(h_ling.view(self.batch_size,self.embedding_dim))\n",
    "            p_letter = F.softmax(logit,dim=1).float()\n",
    "\n",
    "            entropy_letter[:,i] = -torch.sum(p_letter*(p_letter+1e-8).log(),1)\n",
    "\n",
    "            if test:\n",
    "                # Greedy\n",
    "                letter = p_letter.argmax(dim=1).view(self.batch_size,1).long()\n",
    "            else:\n",
    "                # Sample\n",
    "                letter = torch.multinomial(p_letter,1).long()\n",
    "                \n",
    "            # Gather the probabilities for the letters we've picked\n",
    "            probs = torch.gather(p_letter, 1, letter)\n",
    "            log_p_letter = log_p_letter + (probs+1e-8).log()\n",
    "                \n",
    "            message[:,i] = letter.squeeze()\n",
    "            \n",
    "        message = message.long()\n",
    "        entropy_letter = torch.sum(entropy_letter)     \n",
    "   \n",
    "        # Proposal ----------------------------------------------\n",
    "        p_prop = []\n",
    "        prop = []\n",
    "        \n",
    "        #prop = torch.zeros([self.batch_size,num_types]).long()\n",
    "        entropy_prop_list = [0,0,0]\n",
    "        \n",
    "        # log p for REINFORCE \n",
    "        log_p_prop = torch.zeros([self.batch_size,1])\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            log_p_prop = log_p_prop.cuda()\n",
    "\n",
    "        for i in range(num_types):\n",
    "            p_prop.append(F.sigmoid(self.policy_prop[i](h)))\n",
    "            \n",
    "            entropy_prop_list[i] = -torch.sum(p_prop[i]*(p_prop[i]+1e-8).log())\n",
    "            \n",
    "            p_prop[i] = p_prop[i].view(self.batch_size,max_item+1)\n",
    "\n",
    "            if test:\n",
    "                # Greedy\n",
    "                #prop[:,i] = p_prop[i].argmax(dim=1)\n",
    "                prop.append(p_prop[i].argmax(dim=1))\n",
    "            else:\n",
    "                # Sample\n",
    "                #prop[:,i] = torch.multinomial(p_prop,1)\n",
    "                prop.append(torch.multinomial(p_prop,1))\n",
    "                \n",
    "            # Gather the probabilities for the letters we've picked\n",
    "            probs = torch.gather(p_prop[i], 1, prop[i].view(self.batch_size,1))\n",
    "            log_p_prop = log_p_prop + (probs+1e-8).log()\n",
    "              \n",
    "        prop = torch.stack(prop).transpose(0,1)\n",
    "        entropy_prop = sum(entropy_prop_list) # Entropy for exploration\n",
    "\n",
    "        # Combine -----------------------------------------------------------------\n",
    "        entropy_loss = torch.sum(lambda1*entropy_term + lambda3*entropy_prop + lambda2*entropy_letter)\n",
    "        self.log_p = log_p_term + log_p_letter + log_p_prop\n",
    "        \n",
    "        #if entropy_loss != entropy_loss:\n",
    "        #    print(entropy_term)\n",
    "        #    print(entropy_prop)\n",
    "        #    print(entropy_letter)\n",
    "\n",
    "        return (term,message,prop, entropy_loss, log_p_term,log_p_letter,log_p_prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = combined_policy()\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    net = net.cuda()\n",
    "\n",
    "# Dummy inputs\n",
    "x = torch.randint(0,max_item,[128,6]).long()\n",
    "y = torch.randint(0,num_vocab,[128,6]).long()\n",
    "z = torch.randint(0,max_item,[128,3]).long()\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    z = z.cuda()\n",
    "\n",
    "blah = net([x,y,z],True)\n",
    "\n",
    "# Initialize agents\n",
    "Agents = []\n",
    "for i in range(num_agents):\n",
    "    Agents.append(combined_policy())\n",
    "    if torch.cuda.is_available() and use_cuda:\n",
    "        Agents[i] = Agents[i].cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start ----------------\n",
      "Runtime for episodes 0-5: 0.20067548751831055s\n",
      "Runtime for episodes 5-10: 0.1701827049255371s\n",
      "Runtime for episodes 10-15: 0.16709542274475098s\n",
      "Runtime for episodes 15-20: 0.1721348762512207s\n",
      "Runtime for episodes 20-25: 0.1645646095275879s\n",
      "Runtime for episodes 25-30: 0.1717815399169922s\n",
      "Runtime for episodes 30-35: 0.1647026538848877s\n",
      "Runtime for episodes 35-40: 0.1711275577545166s\n",
      "Runtime for episodes 40-45: 0.16682934761047363s\n",
      "Runtime for episodes 45-50: 0.17061424255371094s\n",
      "Runtime for episodes 50-55: 0.16754889488220215s\n",
      "Runtime for episodes 55-60: 0.17400908470153809s\n",
      "Runtime for episodes 60-65: 0.1628406047821045s\n",
      "Runtime for episodes 65-70: 0.16995716094970703s\n",
      "Runtime for episodes 70-75: 0.16509270668029785s\n",
      "Runtime for episodes 75-80: 0.17297577857971191s\n",
      "Runtime for episodes 80-85: 0.16756677627563477s\n",
      "Runtime for episodes 85-90: 0.16950464248657227s\n",
      "Runtime for episodes 90-95: 0.1644606590270996s\n",
      "End ------------------\n",
      "Total runtime: 3.371959686279297s\n"
     ]
    }
   ],
   "source": [
    "baselines = [0 for _ in range(num_agents)] # Baselines for reward calculation\n",
    "\n",
    "# Initialize optimizers for learning\n",
    "optimizers = []\n",
    "for i in range(num_agents):\n",
    "    optimizers.append(optim.Adam(Agents[i].parameters()))\n",
    "    \n",
    "# Train rewards\n",
    "r_list = []\n",
    "for i in range(num_agents):\n",
    "    r_list.append([])\n",
    "\n",
    "print('Start ----------------')\n",
    "time_start = time.time()\n",
    "time_p1 = time.time()\n",
    "# Loop over episodes\n",
    "for i_ep in range(N_ep):\n",
    "    # Setting up games -----------------------------------------------------------------------\n",
    "    # Game setup\n",
    "    # Truncated Poisson sampling for number of turns in each game\n",
    "    N = truncated_poisson_sampling(lam, min_N, max_N, num_games)\n",
    "    # Item pools for each game\n",
    "    pool = create_item_pool(num_types, max_item, num_games)\n",
    "    if torch.cuda.is_available() and use_cuda:\n",
    "        N = N.cuda()\n",
    "        pool = pool.cuda()\n",
    "    # Item contexts for each game\n",
    "    item_contexts = [] # Each agent has different utilities (but same pool)\n",
    "    for i in range(num_agents):\n",
    "        utility = create_agent_utility(num_types, max_utility, num_games)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            utility = utility.cuda()\n",
    "        item_contexts.append(torch.cat([pool, utility],1))\n",
    "        \n",
    "    # Initializations\n",
    "    survivors = torch.ones(num_games).nonzero()               # Keeps track of ongoing games; everyone alive initially\n",
    "    num_alive = len(survivors)                                # Actual batch size for each turn (initially num_games)\n",
    "    prev_messages = torch.zeros(num_games, len_message).long() # Previous linguistic message for each game\n",
    "    prev_proposals = torch.zeros(num_games, num_types).long()  # Previous proposal for each game\n",
    "    if torch.cuda.is_available() and use_cuda:\n",
    "        survivors = survivors.cuda()\n",
    "        prev_messages = prev_messages.cuda()\n",
    "        prev_proposals = prev_proposals.cuda()\n",
    "            \n",
    "            \n",
    "    rewards = [torch.zeros(num_games), torch.zeros(num_games)]       # Rewards for each game for each agent\n",
    "    # Keep track of sum of all rewards (from all games in a batch) for baseline updates (see corpses below)\n",
    "    reward_sums = []\n",
    "    for i in range(num_agents):\n",
    "        reward_sums.append(torch.zeros(1)) # Just a number\n",
    "        Agents[i].log_p = torch.zeros([num_games,1], requires_grad = True)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            rewards[i] = rewards[i].cuda()\n",
    "            reward_sums[i] = reward_sums[i].cuda()\n",
    "            Agents[i].log_p = Agents[i].log_p.cuda()\n",
    "\n",
    "    # Play the games -------------------------------------------------------------------------\n",
    "    for i_turn in range(max_N): # Loop through maximum possible number of turns for all games\n",
    "        \n",
    "        # Losses for each agent\n",
    "        reward_losses = [torch.zeros([],requires_grad=True), torch.zeros([],requires_grad=True)]  \n",
    "        entropy_losses = [torch.zeros([],requires_grad=True), torch.zeros([],requires_grad=True)] # Exploration\n",
    "        for j in range(num_agents):\n",
    "            Agents[j].log_p = torch.zeros([num_alive,1], requires_grad = True)\n",
    "            \n",
    "            if torch.cuda.is_available() and use_cuda:\n",
    "                reward_losses[j] = reward_losses[j].cuda()\n",
    "                entropy_losses[j] = entropy_losses[j].cuda()\n",
    "                Agents[j].log_p = Agents[j].log_p.cuda()\n",
    "        \n",
    "        # Agent IDs\n",
    "        id_1 = i_turn % 2    # Current player\n",
    "        id_2 = int(not id_1) # Other player\n",
    "        \n",
    "        #print(i_ep, i_turn, id_1, id_2)\n",
    "        \n",
    "        # Remove finished games (batch size decreases)\n",
    "        N = N[survivors].view(num_alive, 1)\n",
    "        pool = pool[survivors].view(num_alive, num_types)\n",
    "        prev_messages = prev_messages[survivors].view(num_alive, len_message)\n",
    "        prev_proposals = prev_proposals[survivors].view(num_alive, num_types)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            N = N.cuda()\n",
    "            pool = pool.cuda()\n",
    "            prev_messages = prev_messages.cuda()\n",
    "            prev_proposals = prev_proposals.cuda()\n",
    "        # Quantities different for each agent\n",
    "        for j in range(num_agents):\n",
    "            item_contexts[j] = item_contexts[j][survivors].view(num_alive,num_types*2)\n",
    "            #rewards[j] = rewards[j][survivors].view(num_alive)\n",
    "            #reward_losses[j] = reward_losses[j][survivors].view(num_alive)\n",
    "            if torch.cuda.is_available() and use_cuda:\n",
    "                item_contexts[j] = item_contexts[j].cuda()\n",
    "        \n",
    "        # Agent currently playing\n",
    "        Agent = Agents[id_1]             \n",
    "        item_context = item_contexts[id_1]\n",
    "        \n",
    "        # Play the game -------------------------------------------------------------\n",
    "        term, prev_messages, proposals, entropy_loss, lt,ll,lp = Agent([item_context, prev_messages, prev_proposals], True, num_alive)\n",
    "        entropy_losses[id_1] = entropy_loss\n",
    "        \n",
    "        # Compute reward loss (assumes 2 agents) ------------------------------------\n",
    "        # Games terminated by the current agent (previous proposal accepted)\n",
    "        \n",
    "        finishers = term.squeeze().nonzero()          # squeeze is for getting rid of extra useless dimension that pops up for some reason\n",
    "        num_finishers = len(finishers)\n",
    "\n",
    "        if len(finishers) != 0:\n",
    "            pool_12 = pool[finishers].view(num_finishers,num_types)\n",
    "            \n",
    "            share_2 = prev_proposals[finishers].view(num_finishers,num_types) # Share of other (previous proposal) \n",
    "            share_1 = pool_12 - share_2 # Share of this agent (remainder)\n",
    "            \n",
    "            # Zero reward if proposal exceeds pool\n",
    "            invalid_batches = torch.sum(share_2>pool_12,1)>0\n",
    "            share_2[invalid_batches] = 0\n",
    "            share_1[invalid_batches] = 0\n",
    "            \n",
    "            utility_1 = item_contexts[id_1][:,num_types:] # Recall that item context is a concatenation of pool and utility\n",
    "            utility_1 = utility_1[finishers].view(num_finishers,num_types)\n",
    "            utility_2 = item_contexts[id_2][:,num_types:]\n",
    "            utility_2 = utility_2[finishers].view(num_finishers,num_types)\n",
    "\n",
    "            log_p_1 = Agents[id_1].log_p[finishers].view(num_finishers,1)\n",
    "            log_p_2 = Agents[id_2].log_p[finishers].view(num_finishers,1)\n",
    "\n",
    "            # Calculate reward and reward losses\n",
    "            r1, rl1 = rewards_func(share_1, utility_1, pool_12, log_p_1, baselines[id_1])\n",
    "            r2, rl2 = rewards_func(share_2, utility_2, pool_12, log_p_2, baselines[id_2])\n",
    "         \n",
    "            #for i in range(num_finishers):\n",
    "                #print(r1[i], r2[i])\n",
    "                #if r1[i]==0:\n",
    "                #    print(share_2[i])\n",
    "                #    print(share_1[i])\n",
    "                #    print(utility_1[i])\n",
    "                #    print(utility_2[i])\n",
    "                #    print(pool_12[i])\n",
    "                #print(lt[i])\n",
    "                #print(lp[i])\n",
    "                #print(ll[i])\n",
    "                #print(baselines)\n",
    "            \n",
    "            # Add rewards and reward losses\n",
    "            rewards[id_1] = r1.squeeze()\n",
    "            rewards[id_2] = r2.squeeze()\n",
    "            reward_losses[id_1] = rl1\n",
    "            reward_losses[id_2] = rl2\n",
    "            \n",
    "            #print(rewards[id_1])\n",
    "            \n",
    "            reward_sums[id_1] = reward_sums[id_1] + rewards[id_1].sum()\n",
    "            reward_sums[id_2] = reward_sums[id_2] + rewards[id_2].sum()\n",
    "\n",
    "        prev_proposals = proposals # Don't need previous proposals anymore so update it\n",
    "        \n",
    "        # Gradient descent -----------------------------------------------------------\n",
    "        \n",
    "        for i in range(num_agents):\n",
    "            # optimize\n",
    "            #print('!!!!!!!!!!!!!!!!!!!')\n",
    "            #print(reward_losses[i])\n",
    "            #print(entropy_losses[i])\n",
    "            #print('?????????????????')\n",
    "            loss = reward_losses[i] - entropy_losses[i]\n",
    "            #print(entropy_losses[i])\n",
    "            optimizers[i].zero_grad()\n",
    "            loss.backward()\n",
    "            #print(Agents[i].ff_ling.weight.grad)\n",
    "            optimizers[i].step()\n",
    "        \n",
    "        # Wrapping up the end of turn ------------------------------------------------\n",
    "        # Remove finished games\n",
    "        # In term and term_N, element = 1 means die\n",
    "        term_N = (N <= (i_turn+1)).view(num_alive,1).long() # Last turn reached; i_turn + 1 since i_turn starts counting from 0\n",
    "        # In survivors, element = 1 means live\n",
    "        survivors = (term+term_N) == 0\n",
    "\n",
    "        # Check if everyone's dead\n",
    "        if survivors.sum() == 0: # If all games over, break episode\n",
    "            # Baseline updates\n",
    "            for i in range(num_agents):\n",
    "                # Update with batch-averaged rewards\n",
    "                baselines[i] = smoothing_const * baselines[i] + (1-smoothing_const)*reward_sums[i]/num_games\n",
    "            break;\n",
    "            \n",
    "        # Reshape\n",
    "        survivors = ((term+term_N) == 0).nonzero()[:,0].view(-1,1)\n",
    "        num_alive = len(survivors) # Number of survivors\n",
    "\n",
    "        #print('i_turn = ' + str(i_turn))\n",
    "        \n",
    "    #print('i_ep = ' + str(i_ep))\n",
    "    if (i_ep % ep_time == 0) and (i_ep != 0):\n",
    "        time_p2 = time.time()\n",
    "        print('Runtime for episodes ' + str(i_ep-ep_time) + '-' + str(i_ep) + ': ' + str(time_p2 - time_p1) + 's')\n",
    "        time_p1 = time_p2\n",
    "        \n",
    "    if (i_ep % ep_record == 0):\n",
    "        for j in range(num_agents):\n",
    "            r_list[j].append(reward_sums[j]/num_games)\n",
    "    \n",
    "    #print('----------------')\n",
    "    \n",
    "print('End ------------------')\n",
    "time_finish = time.time()\n",
    "print('Total runtime: ' + str(time_finish-time_start) + 's')\n",
    "\n",
    "#for i in range(num_agents):\n",
    "#    torch.save(Agents[0].state_dict(),'saved_model_agent_' + str(i) + '.pt')\n",
    "    \n",
    "#Agents[0].load_state_dict(torch.load('saved_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agents[0].ff.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(Agents[1].parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUVIQEZEQJQUREQlRUhARkZD/By8RYggVKcPpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe984343eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_list = np.arange(len(r_list[0]))*ep_record\n",
    "plt.plot(epoch_list,r_list[0], label='Agent 1')\n",
    "plt.plot(epoch_list,r_list[1], label='Agent 2')\n",
    "plt.ylabel('Average reward in epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim([0,1])\n",
    "plt.legend()\n",
    "#plt.savefig('N_ep='+str(N_ep)+'_seed='+str(num_seed)+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dummy inputs\n",
    "x = torch.randint(0,max_item,[1,6]).long()\n",
    "y = torch.randint(0,num_vocab,[1,6]).long()\n",
    "z = torch.randint(0,max_item,[1,3]).long()\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    z = z.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1], device='cuda:0'),\n",
       " tensor([[ 2,  2,  2,  7,  5,  4]], device='cuda:0'),\n",
       " tensor([[ 3,  1,  5]], device='cuda:0'),\n",
       " tensor(1.00000e-02 *\n",
       "        6.7271, device='cuda:0'),\n",
       " tensor([[-0.6877]], device='cuda:0'),\n",
       " tensor([[-12.8833]], device='cuda:0'),\n",
       " tensor([[-1.9938]], device='cuda:0'))"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[0] = 4\n",
    "Agents[1]([x,y,z],True,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
