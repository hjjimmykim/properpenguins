{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Notes\n",
    "\n",
    "v3 = This now trains, the test function was a poopyhead, i.e. I removed test.<br>\n",
    "v6 = Train turned into a function; facilitates grid search.<br>\n",
    "\n",
    "All the logarithms used are base e (natural). <br>\n",
    "Assumes 2 self-interested agents alternating turns. <br>\n",
    "Baseline (1 for each agent) gets updated after each episode ends (see corpses). <br>\n",
    "Rewards only possible at the end of each game. <br>\n",
    "Uses same (numerical) encoder for both item context and proposal. Reference code uses 3 distinct ones. It also has max_utility = num_types instead of 10 for us.<br>\n",
    "Check how message policy works again; paper seemed to imply that each output of the lstm is a letter. (we take the hidden output and make a probability over letters out of it).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Network\n",
    "import torch\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "# cuda\n",
    "use_cuda = 1\n",
    "\n",
    "# Random seeds for testing\n",
    "num_seed = 15\n",
    "torch.manual_seed(num_seed)\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    torch.cuda.manual_seed(num_seed)\n",
    "np.random.seed(num_seed)\n",
    "\n",
    "# Utility functions\n",
    "from utility import truncated_poisson_sampling, create_item_pool, create_agent_utility, rewards_func, rewards_func_prosocial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game setup\n",
    "\n",
    "num_agents = 2         # Number of agents playing the game\n",
    "num_types = 3          # Number of item types\n",
    "max_item = 5           # Maximum number of each item in a pool\n",
    "max_utility = 5       # Maximum utility value for agents\n",
    "\n",
    "# Turn sampling\n",
    "lam = 7                # Poisson parameter\n",
    "max_N = 10             # Maximum number of turns\n",
    "min_N = 4              # Minimum number of turns\n",
    "\n",
    "# Linguistic channel\n",
    "num_vocab = 10         # Symbol vocabulary size for linguistic channel\n",
    "len_message = 6        # Linguistic message length\n",
    "\n",
    "# Training\n",
    "alpha = 0.001          # learning rate\n",
    "N_ep = 100000          # Number of episodes\n",
    "num_games = 128        # Number of games per episode (batch size)\n",
    "\n",
    "# Appendix\n",
    "smoothing_const = 0.7  # Smoothing constant for the exponential moving average baseline\n",
    "\n",
    "# Reward Scheme\n",
    "social  = 0            # 0 = selfish | 1 = prosocial   \n",
    "\n",
    "# Channels \n",
    "enable_message = 1     # 0 = Off | 1 = On\n",
    "enable_proposal = 1\n",
    "\n",
    "# Miscellaneous\n",
    "ep_time = int(max(1,N_ep/10))         # Print time every ep_time episodes\n",
    "ep_record = int(max(1,N_ep/1000))        # Record training curve every ep_record episodes\n",
    "save_plot = True\n",
    "show_plot = False\n",
    "\n",
    "# Params list\n",
    "params_dict = {'num_agents':num_agents, \\\n",
    "               'num_types':num_types, \\\n",
    "               'max_item':max_item, \\\n",
    "               'max_utility':max_utility,\\\n",
    "               \\\n",
    "               'lam':lam,\\\n",
    "               'max_N':max_N,\\\n",
    "               'min_N':min_N,\\\n",
    "               \\\n",
    "               'num_vocab':num_vocab,\\\n",
    "               'len_message':len_message,\\\n",
    "               \\\n",
    "               'alpha':alpha,\\\n",
    "               'N_ep':N_ep,\\\n",
    "               'num_games':num_games,\\\n",
    "               \\\n",
    "               'smoothing_const':smoothing_const,\\\n",
    "               \\\n",
    "               'social':social,\\\n",
    "               \\\n",
    "               'enable_message':enable_message,\\\n",
    "               'enable_proposal':enable_proposal}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class combined_policy(nn.Module):\n",
    "    def __init__(self, embedding_dim = 100, batch_size = 128, num_layers = 1, l1 = 0.05, l2 = 0.0001, l3 = 0.005):\n",
    "        super(combined_policy, self).__init__()\n",
    "        # Save variables\n",
    "        self.embedding_dim = embedding_dim # Hidden layer dimensions\n",
    "        self.batch_size = batch_size       # Batch size (updated every forward pass)\n",
    "        self.log_p = torch.zeros([batch_size,1], requires_grad=True)                     # Store policy log likelihood for REINFORCE\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            self.log_p = self.log_p.cuda()\n",
    "        # Regularization parameters\n",
    "        self.l1 = float(l1)\n",
    "        self.l2 = float(l2)\n",
    "        self.l3 = float(l3)\n",
    "\n",
    "        # Encoding -------------------------------------------------------------\n",
    "\n",
    "        # Numerical encoder\n",
    "        self.encoder1 = nn.Embedding(max_utility+1, embedding_dim)\n",
    "        # Linguistic encoder\n",
    "        self.encoder2 = nn.Embedding(num_vocab, embedding_dim)\n",
    "\n",
    "        self.encoder3 = nn.Embedding(num_vocab, embedding_dim)\n",
    "        # Item context LSTM\n",
    "        self.lstm1 = nn.LSTMCell(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=embedding_dim\n",
    "        )\n",
    "\n",
    "        # Linguistic LSTM\n",
    "        self.lstm2 = nn.LSTMCell(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=embedding_dim\n",
    "        )\n",
    "        # Proposal LSTM\n",
    "        self.lstm3 = nn.LSTMCell(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=embedding_dim\n",
    "        )\n",
    "\n",
    "        # Outputs of the 3 LSTMS get concatenated together\n",
    "\n",
    "        # Feed-forward\n",
    "        if enable_message + enable_proposal == 2:\n",
    "            self.ff = nn.Linear(3*embedding_dim, embedding_dim)\n",
    "        elif enable_message + enable_proposal == 1:\n",
    "            self.ff = nn.Linear(2*embedding_dim, embedding_dim)\n",
    "        elif enable_message + enable_proposal == 0:\n",
    "            self.ff = nn.Linear(1*embedding_dim, embedding_dim)\n",
    "\n",
    "        # Output of feed-forward is the input for the policy networks\n",
    "\n",
    "        # Policy ---------------------------------------------------------------\n",
    "\n",
    "        # Termination policy\n",
    "        self.policy_term = nn.Linear(embedding_dim, 1)\n",
    "        # Linguistic policy\n",
    "        self.policy_ling = nn.LSTMCell(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=embedding_dim\n",
    "        )\n",
    "        self.ff_ling = nn.Linear(embedding_dim, num_vocab)\n",
    "        # Proposal policies\n",
    "        self.policy_prop = nn.ModuleList([nn.Linear(embedding_dim, max_item+1) for i in range(num_types)])\n",
    "\n",
    "    def forward(self, x, batch_size=128):\n",
    "        # Inputs --------------------------------------------------------------------\n",
    "        # x = list of three elements consisting of:\n",
    "        #   1. item context (longtensor of shape batch_size x (2*num_types))\n",
    "        #   2. previous linguistic message (longtensor of shape batch_size x len_message)\n",
    "        #   3. previous proposal (longtensor of shape batch_size x num_types)\n",
    "        # test = whether training or testing (testing selects actions greedily)\n",
    "        # batch_size = batch size\n",
    "        # Outputs -------------------------------------------------------------------\n",
    "        # term = binary variable where 1 indicates proposal accepted => game finished (longtensor of shape batch_size x 1)\n",
    "        # message = crafted linguistic message (longtensor of shape batch_size x len_message)\n",
    "        # prop = crafted proposal (longtensor of shape batch_size x num_types)\n",
    "        # entropy_loss = Number containing the sum of policy entropies (should be total entropy by additivity)\n",
    "\n",
    "        # Update batch_size variable (changes throughout training due to sieving (see survivors below))\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Extract inputs ------------------------------------------------------------\n",
    "\n",
    "        # Item context\n",
    "        x1 = x[0]\n",
    "        # Previous linguistic message\n",
    "        x2 = x[1]\n",
    "        # Previous proposal\n",
    "        x3 = x[2]  \n",
    "\n",
    "        # Encoding ------------------------------------------------------------------\n",
    "\n",
    "        # Initial embedding\n",
    "        x1 = self.encoder1(x1)\n",
    "        x2 = self.encoder2(x2)\n",
    "        x3 = self.encoder1(x3) # Same encoder as item context       \n",
    "\n",
    "        # LSTM for item context\n",
    "        h1 = torch.zeros(self.batch_size,self.embedding_dim) # Initial hidden\n",
    "        c1 = torch.zeros(self.batch_size,self.embedding_dim) # Initial cell\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            h1 = h1.cuda()\n",
    "            c1 = c1.cuda()\n",
    "        for i in range(x1.size()[1]):\n",
    "            (h1,c1) = self.lstm1(x1[:,i].view(self.batch_size,self.embedding_dim),(h1,c1))\n",
    "        x1_encoded = h1\n",
    "\n",
    "        # LSTM for linguistic\n",
    "        h2 = torch.zeros(self.batch_size,self.embedding_dim) # Initial hidden\n",
    "        c2 = torch.zeros(self.batch_size,self.embedding_dim) # Initial cell\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            h2 = h2.cuda()\n",
    "            c2 = c2.cuda()\n",
    "\n",
    "        for i in range(x2.size()[1]):\n",
    "            (h2,c2) = self.lstm2(x2[:,i].view(self.batch_size,self.embedding_dim),(h2,c2))\n",
    "        x2_encoded = h2\n",
    "\n",
    "        # LSTM for proposal\n",
    "        h3 = torch.zeros(self.batch_size,self.embedding_dim) # Initial hidden\n",
    "        c3 = torch.zeros(self.batch_size,self.embedding_dim) # Initial cell\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            h3 = h3.cuda()\n",
    "            c3 = c3.cuda()\n",
    "\n",
    "        for i in range(x3.size()[1]):\n",
    "            (h3,c3) = self.lstm3(x3[:,i].view(self.batch_size,self.embedding_dim),(h3,c3))\n",
    "        x3_encoded = h3\n",
    "\n",
    "        # Concatenate side-by-side based on what channels are open\n",
    "        if enable_message == 1 and enable_proposal == 1:\n",
    "            h = torch.cat([x1_encoded,x2_encoded,x3_encoded],1).view(self.batch_size,-1)\n",
    "        elif enable_message == 1 and enable_proposal == 0:\n",
    "            h = torch.cat([x1_encoded,x2_encoded],1).view(self.batch_size,-1)\n",
    "        elif enable_message == 0 and enable_proposal == 1:\n",
    "            h = torch.cat([x1_encoded,x3_encoded],1).view(self.batch_size,-1)\n",
    "        elif enable_message == 0 and enable_proposal == 0:\n",
    "            h = x1_encoded\n",
    "\n",
    "        # Feedforward\n",
    "        h = self.ff(h)\n",
    "        h = F.relu(h) # Hidden layer input for policy networks\n",
    "\n",
    "        # Policy ------------------------------------------------------------------\n",
    "\n",
    "        # Termination -----------------------------------------------\n",
    "        p_term = F.sigmoid(self.policy_term(h)).float()\n",
    "\n",
    "\n",
    "        # Entropy\n",
    "        one_tensor = torch.ones(self.batch_size,1)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            one_tensor = one_tensor.cuda()\n",
    "        entropy_term = -(p_term * (p_term+1e-8).log()) - (one_tensor-p_term) * ((one_tensor-p_term)+1e-8).log()\n",
    "        # Sample\n",
    "        term = torch.bernoulli(p_term).long()\n",
    "\n",
    "        # log p for REINFORCE\n",
    "        log_p_term = torch.zeros(self.batch_size,1)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            log_p_term = log_p_term.cuda()\n",
    "\n",
    "        log_p_term = ((term.float() * p_term) + ((one_tensor-term.float()) * (one_tensor-p_term))+1e-8).log()\n",
    "        # Linguistic construction ----------------------------------\n",
    "        if enable_message == 1:\n",
    "            h_ling = h.clone().view(self.batch_size,self.embedding_dim) # Initial hidden state\n",
    "            c_ling = torch.zeros(self.batch_size,self.embedding_dim) # Initial cell state\n",
    "            letter = torch.zeros(self.batch_size,1).long() # Initial letter (dummy)\n",
    "            entropy_letter = torch.zeros([self.batch_size,len_message])\n",
    "            if torch.cuda.is_available() and use_cuda:\n",
    "                c_ling = c_ling.cuda()\n",
    "                letter = letter.cuda()\n",
    "                entropy_letter = entropy_letter.cuda()\n",
    "\n",
    "            # log p for REINFORCE \n",
    "            log_p_letter = torch.zeros([self.batch_size,1])\n",
    "            if torch.cuda.is_available() and use_cuda:\n",
    "                log_p_letter = log_p_letter.cuda()\n",
    "\n",
    "            message = torch.zeros(self.batch_size,len_message) # Message\n",
    "            if torch.cuda.is_available() and use_cuda:\n",
    "                message = message.cuda()\n",
    "            for i in range(len_message):\n",
    "                embedded_letter = self.encoder3(letter)\n",
    "\n",
    "                h_ling,c_ling = self.policy_ling(embedded_letter.view(self.batch_size,self.embedding_dim),(h_ling,c_ling))\n",
    "                logit = self.ff_ling(h_ling.view(self.batch_size,self.embedding_dim))\n",
    "                p_letter = F.softmax(logit,dim=1).float()\n",
    "\n",
    "                entropy_letter[:,i] = -torch.sum(p_letter*(p_letter+1e-8).log(),1)\n",
    "                letter = torch.multinomial(p_letter,1).long()\n",
    "\n",
    "                # Gather the probabilities for the letters we've picked\n",
    "                probs = torch.gather(p_letter, 1, letter)\n",
    "                log_p_letter = log_p_letter + (probs+1e-8).log()\n",
    "\n",
    "                message[:,i] = letter.squeeze()\n",
    "\n",
    "            message = message.long()\n",
    "            entropy_letter = torch.sum(entropy_letter,1,keepdim=True)\n",
    "\n",
    "        else:\n",
    "            message = torch.zeros(self.batch_size,len_message).long()\n",
    "            entropy_letter = torch.zeros(self.batch_size,1).float()\n",
    "            log_p_letter = torch.zeros(self.batch_size,1).float()\n",
    "\n",
    "        #print(message)\n",
    "\n",
    "        # Proposal ----------------------------------------------\n",
    "        if enable_proposal == 1:\n",
    "            p_prop = []\n",
    "            prop = []\n",
    "            entropy_prop_list = [0,0,0]\n",
    "\n",
    "            # log p for REINFORCE \n",
    "            log_p_prop = torch.zeros([self.batch_size,1])\n",
    "            if torch.cuda.is_available() and use_cuda:\n",
    "                log_p_prop = log_p_prop.cuda()\n",
    "\n",
    "            for i in range(num_types):\n",
    "                p_prop.append(F.softmax(self.policy_prop[i](h),dim=1))\n",
    "\n",
    "                entropy_prop_list[i] = -torch.sum(p_prop[i]*(p_prop[i]+1e-8).log(),1,keepdim=True)\n",
    "\n",
    "                p_prop[i] = p_prop[i].view(self.batch_size,max_item+1)\n",
    "\n",
    "                # Sample\n",
    "                prop.append(torch.multinomial(p_prop[i],1))\n",
    "\n",
    "                # Gather the probabilities for the letters we've picked\n",
    "                probs = torch.gather(p_prop[i], 1, prop[i].view(self.batch_size,1))\n",
    "                log_p_prop = log_p_prop + (probs+1e-8).log()\n",
    "\n",
    "            prop = torch.stack(prop).transpose(0,1)\n",
    "            entropy_prop = torch.sum(torch.cat(entropy_prop_list,1),1,keepdim=True)\n",
    "\n",
    "        else:\n",
    "            prop = torch.zeros(self.batch_size,num_types,1).long()\n",
    "            entropy_prop = torch.zeros(self.batch_size,1).float()\n",
    "            log_p_prop = torch.zeros(self.batch_size,1).float()\n",
    "\n",
    "        entropy_loss = -(self.l1*entropy_term + self.l3*entropy_prop + self.l2*entropy_letter)\n",
    "        entropy_loss = entropy_loss.sum()\n",
    "        self.log_p = self.log_p + log_p_term + log_p_letter + log_p_prop\n",
    "        return (term,message,prop, entropy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------Training----------\n",
    "\n",
    "def train_func(lambda1,lambda2,lambda3,verbose=True):\n",
    "\n",
    "    # Initialize agents\n",
    "    Agents = []\n",
    "    for i in range(num_agents):\n",
    "        Agents.append(combined_policy(l1=lambda1,l2=lambda2,l3=lambda3))\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            Agents[i] = Agents[i].cuda()\n",
    "\n",
    "    baselines = [0 for _ in range(num_agents+1)] # Baselines for reward calculation\n",
    "\n",
    "    # Initialize optimizers for learning\n",
    "    optimizers = []\n",
    "    for i in range(num_agents):\n",
    "        optimizers.append(optim.Adam(Agents[i].parameters()))\n",
    "\n",
    "    # Recording train reward (see end of episode)\n",
    "    r_list = []\n",
    "    for i in range(num_agents):\n",
    "        r_list.append([])\n",
    "\n",
    "    total_r_list = []\n",
    "\n",
    "    if verbose:\n",
    "        print('Start ----------------')\n",
    "    time_start = time.time()\n",
    "    time_p1 = time.time()\n",
    "    # Loop over episodes\n",
    "    for i_ep in range(N_ep):\n",
    "        #print(i_ep, '-----------------------------------------------------')\n",
    "        # Setting up games -----------------------------------------------------------------------\n",
    "\n",
    "        # Game setup\n",
    "\n",
    "        # Truncated Poisson sampling for number of turns in each game\n",
    "        N = truncated_poisson_sampling(lam, min_N, max_N, num_games)\n",
    "\n",
    "        # Item pools for each game\n",
    "        pool = create_item_pool(num_types, max_item, num_games)\n",
    "\n",
    "        # Item contexts for each game\n",
    "        item_contexts = [] # Each agent has different utilities (but same pool)\n",
    "        for i in range(num_agents):\n",
    "            utility = create_agent_utility(num_types, max_utility, num_games)\n",
    "            item_contexts.append(torch.cat([pool, utility],1))\n",
    "\n",
    "        # For getting rid of finished games\n",
    "        survivors = torch.ones(num_games).nonzero()               # Keeps track of ongoing games; everyone alive initially\n",
    "        num_alive = len(survivors)                                # Actual batch size for each turn (initially num_games)\n",
    "\n",
    "        # Initial inputs to the network\n",
    "        prev_messages = torch.zeros(num_games, len_message).long() # Previous linguistic message for each game\n",
    "        prev_proposals = torch.zeros(num_games, num_types).long()  # Previous proposal for each game\n",
    "\n",
    "        # For keeping track of sum of all rewards in the episode (used to calculate mean)\n",
    "        reward_sums = torch.zeros(2)\n",
    "        total_reward_sums = 0\n",
    "\n",
    "        # Initialize loss\n",
    "        losses = []\n",
    "        for j in range(num_agents):\n",
    "            losses.append(torch.zeros([],requires_grad=True))\n",
    "\n",
    "        # Initialize log_p for REINFORCE\n",
    "        for j in range(num_agents):\n",
    "            Agents[j].log_p = torch.zeros([num_alive,1], requires_grad = True)\n",
    "\n",
    "        # cuda stuff\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            N = N.cuda()\n",
    "            pool = pool.cuda()\n",
    "\n",
    "            for j in range(num_agents):\n",
    "                item_contexts[j] = item_contexts[j].cuda()\n",
    "                Agents[j].log_p = Agents[j].log_p.cuda()\n",
    "                losses[j] = losses[j].cuda()\n",
    "\n",
    "            survivors = survivors.cuda()\n",
    "            prev_messages = prev_messages.cuda()\n",
    "            prev_proposals = prev_proposals.cuda()\n",
    "\n",
    "            reward_sums = reward_sums.cuda()\n",
    "\n",
    "        # Play the games -------------------------------------------------------------------------\n",
    "        for i_turn in range(max_N): # Loop through maximum possible number of turns for all games\n",
    "\n",
    "            utility_max = []\n",
    "            reward_losses = []\n",
    "            entropy_losses = []\n",
    "            for j in range(num_agents):\n",
    "                # Losses for each agent\n",
    "                reward_losses.append(torch.zeros([],requires_grad=True))\n",
    "                entropy_losses.append(torch.zeros([],requires_grad=True))\n",
    "\n",
    "                if torch.cuda.is_available() and use_cuda:\n",
    "                    reward_losses[j] = reward_losses[j].cuda()\n",
    "                    entropy_losses[j] = entropy_losses[j].cuda()\n",
    "\n",
    "\n",
    "            # Agent IDs\n",
    "            id_1 = i_turn % 2    # Current player\n",
    "            id_2 = int(not id_1) # Other player\n",
    "\n",
    "            # Remove finished games (batch size decreases)\n",
    "            N = N[survivors].view(num_alive, 1)\n",
    "            pool = pool[survivors].view(num_alive, num_types)\n",
    "            prev_messages = prev_messages[survivors].view(num_alive, len_message)\n",
    "            prev_proposals = prev_proposals[survivors].view(num_alive, num_types)\n",
    "            if torch.cuda.is_available() and use_cuda: # Necessary?\n",
    "                N = N.cuda()\n",
    "                pool = pool.cuda()\n",
    "                prev_messages = prev_messages.cuda()\n",
    "                prev_proposals = prev_proposals.cuda()\n",
    "            for j in range(num_agents):\n",
    "                item_contexts[j] = item_contexts[j][survivors].view(num_alive,num_types*2)\n",
    "                Agents[j].log_p = Agents[j].log_p[survivors].view(num_alive,1)\n",
    "                if torch.cuda.is_available() and use_cuda:\n",
    "                    item_contexts[j] = item_contexts[j].cuda() # Necessaire?\n",
    "\n",
    "            # Agent currently playing\n",
    "            Agent = Agents[id_1]             \n",
    "            item_context = item_contexts[id_1]\n",
    "\n",
    "            # Play the game -------------------------------------------------------------\n",
    "            term, prev_messages, proposals, entropy_loss = Agent([item_context, prev_messages, prev_proposals], num_alive)\n",
    "            entropy_losses[id_1] = entropy_loss\n",
    "\n",
    "            # Compute reward loss (assumes 2 agents) ------------------------------------\n",
    "\n",
    "            # Games terminated by the current agent (previous proposal accepted)\n",
    "            finishers = term.squeeze().nonzero()          # squeeze is for getting rid of extra useless dimension that pops up for some reason\n",
    "            num_finishers = len(finishers)\n",
    "            losses[id_1] = losses[id_1] + entropy_losses[id_1]\n",
    "            losses[id_2] = losses[id_2] + entropy_losses[id_2]\n",
    "            # On the first turn there is no prev. proposal so terminating gives zero reward\n",
    "            if num_finishers != 0 and i_turn != 0:\n",
    "                pool_12 = pool[finishers].view(num_finishers,num_types)\n",
    "\n",
    "                share_2 = prev_proposals[finishers].view(num_finishers,num_types) # Share of other (previous proposal) \n",
    "                share_1 = pool_12 - share_2 # Share of this agent (remainder)\n",
    "\n",
    "                # Zero reward if proposal exceeds pool\n",
    "                invalid_batches = torch.sum(share_2>pool_12,1)>0\n",
    "                share_2[invalid_batches] = 0\n",
    "                share_1[invalid_batches] = 0\n",
    "\n",
    "                utility_1 = item_contexts[id_1][:,num_types:] # Recall that item context is a concatenation of pool and utility\n",
    "                utility_1 = utility_1[finishers].view(num_finishers,num_types)\n",
    "                utility_2 = item_contexts[id_2][:,num_types:]\n",
    "                utility_2 = utility_2[finishers].view(num_finishers,num_types)\n",
    "\n",
    "                # For prosocial\n",
    "                utility_max = np.maximum(utility_1,utility_2)\n",
    "                # cuda\n",
    "                if torch.cuda.is_available() and use_cuda:\n",
    "                    utility_max = utility_max.cuda() # Necessaire?\n",
    "\n",
    "                log_p_1 = Agents[id_1].log_p[finishers].view(num_finishers,1)\n",
    "                log_p_2 = Agents[id_2].log_p[finishers].view(num_finishers,1)\n",
    "\n",
    "                # Calculate reward and reward losses\n",
    "                r1, rl1 = rewards_func_prosocial(share_1, share_2, utility_1, utility_2, pool_12, log_p_1, baselines[-1], utility_max)\n",
    "                r2, rl2 = rewards_func_prosocial(share_1, share_2, utility_1, utility_2, pool_12, log_p_2, baselines[-1], utility_max)\n",
    "\n",
    "                # Calculate total reward\n",
    "                total_reward = r1\n",
    "                total_reward_sums = total_reward_sums +  total_reward.sum()\n",
    "\n",
    "                # Calculate social reward \n",
    "                if social == 0:\n",
    "                    r1, rl1 = rewards_func(share_1, utility_1, pool_12, log_p_1, baselines[id_1])\n",
    "                    r2, rl2 = rewards_func(share_2, utility_2, pool_12, log_p_2, baselines[id_2])\n",
    "\n",
    "                # Add rewards and reward losses\n",
    "                reward_losses[id_1] = rl1\n",
    "                reward_losses[id_2] = rl2\n",
    "\n",
    "                # Summing over all finished games\n",
    "                reward_sums[id_1] = reward_sums[id_1] + r1.sum()\n",
    "                reward_sums[id_2] = reward_sums[id_2] + r2.sum()\n",
    "\n",
    "                # Accumulate reward loss\n",
    "                losses[id_1] += rl1\n",
    "                losses[id_2] += rl2\n",
    "\n",
    "            prev_proposals = proposals # Don't need previous proposals anymore so update it\n",
    "\n",
    "\n",
    "            # Wrapping up the end of turn ------------------------------------------------\n",
    "            # Remove finished games\n",
    "            # In term and term_N, element = 1 means die\n",
    "            term_N = (N <= (i_turn+1)).view(num_alive,1).long() # Last turn reached; i_turn + 1 since i_turn starts counting from 0\n",
    "            # In survivors, element = 1 means live\n",
    "            survivors = (term+term_N) == 0\n",
    "\n",
    "            # Check if everyone's dead\n",
    "            if survivors.sum() == 0: # If all games over, break episode\n",
    "                break;\n",
    "\n",
    "            # Reshape\n",
    "            survivors = ((term+term_N) == 0).nonzero()[:,0].view(-1,1)\n",
    "            num_alive = len(survivors) # Number of survivors\n",
    "\n",
    "        # End of episode\n",
    "\n",
    "        # Gradient descent\n",
    "        for i in range(num_agents):\n",
    "            # optimize\n",
    "            optimizers[i].zero_grad()\n",
    "            losses[i].backward()\n",
    "            #print(Agents[i].policy_term.weight.grad.sum())\n",
    "            optimizers[i].step()\n",
    "\n",
    "        for j in range(num_agents):\n",
    "            r_mean = reward_sums[j]/num_games # Overall episode batch-averaged reward\n",
    "\n",
    "            # Update baseline with batch-averaged reward\n",
    "            baselines[j] = smoothing_const * baselines[j] + (1-smoothing_const)*r_mean\n",
    "\n",
    "            # Record batch-averaged reward\n",
    "            if (i_ep % ep_record == 0):\n",
    "                r_list[j].append(r_mean)\n",
    "\n",
    "        #Calculate total reward\n",
    "        total_r_mean = total_reward_sums/num_games\n",
    "        if (i_ep % ep_record == 0):\n",
    "            total_r_list.append(total_r_mean)\n",
    "\n",
    "        # Update prosocial baseline\n",
    "        baselines[-1] = smoothing_const * baselines[-1] + (1-smoothing_const)*r_mean\n",
    "\n",
    "        # Record partial runtime\n",
    "        if (i_ep % ep_time == 0) and (i_ep != 0):\n",
    "            time_p2 = time.time()\n",
    "            if verbose:\n",
    "                print('Runtime for episodes ' + str(i_ep-ep_time) + '-' + str(i_ep) + ': ' + str(time_p2 - time_p1) + 's')\n",
    "            time_p1 = time_p2\n",
    "\n",
    "    if verbose:\n",
    "        print('End ------------------')\n",
    "        time_finish = time.time()\n",
    "        print('Total runtime: ' + str(time_finish-time_start) + 's')\n",
    "        \n",
    "    # Return trained agent models (list), individual agent reward lists, total reward list\n",
    "    return Agents, r_list, total_r_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_func(r_list, total_r_list, filename):\n",
    "    epoch_list = np.arange(len(r_list[0]))*ep_record\n",
    "    plt.figure()\n",
    "    plt.plot(epoch_list,r_list[0], label='Agent 1')\n",
    "    plt.plot(epoch_list,r_list[1], label='Agent 2')\n",
    "    plt.plot(epoch_list,total_r_list, label='Total Reward')\n",
    "    plt.ylabel('Average reward in epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.xlim([0,ep_record*len(r_list[0])])\n",
    "    plt.ylim([0,1])\n",
    "    plt.legend()\n",
    "    if save_plot:\n",
    "        plt.savefig(filename)\n",
    "    if show_plot:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting\n",
    "lambda1_list = 0.1**np.arange(1,4)\n",
    "lambda2_list = 0.1**np.arange(1,5)\n",
    "lambda3_list = 0.1**np.arange(1,4)\n",
    "\n",
    "lambdas_list = [[l1,l2,l3] for l1 in lambda1_list for l2 in lambda2_list for l3 in lambda3_list]\n",
    "\n",
    "# Savepath\n",
    "savepath = 'gridsearchresult_social='+str(social)+'_N_ep='+str(N_ep)+'_seed='+str(num_seed)+'/'\n",
    "if not os.path.exists(savepath):\n",
    "    os.makedirs(savepath)\n",
    "\n",
    "# Save parameters\n",
    "with open(savepath+'params_dict.pickle','wb') as handle:\n",
    "    pickle.dump(params_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# Template for loading\n",
    "# with open(savepath+'params_dict.pickle','rb') as handle:\n",
    "#     params_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start ----------------\n",
      "Runtime for episodes 0-10000: 1126.2092769145966s\n",
      "Runtime for episodes 10000-20000: 1202.1451144218445s\n",
      "Runtime for episodes 20000-30000: 1231.7717459201813s\n",
      "Runtime for episodes 30000-40000: 1236.402307510376s\n",
      "Runtime for episodes 40000-50000: 1238.9661350250244s\n",
      "Runtime for episodes 50000-60000: 1241.7606344223022s\n",
      "Runtime for episodes 60000-70000: 1248.7581098079681s\n",
      "Runtime for episodes 70000-80000: 1244.8982355594635s\n",
      "Runtime for episodes 80000-90000: 1247.7369997501373s\n",
      "End ------------------\n",
      "Total runtime: 12265.921932220459s\n",
      "Start ----------------\n",
      "Runtime for episodes 0-10000: 1122.6138005256653s\n",
      "Runtime for episodes 10000-20000: 1203.1771366596222s\n",
      "Runtime for episodes 20000-30000: 1214.3044209480286s\n",
      "Runtime for episodes 30000-40000: 1226.4572911262512s\n",
      "Runtime for episodes 40000-50000: 1240.415965795517s\n",
      "Runtime for episodes 50000-60000: 1247.30109167099s\n",
      "Runtime for episodes 60000-70000: 1245.7327270507812s\n",
      "Runtime for episodes 70000-80000: 1247.8956763744354s\n",
      "Runtime for episodes 80000-90000: 1245.5277903079987s\n",
      "End ------------------\n",
      "Total runtime: 12240.90882563591s\n",
      "Start ----------------\n",
      "Runtime for episodes 0-10000: 1113.2647218704224s\n",
      "Runtime for episodes 10000-20000: 1208.625500202179s\n",
      "Runtime for episodes 20000-30000: 1215.9152381420135s\n",
      "Runtime for episodes 30000-40000: 1222.0469660758972s\n",
      "Runtime for episodes 40000-50000: 1219.6432819366455s\n",
      "Runtime for episodes 50000-60000: 1219.4272973537445s\n",
      "Runtime for episodes 60000-70000: 1221.294097661972s\n",
      "Runtime for episodes 70000-80000: 1226.3415756225586s\n",
      "Runtime for episodes 80000-90000: 1224.9320259094238s\n",
      "End ------------------\n",
      "Total runtime: 12098.948190450668s\n",
      "Start ----------------\n",
      "Runtime for episodes 0-10000: 1140.306771993637s\n",
      "Runtime for episodes 10000-20000: 1211.2723314762115s\n",
      "Runtime for episodes 20000-30000: 1231.7658519744873s\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "for lambda1,lambda2,lambda3 in lambdas_list:\n",
    "    # Simulation\n",
    "    Agents, r_list, total_r_list = train_func(lambda1,lambda2,lambda3)\n",
    "    \n",
    "    # Plot filename\n",
    "    fname_plot = 'plot_l1='+str(lambda1)+'_l2='+str(lambda2)+'_l3='+str(lambda3)+'.png'\n",
    "    # Save plot\n",
    "    plot_func(r_list,total_r_list,savepath+fname_plot)\n",
    "    \n",
    "    # Save model\n",
    "    for i in range(num_agents):\n",
    "        fname_model = 'model_l1='+str(lambda1)+'_l2='+str(lambda2)+'_l3='+str(lambda3)+'_agent_'+str(i)+'.pt'\n",
    "        torch.save(Agents[0].state_dict(),savepath+fname_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# Template for loading\n",
    "\n",
    "#Agents[0].load_state_dict(torch.load('saved_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
