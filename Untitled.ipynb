{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Network\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Optimizer\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambda1 = 0.05 # For pi_term, pi_prop\n",
    "lambda2 = 0.001 # For pi_utt\n",
    "\n",
    "vocab_size = 11 # Symbol vocabulary size\n",
    "utt_len = 6 # Utterance length\n",
    "\n",
    "smoothing_const = 0.7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim=15, hidden_dim, num_layers=1, bias=False, batch_first=False, dropout=0, bidirectional=False, cuda):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        \n",
    "        # Variables\n",
    "        num1 = 4\n",
    "        num2 = 2\n",
    "        \n",
    "        self.encoder1 = nn.Embedding(6, num1)\n",
    "        self.encoder2 = nn.Embedding(3, num2)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(num1, output_dim1, layers, bias, batch_first, dropout, bidirectional)\n",
    "        self.hidden2category = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "        self.lstm2 = nn.LSTM(num1, output_dim2, layers, bias, batch_first, dropout, bidirectional)\n",
    "        self.hidden2category = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "        self.lstm3 = nn.LSTM(num2, output_dim3, layers, bias, batch_first, dropout, bidirectional)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        input_tensor = self.encoder(input_tensor)\n",
    "        output_tensor, hidden = self.lstm(input_tensor, hidden)\n",
    "        output_tensor = output_tensor.contiguous().view(output_tensor.size(0)*output_tensor.size(1), output_tensor.size(2))\n",
    "        \n",
    "        if self.direction == 2:\n",
    "            output_tensor_forward, output_tensor_reverse = torch.chunk(output_tensor, 2, 1)\n",
    "            output_tensor_forward = self.hidden2category(output_tensor_forward)\n",
    "            output_tensor_reverse = self.hidden2category(output_tensor_reverse)\n",
    "            return torch.cat((output_tensor_forward, output_tensor_reverse), 0), hidden\n",
    "        else:\n",
    "            output_tensor = self.hidden2category(output_tensor)\n",
    "            return output_tensor, hidden\n",
    "\n",
    "class afterLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_size=400, hidden1_dropout_prob=0.2, hidden2_dropout_prob=0.5):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        return x\n",
    "    \n",
    "class policy_term(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_size=400, hidden1_dropout_prob=0.2, hidden2_dropout_prob=0.5):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "\n",
    "        return x\n",
    "    \n",
    "class policy_utt(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_size=400, hidden1_dropout_prob=0.2, hidden2_dropout_prob=0.5):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class policy_prop(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_size=400, hidden1_dropout_prob=0.2, hidden2_dropout_prob=0.5):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
