{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "All the logarithms used are base 2. <br>\n",
    "Assumes 2 self-interested agents alternating turns. <br>\n",
    "Baseline (1 for each agent) gets updated after each episode ends (see corpses). <br>\n",
    "Rewards only possible at the end of each game. <br>\n",
    "Uses same (numerical) encoder for both item context and proposal. Reference code uses 3 distinct ones. It also has max_utility = num_types instead of 10 for us.<br>\n",
    "Check how message policy works again; paper seemed to imply that each output of the lstm is a letter. (we take the hidden output and make a probability over letters out of it).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Network\n",
    "import torch\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "# cuda\n",
    "use_cuda = 0\n",
    "\n",
    "# Random seeds for testing\n",
    "num_seed = 10\n",
    "torch.manual_seed(num_seed)\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    torch.cuda.manual_seed(num_seed)\n",
    "np.random.seed(num_seed)\n",
    "\n",
    "# Utility functions\n",
    "from utility import truncated_poisson_sampling, create_item_pool, create_agent_utility, rewards_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Game setup\n",
    "num_agents = 2         # Number of agents playing the game\n",
    "num_types = 3          # Number of item types\n",
    "max_item = 5           # Maximum number of each item in a pool\n",
    "max_utility = 5       # Maximum utility value for agents\n",
    "\n",
    "# Turn sampling\n",
    "lam = 7                # Poisson parameter\n",
    "max_N = 10             # Maximum number of turns\n",
    "min_N = 4              # Minimum number of turns\n",
    "\n",
    "# Linguistic channel\n",
    "num_vocab = 10         # Symbol vocabulary size for linguistic channel\n",
    "len_message = 6        # Linguistic message length\n",
    "\n",
    "# Training\n",
    "alpha = 0.001          # learning rate\n",
    "N_ep = 500              # Number of episodes\n",
    "num_games = 128        # Number of games per episode (batch size)\n",
    "\n",
    "# Appendix\n",
    "lambda1 = 0.05         # Entropy regularizer for pi_term\n",
    "lambda2 = 0.0001        # Entropy regularizer for pi_utt\n",
    "lambda3 = 0.005        # Entropy regularizer for pi_prop\n",
    "smoothing_const = 0.7  # Smoothing constant for the exponential moving average baseline\n",
    "\n",
    "# Miscellaneous\n",
    "ep_time = 100         # Print time every ep_time episodes\n",
    "ep_record = 10        # Record training curve every ep_record episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class combined_policy(nn.Module):\n",
    "    def __init__(self, embedding_dim = 100, batch_size = 128, num_layers = 1, bias = True, batch_first = False, dropout = 0, bidirectional = False):\n",
    "        super(combined_policy, self).__init__()\n",
    "        # Save variables\n",
    "        self.embedding_dim = embedding_dim # Hidden layer dimensions\n",
    "        self.batch_size = batch_size       # Batch size (updated every forward pass)\n",
    "        self.log_p = torch.zeros([batch_size,1], requires_grad=True)                     # Store policy log likelihood for REINFORCE\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            self.log_p = self.log_p.cuda()\n",
    "        \n",
    "        # Encoding -------------------------------------------------------------\n",
    "        \n",
    "        # Numerical encoder\n",
    "        self.encoder1 = nn.Embedding(max_utility+1, embedding_dim)\n",
    "        # Linguistic encoder\n",
    "        self.encoder2 = nn.Embedding(num_vocab+1, embedding_dim)\n",
    "        \n",
    "        # Item context LSTM\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        # Linguistic LSTM\n",
    "        self.lstm2 = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        # Proposal LSTM\n",
    "        self.lstm3 = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        \n",
    "        # Outputs of the 3 LSTMS get concatenated together\n",
    "        \n",
    "        # Feed-forward\n",
    "        self.ff = nn.Linear(3*embedding_dim, embedding_dim)\n",
    "        \n",
    "        # Output of feed-forward is the input for the policy networks\n",
    "        \n",
    "        # Policy ---------------------------------------------------------------\n",
    "        \n",
    "        # Termination policy\n",
    "        self.policy_term = nn.Linear(embedding_dim, 1)\n",
    "        # Linguistic policy\n",
    "        self.policy_ling = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        self.ff_ling = nn.Linear(embedding_dim, num_vocab)\n",
    "        # Proposal policies\n",
    "        self.policy_prop = nn.ModuleList([nn.Linear(embedding_dim, max_item+1) for i in range(num_types)])\n",
    "        \n",
    "    def forward(self, x, test, batch_size=128):\n",
    "        # Inputs --------------------------------------------------------------------\n",
    "        # x = list of three elements consisting of:\n",
    "        #   1. item context (longtensor of shape batch_size x (2*num_types))\n",
    "        #   2. previous linguistic message (longtensor of shape batch_size x len_message)\n",
    "        #   3. previous proposal (longtensor of shape batch_size x num_types)\n",
    "        # test = whether training or testing (testing selects actions greedily)\n",
    "        # batch_size = batch size\n",
    "        # Outputs -------------------------------------------------------------------\n",
    "        # term = binary variable where 1 indicates proposal accepted => game finished (longtensor of shape batch_size x 1)\n",
    "        # message = crafted linguistic message (longtensor of shape batch_size x len_message)\n",
    "        # prop = crafted proposal (longtensor of shape batch_size x num_types)\n",
    "        # entropy_loss = Number containing the sum of policy entropies (should be total entropy by additivity)\n",
    "        \n",
    "        # Update batch_size variable (changes throughout training due to sieving (see survivors below))\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Extract inputs ------------------------------------------------------------\n",
    "        \n",
    "        # Item context\n",
    "        x1 = x[0]\n",
    "        # Previous linguistic message\n",
    "        x2 = x[1]\n",
    "        # Previous proposal\n",
    "        x3 = x[2]  \n",
    "\n",
    "        # Encoding ------------------------------------------------------------------\n",
    "\n",
    "        # Initial embedding\n",
    "        x1 = self.encoder1(x1).transpose(0,1)\n",
    "        x2 = self.encoder2(x2).transpose(0,1)\n",
    "        x3 = self.encoder1(x3).transpose(0,1) # Same encoder as item context\n",
    "        \n",
    "            \n",
    "        # LSTM for item context\n",
    "        h1 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial hidden\n",
    "        c1 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial cell\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            h1 = h1.cuda()\n",
    "            c1 = c1.cuda()\n",
    "\n",
    "        for i in range(x1.size()[0]):\n",
    "            _, (h1,c1) = self.lstm1(x1[i].view(1,self.batch_size,self.embedding_dim),(h1,c1))\n",
    "        x1_encoded = h1\n",
    "        \n",
    "        # LSTM for linguistic\n",
    "        h2 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial hidden\n",
    "        c2 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial cell\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            h2 = h2.cuda()\n",
    "            c2 = c2.cuda()\n",
    "\n",
    "        for i in range(x2.size()[0]):\n",
    "            _, (h2,c2) = self.lstm2(x2[i].view(1,self.batch_size,self.embedding_dim),(h2,c2))\n",
    "        x2_encoded = h2\n",
    "        \n",
    "        # LSTM for proposal\n",
    "        h3 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial hidden\n",
    "        c3 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial cell\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            h3 = h3.cuda()\n",
    "            c3 = c3.cuda()\n",
    "\n",
    "        for i in range(x3.size()[0]):\n",
    "            _, (h3,c3) = self.lstm2(x3[i].view(1,self.batch_size,self.embedding_dim),(h3,c3))\n",
    "        x3_encoded = h3\n",
    "\n",
    "        # Concatenate side-by-side\n",
    "        h = torch.cat([x1_encoded,x2_encoded,x3_encoded],2).squeeze()\n",
    "\n",
    "        # Feedforward\n",
    "        h = self.ff(h)\n",
    "        h = F.relu(h) # Hidden layer input for policy networks\n",
    "        \n",
    "        # Policy ------------------------------------------------------------------\n",
    "\n",
    "        # Termination -----------------------------------------------\n",
    "        p_term = F.sigmoid(self.policy_term(h)).float()\n",
    "\n",
    "        # Entropy\n",
    "        one_tensor = torch.ones(self.batch_size,1)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            one_tensor = one_tensor.cuda()\n",
    "        entropy_term = -(p_term * (p_term).log()) - ((one_tensor-p_term) * (one_tensor-p_term).log())\n",
    "        entropy_term = torch.sum(entropy_term)\n",
    "        \n",
    "        if test:\n",
    "            # Greedy\n",
    "            term = torch.round(p_term).long()\n",
    "        else:\n",
    "            # Sample\n",
    "            term = torch.bernoulli(p_term).long()\n",
    "            \n",
    "        # log p for REINFORCE\n",
    "        log_p_term = torch.zeros(self.batch_size,1)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            log_p_term = log_p_term.cuda()\n",
    "\n",
    "        log_p_term = ((term.float() * p_term) + ((one_tensor-term.float()) * (one_tensor-p_term))).log()\n",
    "\n",
    "        # Linguistic construction ----------------------------------\n",
    "        h_ling = h.clone().view(1,self.batch_size,self.embedding_dim) # Initial hidden state\n",
    "        c_ling = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial cell state\n",
    "        letter = torch.zeros(self.batch_size,1).long() # Initial letter (dummy)\n",
    "        entropy_letter = torch.zeros([self.batch_size,len_message])\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            c_ling = c_ling.cuda()\n",
    "            letter = letter.cuda()\n",
    "            entropy_letter = entropy_letter.cuda()\n",
    "        \n",
    "        # log p for REINFORCE \n",
    "        log_p_letter = torch.zeros([self.batch_size,1])\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            log_p_letter = log_p_letter.cuda()\n",
    "\n",
    "        message = torch.zeros(self.batch_size,len_message) # Message\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            message = message.cuda()\n",
    "        for i in range(len_message):\n",
    "            embedded_letter = self.encoder2(letter)\n",
    "\n",
    "            _, (h_ling,c_ling) = self.policy_ling(embedded_letter.view(1,self.batch_size,self.embedding_dim),(h_ling,c_ling))\n",
    "            logit = self.ff_ling(h_ling.view(self.batch_size,self.embedding_dim))\n",
    "            p_letter = F.softmax(logit,dim=1).float()\n",
    "\n",
    "            entropy_letter[:,i] = -torch.sum(p_letter*(p_letter+1e-8).log(),1)\n",
    "\n",
    "            if test:\n",
    "                # Greedy\n",
    "                letter = p_letter.argmax(dim=1).view(self.batch_size,1).long()\n",
    "            else:\n",
    "                # Sample\n",
    "                letter = torch.multinomial(p_letter,1).long()\n",
    "                \n",
    "            # Gather the probabilities for the letters we've picked\n",
    "            probs = torch.gather(p_letter, 1, letter)\n",
    "            log_p_letter = log_p_letter + (probs).log()\n",
    "                \n",
    "            message[:,i] = letter.squeeze()\n",
    "            \n",
    "        message = message.long()\n",
    "        entropy_letter = torch.sum(entropy_letter)     \n",
    "   \n",
    "        # Proposal ----------------------------------------------\n",
    "        p_prop = []\n",
    "        prop = []\n",
    "        \n",
    "        #prop = torch.zeros([self.batch_size,num_types]).long()\n",
    "        entropy_prop_list = [0,0,0]\n",
    "        \n",
    "        # log p for REINFORCE \n",
    "        log_p_prop = torch.zeros([self.batch_size,1])\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            log_p_prop = log_p_prop.cuda()\n",
    "\n",
    "        for i in range(num_types):\n",
    "            p_prop.append(F.sigmoid(self.policy_prop[i](h)))\n",
    "            \n",
    "            entropy_prop_list[i] = -torch.sum(p_prop[i]*p_prop[i].log())\n",
    "            \n",
    "            p_prop[i] = p_prop[i].view(self.batch_size,max_item+1)\n",
    "\n",
    "            if test:\n",
    "                # Greedy\n",
    "                #prop[:,i] = p_prop[i].argmax(dim=1)\n",
    "                prop.append(p_prop[i].argmax(dim=1))\n",
    "            else:\n",
    "                # Sample\n",
    "                #prop[:,i] = torch.multinomial(p_prop,1)\n",
    "                prop.append(torch.multinomial(p_prop,1))\n",
    "                \n",
    "            # Gather the probabilities for the letters we've picked\n",
    "            probs = torch.gather(p_prop[i], 1, prop[i].view(self.batch_size,1))\n",
    "            log_p_prop = log_p_prop + probs.log()\n",
    "              \n",
    "        prop = torch.stack(prop).transpose(0,1)\n",
    "        entropy_prop = sum(entropy_prop_list) # Entropy for exploration\n",
    "\n",
    "        # Combine -----------------------------------------------------------------\n",
    "        entropy_loss = torch.sum(lambda1*entropy_term + lambda3*entropy_prop + lambda2*entropy_letter)\n",
    "        self.log_p = log_p_term + log_p_letter + log_p_prop\n",
    "\n",
    "        return (term,message,prop, entropy_loss, log_p_term,log_p_letter,log_p_prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = combined_policy()\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    net = net.cuda()\n",
    "\n",
    "# Dummy inputs\n",
    "x = torch.randint(0,max_item,[128,6]).long()\n",
    "y = torch.randint(0,num_vocab,[128,6]).long()\n",
    "z = torch.randint(0,max_item,[128,3]).long()\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    z = z.cuda()\n",
    "\n",
    "blah = net([x,y,z],True)\n",
    "\n",
    "# Initialize agents\n",
    "Agents = []\n",
    "for i in range(num_agents):\n",
    "    Agents.append(combined_policy())\n",
    "    if torch.cuda.is_available() and use_cuda:\n",
    "        Agents[i] = Agents[i].cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start ----------------\n",
      "Runtime for episodes 0-100: 16.96652388572693s\n",
      "Runtime for episodes 100-200: 27.50291609764099s\n",
      "Runtime for episodes 200-300: 38.33348608016968s\n",
      "Runtime for episodes 300-400: 30.562882900238037s\n",
      "End ------------------\n",
      "Total runtime: 143.52769088745117s\n"
     ]
    }
   ],
   "source": [
    "baselines = [0 for _ in range(num_agents)] # Baselines for reward calculation\n",
    "\n",
    "# Initialize optimizers for learning\n",
    "optimizers = []\n",
    "for i in range(num_agents):\n",
    "    optimizers.append(optim.Adam(Agents[i].parameters()))\n",
    "    \n",
    "# Train rewards\n",
    "r_list = []\n",
    "for i in range(num_agents):\n",
    "    r_list.append([])\n",
    "\n",
    "print('Start ----------------')\n",
    "time_start = time.time()\n",
    "time_p1 = time.time()\n",
    "# Loop over episodes\n",
    "for i_ep in range(N_ep):\n",
    "    # Setting up games -----------------------------------------------------------------------\n",
    "    # Game setup\n",
    "    # Truncated Poisson sampling for number of turns in each game\n",
    "    N = truncated_poisson_sampling(lam, min_N, max_N, num_games)\n",
    "    # Item pools for each game\n",
    "    pool = create_item_pool(num_types, max_item, num_games)\n",
    "    if torch.cuda.is_available() and use_cuda:\n",
    "        N = N.cuda()\n",
    "        pool = pool.cuda()\n",
    "    # Item contexts for each game\n",
    "    item_contexts = [] # Each agent has different utilities (but same pool)\n",
    "    for i in range(num_agents):\n",
    "        utility = create_agent_utility(num_types, max_utility, num_games)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            utility = utility.cuda()\n",
    "        item_contexts.append(torch.cat([pool, utility],1))\n",
    "        \n",
    "    # Initializations\n",
    "    survivors = torch.ones(num_games).nonzero()               # Keeps track of ongoing games; everyone alive initially\n",
    "    num_alive = len(survivors)                                # Actual batch size for each turn (initially num_games)\n",
    "    prev_messages = torch.zeros(num_games, len_message).long() # Previous linguistic message for each game\n",
    "    prev_proposals = torch.zeros(num_games, num_types).long()  # Previous proposal for each game\n",
    "    if torch.cuda.is_available() and use_cuda:\n",
    "        survivors = survivors.cuda()\n",
    "        prev_messages = prev_messages.cuda()\n",
    "        prev_proposals = prev_proposals.cuda()\n",
    "            \n",
    "            \n",
    "    rewards = [torch.zeros(num_games), torch.zeros(num_games)]       # Rewards for each game for each agent\n",
    "    # Keep track of sum of all rewards (from all games in a batch) for baseline updates (see corpses below)\n",
    "    reward_sums = []\n",
    "    for i in range(num_agents):\n",
    "        reward_sums.append(torch.zeros(1)) # Just a number\n",
    "        Agents[i].log_p = torch.zeros([num_games,1], requires_grad = True)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            rewards[i] = rewards[i].cuda()\n",
    "            reward_sums[i] = reward_sums[i].cuda()\n",
    "            Agents[i].log_p = Agents[i].log_p.cuda()\n",
    "\n",
    "    # Play the games -------------------------------------------------------------------------\n",
    "    for i_turn in range(max_N): # Loop through maximum possible number of turns for all games\n",
    "        \n",
    "        # Losses for each agent\n",
    "        reward_losses = [torch.zeros([],requires_grad=True), torch.zeros([],requires_grad=True)]  \n",
    "        entropy_losses = [torch.zeros([],requires_grad=True), torch.zeros([],requires_grad=True)] # Exploration\n",
    "        for j in range(num_agents):\n",
    "            Agents[j].log_p = torch.zeros([num_alive,1], requires_grad = True)\n",
    "            \n",
    "            if torch.cuda.is_available() and use_cuda:\n",
    "                reward_losses[j] = reward_losses[j].cuda()\n",
    "                entropy_losses[j] = entropy_losses[j].cuda()\n",
    "                Agents[j].log_p = Agents[j].log_p.cuda()\n",
    "        \n",
    "        # Agent IDs\n",
    "        id_1 = i_turn % 2    # Current player\n",
    "        id_2 = int(not id_1) # Other player\n",
    "        \n",
    "        # Remove finished games (batch size decreases)\n",
    "        N = N[survivors].view(num_alive, 1)\n",
    "        pool = pool[survivors].view(num_alive, num_types)\n",
    "        prev_messages = prev_messages[survivors].view(num_alive, len_message)\n",
    "        prev_proposals = prev_proposals[survivors].view(num_alive, num_types)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            N = N.cuda()\n",
    "            pool = pool.cuda()\n",
    "            prev_messages = prev_messages.cuda()\n",
    "            prev_proposals = prev_proposals.cuda()\n",
    "        # Quantities different for each agent\n",
    "        for j in range(num_agents):\n",
    "            item_contexts[j] = item_contexts[j][survivors].view(num_alive,num_types*2)\n",
    "            #rewards[j] = rewards[j][survivors].view(num_alive)\n",
    "            #reward_losses[j] = reward_losses[j][survivors].view(num_alive)\n",
    "            if torch.cuda.is_available() and use_cuda:\n",
    "                item_contexts[j] = item_contexts[j].cuda()\n",
    "        \n",
    "        # Agent currently playing\n",
    "        Agent = Agents[id_1]             \n",
    "        item_context = item_contexts[id_1]\n",
    "        \n",
    "        # Play the game -------------------------------------------------------------\n",
    "        term, prev_messages, proposals, entropy_loss, lt,ll,lp = Agent([item_context, prev_messages, prev_proposals], True, num_alive)\n",
    "        entropy_losses[id_1] = entropy_loss\n",
    "        \n",
    "        # Compute reward loss (assumes 2 agents) ------------------------------------\n",
    "        # Games terminated by the current agent (previous proposal accepted)\n",
    "        \n",
    "        finishers = term.squeeze().nonzero()          # squeeze is for getting rid of extra useless dimension that pops up for some reason\n",
    "        num_finishers = len(finishers)\n",
    "\n",
    "        if len(finishers) != 0:\n",
    "            pool_12 = pool[finishers].view(num_finishers,num_types)\n",
    "            \n",
    "            share_2 = prev_proposals[finishers].view(num_finishers,num_types) # Share of other (previous proposal) \n",
    "            share_1 = pool_12 - share_2 # Share of this agent (remainder)\n",
    "            \n",
    "            # Zero reward if proposal exceeds pool\n",
    "            invalid_batches = torch.sum(share_2>pool_12,1)>0\n",
    "            share_2[invalid_batches] = 0\n",
    "            share_1[invalid_batches] = 0\n",
    "            \n",
    "            utility_1 = item_contexts[id_1][:,num_types:] # Recall that item context is a concatenation of pool and utility\n",
    "            utility_1 = utility_1[finishers].view(num_finishers,num_types)\n",
    "            utility_2 = item_contexts[id_2][:,num_types:]\n",
    "            utility_2 = utility_2[finishers].view(num_finishers,num_types)\n",
    "\n",
    "            log_p_1 = Agents[id_1].log_p[finishers].view(num_finishers,1)\n",
    "            log_p_2 = Agents[id_2].log_p[finishers].view(num_finishers,1)\n",
    "\n",
    "            # Calculate reward and reward losses\n",
    "            r1, rl1 = rewards_func(share_1, utility_1, pool_12, log_p_1, baselines[id_1])\n",
    "            r2, rl2 = rewards_func(share_2, utility_2, pool_12, log_p_2, baselines[id_2])\n",
    "         \n",
    "            #for i in range(num_finishers):\n",
    "                #print(r1[i], r2[i])\n",
    "                #if r1[i]==0:\n",
    "                #    print(share_2[i])\n",
    "                #    print(share_1[i])\n",
    "                #    print(utility_1[i])\n",
    "                #    print(utility_2[i])\n",
    "                #    print(pool_12[i])\n",
    "                #print(lt[i])\n",
    "                #print(lp[i])\n",
    "                #print(ll[i])\n",
    "                #print(baselines)\n",
    "            \n",
    "            # Add rewards and reward losses\n",
    "            rewards[id_1] = r1.squeeze()\n",
    "            rewards[id_2] = r2.squeeze()\n",
    "            reward_losses[id_1] = rl1\n",
    "            reward_losses[id_2] = rl2\n",
    "            reward_sums[id_1] = reward_sums[id_1] + rewards[id_1].sum()\n",
    "            reward_sums[id_2] = reward_sums[id_2] + rewards[id_2].sum()\n",
    "\n",
    "        prev_proposals = proposals # Don't need previous proposals anymore so update it\n",
    "        \n",
    "        # Gradient descent -----------------------------------------------------------\n",
    "        \n",
    "        for i in range(num_agents):\n",
    "            # optimize\n",
    "            loss = reward_losses[i] - entropy_loss\n",
    "            \n",
    "            optimizers[i].zero_grad()\n",
    "            loss.backward()\n",
    "            optimizers[i].step()\n",
    "        \n",
    "        # Wrapping up the end of turn ------------------------------------------------\n",
    "        # Remove finished games\n",
    "        # In term and term_N, element = 1 means die\n",
    "        term_N = (N <= (i_turn+1)).view(num_alive,1).long() # Last turn reached; i_turn + 1 since i_turn starts counting from 0\n",
    "        # In survivors, element = 1 means live\n",
    "        survivors = (term+term_N) == 0\n",
    "\n",
    "        # Check if everyone's dead\n",
    "        if survivors.sum() == 0: # If all games over, break episode\n",
    "            # Baseline updates\n",
    "            for i in range(num_agents):\n",
    "                # Update with batch-averaged rewards\n",
    "                baselines[i] = smoothing_const * baselines[i] + (1-smoothing_const)*reward_sums[i]/num_games\n",
    "            break;\n",
    "            \n",
    "        # Reshape\n",
    "        survivors = ((term+term_N) == 0).nonzero()[:,0].view(-1,1)\n",
    "        num_alive = len(survivors) # Number of survivors\n",
    "\n",
    "        #print('i_turn = ' + str(i_turn))\n",
    "        \n",
    "    #print('i_ep = ' + str(i_ep))\n",
    "    if (i_ep % ep_time == 0) and (i_ep != 0):\n",
    "        time_p2 = time.time()\n",
    "        print('Runtime for episodes ' + str(i_ep-ep_time) + '-' + str(i_ep) + ': ' + str(time_p2 - time_p1) + 's')\n",
    "        time_p1 = time_p2\n",
    "        \n",
    "    if (i_ep % ep_record == 0):\n",
    "        for j in range(num_agents):\n",
    "            r_list[j].append(reward_sums[j]/num_games)\n",
    "    \n",
    "    #print('----------------')\n",
    "    \n",
    "print('End ------------------')\n",
    "time_finish = time.time()\n",
    "print('Total runtime: ' + str(time_finish-time_start) + 's')\n",
    "\n",
    "#for i in range(num_agents):\n",
    "#    torch.save(Agents[0].state_dict(),'saved_model_agent_' + str(i) + '.pt')\n",
    "    \n",
    "#Agents[0].load_state_dict(torch.load('saved_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Agents[0].ff.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.5584, -0.7907,  0.1409,  1.4287, -0.5893,  1.0795,  0.3065,\n",
       "          -0.4560, -1.2641,  0.7436,  1.9096,  1.6795,  1.3270, -0.6309,\n",
       "          -0.0486,  1.6310,  1.0969, -0.7225,  0.4933, -0.3953, -0.2595,\n",
       "           0.0711,  0.8033,  1.0432, -1.2801,  0.3592, -1.3392, -0.2980,\n",
       "           0.2436,  1.0658,  0.6458, -1.8925,  1.5777, -0.1021, -0.8419,\n",
       "          -0.1497, -0.9936,  0.4530,  0.6284, -0.7238,  0.0942, -0.2792,\n",
       "           1.5958,  0.1547, -0.6374,  0.2372, -0.0156, -1.0563,  0.4309,\n",
       "          -0.1283, -0.4732,  0.3364, -0.0201,  0.3057, -0.6327, -1.3136,\n",
       "           0.6472, -0.2028, -0.9147, -0.4413, -0.5445,  1.1012, -0.1601,\n",
       "           0.2235,  0.1716,  0.8635,  1.5176,  2.2821, -0.5125,  0.7366,\n",
       "           0.8697, -0.9169,  0.5475, -0.6047,  0.3469, -0.3529,  1.6346,\n",
       "          -0.8272, -2.3246, -0.9728,  2.1884,  0.9029, -0.3156, -2.0569,\n",
       "          -0.7566, -0.7808, -0.5229, -0.7290, -1.5636,  1.1469, -0.7626,\n",
       "          -0.7614, -2.2353,  0.9225,  0.3296, -0.8910,  0.4213, -0.7871,\n",
       "           1.5681, -0.2654],\n",
       "         [-0.3675, -0.8872,  0.0991,  1.0083, -0.5281,  0.8868,  0.8472,\n",
       "           0.5314,  0.9653,  0.7753, -0.0294,  0.4472, -0.2754, -1.1716,\n",
       "           0.2029,  1.2038,  0.2738,  0.2058, -0.6071,  1.2108, -0.9069,\n",
       "          -1.0759, -0.1918,  0.5047, -0.3806,  0.0670, -2.1428, -2.8081,\n",
       "           1.6834, -0.6649,  0.6838, -1.1902,  1.1059,  0.2810,  0.3408,\n",
       "           0.4193,  0.3422,  0.4593,  0.9157, -1.2289,  1.1661, -0.3579,\n",
       "           0.6429, -0.7165,  0.8422, -1.1864, -0.0855,  1.4098, -0.0605,\n",
       "          -1.1724, -1.5377, -1.1309, -0.9459,  0.6494, -0.6994,  1.3952,\n",
       "          -0.1212,  2.2225,  0.4808,  0.8186,  0.3606, -0.1543, -1.7998,\n",
       "          -0.6077, -0.0589,  0.0945,  0.3233, -0.2393, -1.3589,  2.4268,\n",
       "           0.2469, -2.1447,  0.7780,  1.0757, -0.3207,  0.2023,  0.2881,\n",
       "           0.0546, -0.3778, -1.2500,  0.3209,  1.3500,  1.5594, -1.9237,\n",
       "           0.5545,  0.6290,  0.8387,  0.2174, -1.7095, -0.7932,  1.2015,\n",
       "          -0.1391, -0.5721, -0.1671, -1.2898,  0.1585,  1.9766, -0.4910,\n",
       "           0.3109,  0.5450],\n",
       "         [ 0.5541,  0.9885,  1.3751,  0.5094,  0.1883,  0.5258, -0.6131,\n",
       "          -0.3563,  1.7706,  0.4280,  0.4160, -0.9186,  1.5696,  1.9158,\n",
       "          -0.1453, -0.6917, -0.5888,  0.8854,  1.1061,  1.7917,  1.1770,\n",
       "           0.6007, -0.4209, -0.0565, -0.5622,  0.7977,  0.3516,  0.4928,\n",
       "          -1.7377, -0.1982, -0.8149, -0.0723, -0.9505, -0.5965, -2.6593,\n",
       "          -0.0979, -0.0083, -0.2750, -0.7422,  1.2468, -0.5366, -1.5941,\n",
       "           0.0045, -1.0004, -0.7593, -1.6300,  0.5151, -0.4369, -2.8256,\n",
       "          -0.0597, -0.6508, -0.4391,  0.3299, -1.1545, -0.8456,  1.6413,\n",
       "           1.6510,  0.9847,  0.2160, -0.3523, -1.0604,  3.0532, -1.0029,\n",
       "           0.7130,  1.2173, -0.6110, -0.5152, -0.1886,  0.6513, -0.2469,\n",
       "           0.4286, -0.2505,  0.0385, -0.4683, -0.5180,  0.3645, -1.1115,\n",
       "          -0.1997, -0.2554,  0.6727,  0.4029, -0.5674, -0.4129, -0.9015,\n",
       "          -3.9127, -0.0607, -0.7339,  0.4641,  0.6226, -0.3939,  1.0530,\n",
       "           2.6633, -1.5890, -1.3787, -1.1948,  1.0669,  0.1543,  1.1217,\n",
       "          -0.5551,  0.4204],\n",
       "         [ 0.4350, -0.3741, -0.0421,  0.8062, -0.1040, -0.5135, -0.3900,\n",
       "           0.8887,  0.6216, -0.4541,  1.3183,  0.2050,  0.4110, -0.3203,\n",
       "          -1.3161,  1.0644, -0.5783, -0.0799, -0.0921,  0.2743, -1.1546,\n",
       "          -1.2890,  0.7316,  0.6631, -0.2555, -0.0548,  1.7747,  0.0478,\n",
       "          -0.3701, -0.4127,  1.4731, -0.5085,  0.4899, -0.2214, -1.2693,\n",
       "           1.1128, -0.6708, -0.5510,  0.1956, -0.0564, -0.1826,  1.0629,\n",
       "           0.3981,  0.7539, -0.1891,  0.8211, -1.1440, -0.4406,  0.6875,\n",
       "           2.3134, -0.8450, -0.1102,  0.4024,  0.2265,  0.3617,  1.1275,\n",
       "          -0.8792, -0.9035, -1.6342, -0.1944,  0.8847, -0.1175,  0.3895,\n",
       "           0.3087,  0.8108, -0.2638,  0.9106,  0.3812, -0.9402,  0.4985,\n",
       "           0.0688, -0.4687, -0.7613, -0.0908, -1.5663,  1.4720,  0.1826,\n",
       "          -0.8152,  0.0200,  0.7322,  0.1129,  1.1236, -0.7688,  0.7889,\n",
       "           0.5037, -0.4979, -2.0464, -0.3152,  0.2834, -0.7088,  1.4190,\n",
       "           0.7036,  1.7310, -0.7354, -0.1697, -0.4796, -0.1553,  0.2904,\n",
       "           0.0979, -0.0388],\n",
       "         [-0.6421, -0.1494, -0.6945,  0.6388, -0.2036,  0.9391,  1.1601,\n",
       "          -0.9884, -2.6774,  0.2212, -1.2233, -0.8410, -0.7545, -1.1011,\n",
       "           0.0141,  0.7847,  0.6489,  0.4363,  0.8182,  0.3509, -0.8328,\n",
       "          -0.6750, -1.4806, -0.9208, -1.1701, -0.6919,  1.1760,  1.3762,\n",
       "          -0.4788, -1.6458, -0.1650, -0.8674,  0.3929, -0.1029, -0.3577,\n",
       "           0.7148, -0.3573, -0.4887,  0.3706, -0.4333, -0.9144,  0.0334,\n",
       "          -0.1632, -2.5438,  0.1684, -0.0159,  0.6096, -1.7974,  0.6295,\n",
       "           3.1264, -0.1915, -0.4032, -0.6779,  0.7542,  0.4158,  0.8717,\n",
       "          -0.8017, -0.5269,  0.2083,  1.3494,  0.4578, -0.3246,  0.8144,\n",
       "           0.2716, -0.6606, -0.1115, -1.7426, -0.3371,  0.7519,  0.7097,\n",
       "           1.6833, -0.1916,  0.4128,  0.0672,  0.7112, -1.4597,  0.5770,\n",
       "          -0.2961,  0.8664,  0.2292, -0.6883, -0.2985,  1.1452, -0.1555,\n",
       "           0.2982,  0.6697, -1.7037,  1.7767,  0.9155,  0.4449, -1.4332,\n",
       "           0.5826,  2.1915,  0.9757, -2.8698,  0.2761, -1.1375,  0.5505,\n",
       "          -0.1640,  0.7711],\n",
       "         [-0.0551,  0.7428,  0.5195, -1.8120,  0.1172,  1.3745, -1.1607,\n",
       "          -0.4540,  0.0434, -0.7671, -0.7165, -0.1472, -1.3735, -0.5211,\n",
       "          -0.7982,  0.0131, -0.5057, -0.8862,  0.4568, -0.2755,  2.6795,\n",
       "          -0.9092,  0.1633, -0.4221, -0.5372,  0.6803, -0.5267,  0.5352,\n",
       "          -1.5182, -1.4860,  0.4258,  0.0944,  2.1298, -0.4250,  1.8213,\n",
       "          -0.8973, -1.6314, -0.4793, -0.9159, -1.3523,  0.0727,  1.5447,\n",
       "           0.4886,  0.3677,  0.6038,  0.8762, -1.2844, -0.9917,  0.6600,\n",
       "          -0.3405, -0.9060,  0.7689, -0.8422,  1.8417,  0.0645,  2.5282,\n",
       "           1.4362, -0.0283,  1.2642, -0.3416, -0.0006, -0.3040, -0.6293,\n",
       "           0.4625, -0.7021,  0.6820, -0.9867,  0.4812, -0.2136,  0.2870,\n",
       "          -0.6293, -0.2754,  0.9847, -0.0460,  0.7296,  3.2300, -0.0749,\n",
       "           1.8433, -0.6305, -1.6138, -1.2003, -0.1567,  0.5162,  0.8485,\n",
       "           0.3560,  0.1903,  1.0273,  0.5617,  0.5228,  1.8136, -1.1095,\n",
       "          -1.8713, -0.1836,  0.7117, -0.5212,  0.3543,  2.5995,  0.4297,\n",
       "          -0.4769, -0.4432]]), Parameter containing:\n",
       " tensor([[ 0.9501, -0.5622, -1.2345,  ..., -0.3144, -0.2195, -1.6158],\n",
       "         [-1.3308,  0.4858,  0.8518,  ..., -0.0032,  0.2478,  0.5941],\n",
       "         [-0.2859, -0.9720,  1.1122,  ..., -1.1657, -0.9918, -0.5163],\n",
       "         ...,\n",
       "         [-0.4523, -0.1050, -1.9282,  ..., -1.7200,  1.0882, -0.0808],\n",
       "         [-1.4068,  1.8528, -0.6117,  ...,  0.7585, -0.8207, -1.4663],\n",
       "         [-1.0484, -2.2061,  1.6272,  ...,  2.0382,  1.1163,  0.3552]]), Parameter containing:\n",
       " tensor([[ 2.2191e-02,  7.4817e-02,  8.1275e-02,  ...,  7.1547e-02,\n",
       "           6.9190e-02, -6.6410e-02],\n",
       "         [ 5.7779e-02,  1.6219e-02, -4.9097e-02,  ...,  1.9735e-02,\n",
       "          -7.7880e-02,  7.8228e-02],\n",
       "         [-1.9167e-02,  3.9858e-02, -4.3427e-02,  ..., -3.3245e-02,\n",
       "           8.9531e-02, -9.6423e-02],\n",
       "         ...,\n",
       "         [-8.8793e-02, -7.3076e-03, -6.0739e-02,  ..., -5.6009e-02,\n",
       "          -9.2457e-02,  8.5020e-02],\n",
       "         [-1.1385e-03,  9.6318e-02, -9.9786e-02,  ...,  5.6387e-02,\n",
       "           1.4826e-03,  9.7197e-02],\n",
       "         [-9.3404e-03,  8.4590e-02,  2.7401e-02,  ..., -5.7038e-02,\n",
       "          -6.2607e-02,  5.9407e-02]]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [[ 5.9713,  3.1686,  2.1387,  ...,  2.0034,  6.8307,  6.8652],\n",
       "         [ 2.3087, -9.2150, -1.5164,  ..., -6.3284, -8.1877, -2.2758],\n",
       "         [ 2.0328,  5.2229,  9.2911,  ..., -2.3640, -5.8935,  8.5955],\n",
       "         ...,\n",
       "         [-4.0718, -9.6592, -7.8029,  ..., -9.9405, -2.9672, -7.2257],\n",
       "         [ 2.1869, -3.1207,  9.1469,  ...,  2.5691,  7.9773, -2.5708],\n",
       "         [ 8.9127,  2.3239, -0.7079,  ..., -0.4856,  4.7995,  1.4863]]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [ 6.7245, -0.6586,  8.7754,  6.2309, -9.5328, -7.2304, -1.9959,\n",
       "          9.2770,  8.1079, -0.8585,  3.3492, -7.6310,  9.6855, -3.5067,\n",
       "         -2.7389,  3.1717, -2.5619,  7.9831,  0.7162, -9.5017, -9.5437,\n",
       "         -6.1870,  8.3637,  4.9935,  9.0849,  3.5417,  8.2932,  8.0995,\n",
       "          8.5340, -7.3730, -3.1678, -1.5319,  7.0407,  8.5556,  6.4700,\n",
       "         -3.7532, -5.8407,  3.6109, -7.5735,  1.4564,  1.8578,  8.3418,\n",
       "         -9.1270,  3.0193,  4.6458,  7.6755,  9.6769,  9.3027,  5.8008,\n",
       "          6.8421, -0.0569,  8.1451,  7.5592,  8.1131,  3.1627,  3.5547,\n",
       "         -8.2102, -8.4780, -0.8528,  1.9828, -1.3803,  0.5665,  7.2533,\n",
       "          2.5426, -0.5784, -4.3533,  8.4097, -0.6564,  7.5677, -2.7235,\n",
       "          7.7150, -0.7341,  5.6853,  2.9714,  8.6944,  1.0661,  8.5979,\n",
       "         -8.1822, -3.7432,  7.9657, -8.5269,  1.8771,  7.1066,  1.5686,\n",
       "         -3.3157, -0.7312,  5.1356,  3.2173, -9.5275, -5.6339,  5.5200,\n",
       "          4.2772, -9.8524,  7.4248, -7.3035,  7.1324,  9.5614,  9.1674,\n",
       "         -7.5611,  6.2813, -9.6478, -0.0786, -1.0559,  7.3693,  0.4322,\n",
       "          5.8891, -3.6847, -1.0628, -0.0759,  4.5797,  5.1297, -8.9063,\n",
       "         -4.6366, -7.4117, -9.0455,  3.7991,  6.2185, -6.1851, -9.0044,\n",
       "         -0.7061,  9.7842, -1.3374,  8.6338,  3.7858,  4.1134,  4.3753,\n",
       "          0.4391,  7.0071,  4.6744,  7.2809, -0.8312,  0.0146,  4.0044,\n",
       "          7.1287,  5.8816, -1.3787,  4.4762, -4.7444,  5.5855, -6.9071,\n",
       "         -3.9681,  7.8809,  9.9755,  4.0574, -0.3813,  8.8512, -8.9467,\n",
       "          3.2728, -5.9402,  4.2201, -3.0983,  1.1894,  2.9622, -8.8285,\n",
       "         -0.2820, -4.9255,  7.9155, -1.2919,  4.9439,  9.2788, -6.6223,\n",
       "         -1.2894,  2.2011,  4.8246, -0.2464,  6.9765,  6.0706,  8.9771,\n",
       "         -8.7580, -1.8567, -6.8285,  9.7960, -7.8072, -1.8534,  6.0011,\n",
       "         -7.1479,  9.8057,  2.3157, -3.9069, -6.2827, -1.8020,  7.7843,\n",
       "         -8.8053, -9.6244, -9.6065,  2.3434, -1.0771, -0.5829, -0.4496,\n",
       "         -4.9158, -5.2338,  8.8559, -8.3535,  2.9787, -7.5464, -8.6952,\n",
       "          4.7235,  5.5940, -5.8979, -2.0056,  4.0431,  3.5955,  6.1069,\n",
       "          0.0791,  2.4566,  0.1488,  1.3270,  6.0515,  8.8009, -2.3616,\n",
       "          7.9361,  4.9551,  5.0521, -2.8633, -9.3514,  5.6460,  6.4456,\n",
       "         -8.7793,  0.7790, -8.2998, -8.7425, -1.7214, -0.4585,  4.6049,\n",
       "         -6.6968, -2.3556, -9.1030, -7.0324,  4.6011, -8.2978, -2.8952,\n",
       "         -4.0098,  0.8149,  3.2234, -1.8644,  7.1937,  6.7253, -1.1803,\n",
       "         -8.5628, -4.1273, -7.6801,  5.8980, -7.1731,  8.1459,  8.4266,\n",
       "          1.5688, -0.3259, -6.4826,  0.0430, -8.9594, -3.8344,  7.8769,\n",
       "          8.0619,  9.6815, -0.1229,  7.6345,  4.5669,  2.0028, -2.5688,\n",
       "          3.8255,  1.0156,  8.7435,  1.8552, -8.5634, -2.6972, -8.6056,\n",
       "         -2.6542, -1.6711,  7.7974,  5.5875,  3.8891, -5.9392, -1.4769,\n",
       "         -9.4509, -5.2295, -3.3265, -4.3973,  5.2466, -7.7439,  1.5015,\n",
       "         -7.8263, -1.4673,  9.8783, -4.3129, -6.0684, -1.8438, -6.4951,\n",
       "         -0.3480, -3.2080, -8.9741, -3.0298,  0.9437,  1.3158,  1.4117,\n",
       "         -3.5002,  7.0504, -2.3818, -6.1910,  3.8925,  1.1132, -4.0584,\n",
       "         -0.5241,  5.1540,  6.0567, -7.0341, -4.8529,  2.6543,  9.0160,\n",
       "         -0.8964,  3.0672,  0.5407,  1.8293,  8.5712, -9.0014,  0.2272,\n",
       "         -2.7241, -3.8679,  6.4273, -7.8462,  8.8033,  6.4227,  7.6925,\n",
       "          0.1778, -0.3323, -6.7242,  6.4639,  2.0997, -1.7294, -0.9238,\n",
       "         -9.8739,  2.1216,  2.5502,  6.6028,  3.9970,  8.5391, -8.9284,\n",
       "         -1.9039, -6.2136,  3.7731,  7.1217,  1.8109,  2.9968, -0.9933,\n",
       "          5.9319, -5.7744,  9.8966,  3.8158,  4.0832,  5.6173,  3.8646,\n",
       "         -4.5578, -4.8098, -3.0750,  4.6100, -2.3354, -8.1564, -5.7435,\n",
       "         -0.1192, -9.6805,  4.0709,  0.1101,  7.1347, -4.4301, -4.3037,\n",
       "          7.4320, -2.8862,  7.1136,  3.8554,  6.6653,  7.0565, -0.9074,\n",
       "         -1.8146, -1.7458, -1.6045,  2.4380, -1.3163,  6.8364,  5.1660,\n",
       "         -6.5499,  8.6987, -0.4508,  4.0891, -5.3564,  3.7322, -3.8492,\n",
       "         -3.1593, -8.9647, -9.8380,  8.5004, -1.8626, -8.6216,  8.4021,\n",
       "         -0.1201, -8.8145, -6.3077,  0.5086,  2.2825,  6.4874,  9.8355,\n",
       "          9.9094]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [ 6.0189, -1.3288, -2.6329, -1.8464, -6.9410, -5.1085, -8.0115,\n",
       "          7.3947,  4.3267,  8.5821,  8.5904,  1.4584, -1.5575, -2.8507,\n",
       "         -4.9488,  1.8168,  1.5135,  9.9725,  7.2095, -8.2515, -1.7607,\n",
       "          7.4669, -8.2001, -5.9684,  5.2472,  5.9519,  9.1346, -4.2763,\n",
       "          8.4926, -3.7295, -6.6761,  5.1281,  6.0173, -6.4874, -6.4177,\n",
       "          9.8772,  7.6006, -6.4508,  6.3987, -5.0911,  1.0408,  7.3422,\n",
       "         -7.9046,  9.8214,  8.3310, -4.5775, -4.3052, -1.8165,  2.9024,\n",
       "          8.4817, -3.1765, -9.4250, -8.0801,  2.8578,  3.0425,  0.5078,\n",
       "          9.1696,  6.8086, -5.0144,  5.7790,  8.0209,  2.1992,  2.5747,\n",
       "         -5.6459,  9.3143, -4.4767, -4.3056,  5.1747, -5.8664, -3.5385,\n",
       "         -7.7968, -1.2258, -9.7748, -2.3125, -3.5648,  6.1927,  3.0577,\n",
       "         -7.5472,  9.1050, -1.3162, -4.1477, -1.1412,  0.6068, -8.0481,\n",
       "         -4.3245,  2.8727, -2.6549,  6.7608, -0.0270,  8.0594, -4.7669,\n",
       "          9.8656,  5.1742,  9.3776, -3.8992, -4.0521,  7.3752, -9.0316,\n",
       "         -0.7741, -7.5584,  8.5317,  4.9083,  9.8513, -2.3869, -4.8657,\n",
       "          7.0226,  6.7309, -6.5249, -4.8552, -6.0851,  6.2487, -9.0664,\n",
       "         -6.2783,  2.2363,  7.5633,  6.8579, -3.5579, -8.6117,  2.5395,\n",
       "          7.7434, -7.9721,  5.9932,  3.5568,  6.1079,  8.4354,  7.0158,\n",
       "         -5.3729, -4.7935, -9.3588, -3.9507, -4.0346,  1.5467, -6.2311,\n",
       "         -2.1240, -0.8343, -2.3571,  5.5207,  5.2123, -8.4750,  2.2880,\n",
       "         -9.5209,  3.9143,  7.4919, -6.2504, -2.6690,  4.7974,  0.7846,\n",
       "          2.9415,  5.5759, -0.1959,  4.7319, -3.7796, -4.6542, -8.9677,\n",
       "         -9.8724,  5.1420, -1.8239, -3.5856,  9.6867, -4.5563, -9.0238,\n",
       "          1.0044,  9.5014,  5.4718,  6.6752,  7.6954,  8.4780,  3.3097,\n",
       "          2.0151, -1.7600,  3.8867,  9.0707,  1.5058, -4.1530,  5.6690,\n",
       "          2.0171, -1.6394,  7.3658,  0.1054, -7.9169, -3.5026,  2.9893,\n",
       "          9.1418,  4.9775,  2.2296,  3.7135, -8.5802,  5.3324, -6.5174,\n",
       "         -4.0471,  6.4000, -5.8488, -4.5728,  0.1389,  7.7887, -4.7476,\n",
       "          3.4143,  4.0057,  7.7361, -8.4227,  7.9570,  1.6361, -9.8557,\n",
       "         -2.5531,  1.5256,  6.5841,  8.1152, -6.9023, -3.5464, -0.2372,\n",
       "         -5.9082,  9.6042,  8.8198, -2.9219,  0.7342,  0.0952,  3.1942,\n",
       "         -2.2675,  1.9730, -8.2508, -3.4517,  8.5809, -7.8968,  1.9680,\n",
       "         -3.3636,  4.9790, -7.4231, -3.4030, -4.7362,  9.4187, -9.1307,\n",
       "          1.2980,  0.4562,  4.7165,  8.5630,  5.0143, -8.9388, -4.2581,\n",
       "         -4.3424, -5.5895,  2.0372,  7.0550,  1.1431, -4.0061, -4.6392,\n",
       "         -3.5905, -1.2607,  1.3029,  1.5748,  2.0320,  3.7536,  2.6783,\n",
       "         -8.6875, -3.0796,  1.6295,  4.2161,  9.6846, -8.4934, -0.4012,\n",
       "          9.2968,  3.3835,  5.4665, -5.5652, -4.1088,  8.6071,  5.4964,\n",
       "         -5.1024,  1.6772, -9.6038, -3.4582, -8.5982,  7.7956, -7.6101,\n",
       "          9.5614, -2.4657, -8.0381,  2.9291, -6.1164,  7.5738,  8.4668,\n",
       "         -1.6088,  6.1506,  4.5591,  9.6033,  7.4930, -2.9436, -5.9782,\n",
       "         -1.4670, -7.0227, -8.5990,  8.8400,  9.2774, -4.4417, -0.5792,\n",
       "         -7.5463,  5.6384,  7.1784, -1.5463, -9.1122,  7.4193,  2.3180,\n",
       "         -9.4449, -9.7068, -2.6267, -0.2557,  4.9270,  0.7370,  2.1092,\n",
       "          6.8289,  4.6434, -8.6847, -6.5310, -2.3112,  6.7080, -8.4139,\n",
       "         -7.9265, -8.1854,  3.3192,  5.3537, -3.4979, -3.1951, -1.3125,\n",
       "         -1.9851, -9.8125, -6.8389, -3.1254, -2.0523, -1.1887, -5.0729,\n",
       "         -2.3041, -4.8133,  4.3748, -9.8304, -6.1235, -9.5237,  2.5693,\n",
       "         -2.1080, -7.3184,  2.9909,  5.4485, -2.9430, -3.4877,  2.5722,\n",
       "          9.2295, -4.7111,  5.3952, -9.5304,  8.8008, -7.1892, -2.8406,\n",
       "         -7.8320,  5.3764, -8.1298,  8.1938, -7.8621,  7.7664,  8.6428,\n",
       "         -9.8320,  7.9847, -9.2593,  8.2213,  1.5971,  2.2553, -1.2238,\n",
       "         -6.0876, -5.9706,  3.8890, -9.6536, -3.1696, -9.4052, -3.7891,\n",
       "         -4.7376, -0.0028,  1.6451,  7.4878, -9.3077, -3.3267, -2.4925,\n",
       "          3.9942, -1.8258,  2.2081,  2.6333,  3.3700, -2.3548,  7.5713,\n",
       "         -8.2786, -3.7208,  1.7101,  9.4347, -8.2273, -7.4760, -8.4970,\n",
       "         -2.4031, -3.9740, -7.7694,  7.4882, -0.6176,  3.0796,  0.1306,\n",
       "         -6.4472]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [[ 8.8841, -7.1464, -3.2267,  ..., -8.7960, -9.7236,  0.9627],\n",
       "         [ 3.7970, -4.3814, -8.0733,  ..., -5.6576, -9.6449,  4.5549],\n",
       "         [-4.9769,  4.7063, -7.8550,  ...,  7.4314, -4.3336, -1.5986],\n",
       "         ...,\n",
       "         [ 7.1142, -2.3423,  3.2118,  ..., -6.5724,  7.4973,  8.8554],\n",
       "         [ 9.6939,  9.8369, -0.8013,  ..., -4.5405,  6.7567,  1.2074],\n",
       "         [-0.3585,  8.8919,  5.2819,  ..., -5.4565,  2.2979, -0.8157]]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [[-9.1370, -5.3446, -2.4935,  ..., -4.6787, -2.5645,  9.4277],\n",
       "         [ 9.0323,  0.6262,  6.1074,  ...,  3.8384, -6.5086,  7.9781],\n",
       "         [-6.8944, -6.9716, -5.8721,  ...,  1.9147, -5.2857,  5.9952],\n",
       "         ...,\n",
       "         [-5.0958,  3.6296,  2.3986,  ..., -5.6799,  3.0400,  0.5116],\n",
       "         [-7.9405,  3.7516,  8.7538,  ...,  9.2966,  4.6496, -7.7039],\n",
       "         [ 6.9141,  6.3299, -1.7484,  ...,  0.9791,  4.1801, -1.9893]]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [-4.9996,  2.6416, -7.4479,  7.9798, -6.3719, -4.1216, -7.8166,\n",
       "          3.5219, -5.2268,  8.3751, -1.6497, -6.8095,  0.2643, -7.9057,\n",
       "         -0.6238,  9.1553, -7.9940, -8.7405, -0.7897,  8.5917, -2.8242,\n",
       "          7.1731, -8.2037, -1.1480, -3.3553, -4.8966, -2.2716,  6.1380,\n",
       "          2.0065,  0.6802, -1.9905, -8.0017, -3.2452, -1.8298,  6.5740,\n",
       "          8.2003,  5.0318,  7.4965, -7.9749,  6.3139,  5.3075,  4.1105,\n",
       "         -8.2405, -5.5801,  9.3925,  9.1648, -2.4267, -6.3984,  2.8751,\n",
       "          3.4465, -4.7850,  7.7909, -0.1389,  7.5219,  7.6139,  2.2390,\n",
       "         -4.6081, -5.8200, -7.5269, -3.7450,  8.1472, -4.1395, -0.1148,\n",
       "         -6.0395,  8.0766,  4.4759,  8.2208, -3.1872,  0.9773, -6.6834,\n",
       "         -2.6961, -9.0271, -1.1160,  3.4096, -2.5515, -5.0585, -4.6846,\n",
       "         -7.4121, -0.5025, -4.9065,  2.6466, -1.5025,  1.6010,  4.2343,\n",
       "         -9.2609,  1.4884,  7.1720, -6.2204,  4.5660, -0.8536, -2.7539,\n",
       "          7.4937, -0.8101,  7.3335,  5.8716, -8.3979,  2.7838, -8.0659,\n",
       "          5.7741,  0.6490,  9.1627,  8.4165, -6.1459,  8.9800, -7.5521,\n",
       "          9.9905,  9.9017,  6.4148,  1.2530, -0.0499, -9.7692,  1.1167,\n",
       "         -1.5694, -5.4733,  8.1061,  5.0886,  2.2909, -7.5934,  5.1114,\n",
       "          1.6150,  0.7077,  3.8547, -5.1905, -6.2259,  7.0292, -7.5137,\n",
       "          6.0785,  2.6628,  6.5526,  3.9270, -7.5963,  8.8194, -5.5917,\n",
       "         -4.0481, -8.6810, -3.7054, -1.8977, -6.3768,  8.6604,  1.9057,\n",
       "         -9.3310, -8.9863, -2.2081, -6.1591, -1.1787, -0.0341,  7.4347,\n",
       "          2.4095, -6.0015, -2.7595,  9.0726,  8.1638,  1.8528, -8.1041,\n",
       "          0.3188,  8.7751, -3.4104, -9.9021, -2.0996,  5.2260, -3.2017,\n",
       "         -6.1146,  4.3809,  9.5711, -6.8944, -5.5770, -2.0732,  4.4576,\n",
       "         -3.0467,  3.3706, -5.6889, -7.2899, -1.3409,  2.4542,  8.4693,\n",
       "          5.8511,  4.0607,  5.1920, -1.6466,  9.2976, -9.8787, -4.6724,\n",
       "          2.3372,  2.5573, -2.1513,  9.5783,  3.3586, -8.2494, -0.0882,\n",
       "          1.0299, -7.4340, -7.2002,  9.9796, -3.4269, -8.5775,  5.0808,\n",
       "         -5.8885,  3.7899, -0.5742,  5.3062, -6.8937, -5.5200,  1.5853,\n",
       "         -5.4239, -1.2972,  5.4517,  2.7898,  2.2350,  7.8065, -9.7986,\n",
       "         -4.9892, -6.2393,  0.1386,  1.5250, -4.4317, -6.9417, -6.7518,\n",
       "          5.4515,  9.6236, -0.1180,  4.9948,  1.7321, -7.4605, -4.2767,\n",
       "         -4.2973, -5.7536, -1.1029, -3.4431, -5.8709,  2.5843,  9.4263,\n",
       "          2.9201, -5.5130,  7.2804,  6.4618,  6.7631,  5.6508, -7.2060,\n",
       "          5.7789, -6.7350,  3.0800,  8.4522, -7.3394, -6.2784, -9.6417,\n",
       "          9.1870, -2.3849, -1.3285,  7.8946,  9.1513, -5.1566,  6.6184,\n",
       "         -3.3600, -5.7824, -5.0154,  0.7384, -9.1789,  8.4052,  2.8040,\n",
       "         -1.0438,  8.1192, -5.1437, -3.8191, -1.7754,  6.7639,  4.4239,\n",
       "          4.5230,  5.0355, -8.4626,  8.3013,  2.3516,  1.6569, -2.2354,\n",
       "         -8.6029,  9.8192,  8.9763, -4.5754, -8.1966, -1.0679, -1.3966,\n",
       "          5.7293,  1.6839, -0.3622, -3.0810,  5.8950,  2.3400, -3.3350,\n",
       "          3.7285, -8.9940, -1.2348, -4.9555, -6.2669, -5.4533,  0.1838,\n",
       "          6.3856,  5.6697,  5.4600,  4.9320, -8.2936, -9.1207, -7.1932,\n",
       "          1.5500,  1.6465, -1.7673,  5.9791,  3.4362,  8.4547,  0.4331,\n",
       "         -6.4745, -9.2137, -8.8667, -7.3976,  4.9551,  2.8262,  3.8011,\n",
       "         -5.6599,  4.9579,  0.8711, -4.2149, -9.7356, -1.4573, -4.7886,\n",
       "          2.6562,  0.7014,  8.4791,  7.0080, -6.4484, -9.1415, -3.3303,\n",
       "          2.3911, -5.5534, -9.6919,  2.3535,  0.4104, -3.6842, -0.2953,\n",
       "         -9.3960, -5.4583, -5.0335,  9.7956, -0.0826, -2.7985,  7.1730,\n",
       "          8.9223,  1.5859,  7.2555,  7.6364, -1.7642, -0.6642, -3.8873,\n",
       "         -2.8918,  0.5945, -1.1243,  6.1283,  7.2795, -4.3486,  6.7369,\n",
       "         -6.7661,  6.8051, -5.4196, -7.9247,  9.2976,  6.1543,  1.3949,\n",
       "          2.0595, -3.5596, -7.3679, -7.9558,  6.3384,  4.7080, -1.5152,\n",
       "         -0.6016,  5.5242,  1.8501,  1.1960,  0.8105, -3.3784,  2.8581,\n",
       "          8.2292, -7.1137,  1.0616,  5.9886, -2.3609,  8.9392,  5.3507,\n",
       "          2.0244, -5.8903, -2.5028,  1.3112,  1.2220, -9.3879, -2.9640,\n",
       "         -8.2965,  9.5780, -7.8605, -8.7348, -8.2469,  9.0551, -3.7503,\n",
       "          6.9639]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [-7.6393,  8.5568,  9.8059,  4.7715, -9.5873,  0.6786, -6.3918,\n",
       "         -4.9284,  4.9197,  0.1748,  0.6611,  6.5682,  4.1743, -2.6261,\n",
       "         -8.1015, -2.6544, -5.1947, -0.7287, -9.5951, -1.3019,  2.9679,\n",
       "         -4.1889,  9.8966,  4.2647,  7.6279,  3.3405,  6.9963, -8.9600,\n",
       "          5.2416,  7.3841, -6.6354,  2.0796, -2.4931, -5.3685,  6.9487,\n",
       "          8.8732,  1.9848, -0.6322,  0.8349,  6.9277,  8.9227,  9.2193,\n",
       "         -9.3975, -6.0450,  8.0420, -9.4382, -8.5295, -9.9898,  5.0968,\n",
       "         -1.5952, -0.1515, -7.7247,  0.8242, -0.0590,  1.6079, -5.3483,\n",
       "         -9.9913,  8.8683,  2.1701, -3.9029, -5.0265,  5.1416, -8.0042,\n",
       "          3.7964,  7.5418, -6.7974,  8.0075,  7.8037,  2.1323, -5.0091,\n",
       "          6.5656, -8.4668, -3.2456, -5.2935, -5.8597,  7.4529,  8.7625,\n",
       "          7.1462,  3.4318,  4.8126, -0.6572, -5.5234, -6.1991, -0.3028,\n",
       "         -5.1292, -7.5066, -6.2539, -5.1137, -5.0329,  4.6830, -4.7065,\n",
       "         -1.1263, -1.8952, -3.4596,  9.6286, -6.2038,  1.5501, -5.0120,\n",
       "         -3.2270, -3.1903, -9.0504,  5.2177, -2.1417, -1.5126,  7.2981,\n",
       "         -8.7646,  1.8053, -1.4633, -6.8585,  8.6658,  2.8947,  2.8170,\n",
       "         -5.0780, -2.2886,  7.5483, -1.5793, -2.4220,  9.6553,  2.3271,\n",
       "         -6.0921, -5.0359,  8.9384, -1.8878,  4.8138,  8.4029,  4.1473,\n",
       "         -1.7862,  4.3649, -1.0395, -1.9403,  5.2066,  5.0290, -1.8926,\n",
       "          8.8809,  3.9461,  5.6913, -9.4074, -0.8609,  6.2859, -2.2047,\n",
       "         -2.4993, -1.5930, -2.0701, -3.4745,  2.3890,  0.2747, -3.2809,\n",
       "          5.6392, -4.6218,  7.6699,  4.3672,  0.0806, -5.3280, -0.0867,\n",
       "          4.7463,  2.7453,  6.5030, -6.0273,  3.9669, -9.4562,  8.4771,\n",
       "         -3.7662, -7.6369, -5.3907, -1.0832,  2.8796,  2.5335,  2.5781,\n",
       "          6.2495,  5.6557, -4.4413,  4.9075,  9.5661,  9.5163,  8.3829,\n",
       "         -7.1975,  2.0428, -9.2011,  0.9135,  9.9645, -7.9575,  9.5262,\n",
       "          9.7495, -4.7949, -4.0580,  4.7306,  1.7566,  6.5997,  6.9145,\n",
       "         -7.3150, -9.8983,  5.7926, -7.1592, -9.0568,  6.5992, -0.5764,\n",
       "          4.3601,  2.4501,  1.7914,  7.2517,  0.3026, -8.4176, -3.8068,\n",
       "          5.5802,  9.5881, -3.9367,  6.6040,  8.9269, -4.6992,  8.1025,\n",
       "          0.2904,  8.3522, -6.1472, -8.4797, -3.6206,  0.6931,  2.7343,\n",
       "          6.8021, -8.6976, -6.9659, -7.1803,  9.9047,  6.9373,  1.6511,\n",
       "          6.6050,  0.6960,  6.6383, -0.9717, -6.7199, -8.0494,  0.9119,\n",
       "          6.2031,  2.3510,  8.9184,  7.0772, -9.8417, -4.5960,  3.6694,\n",
       "          5.4950,  5.6394,  3.9330,  3.2269, -2.5368, -5.6040,  4.4317,\n",
       "          0.7418, -7.1221,  0.0151, -3.8378,  4.1905,  8.0547, -5.1186,\n",
       "          8.5286,  6.7936,  7.0992,  1.3856,  3.2820, -4.7614, -2.4774,\n",
       "         -0.5654,  9.9261, -8.1747,  5.9569, -1.7592, -6.1610,  5.7981,\n",
       "         -7.5315,  8.2244,  5.4488,  2.1862, -0.0965, -8.9277,  4.0914,\n",
       "          5.2610,  3.3845,  5.6909, -5.5158,  6.7297, -2.6929,  3.7844,\n",
       "         -1.4372, -2.1948,  1.9960,  4.4693, -5.4598, -6.8483,  9.7456,\n",
       "          8.8131, -3.5560, -4.4391,  2.1789, -0.2502,  5.0736,  8.9666,\n",
       "         -9.2715, -2.0260,  7.9764,  2.7409,  1.2078, -8.0000,  0.3646,\n",
       "         -3.0225,  0.2785,  0.5227,  5.4176, -0.5773, -0.7570, -2.1328,\n",
       "          2.1347, -8.3483,  0.7468, -3.3761,  4.7847,  7.3978,  0.9872,\n",
       "         -0.4095,  0.1805,  4.8209,  3.5931, -9.4058,  1.4427,  8.7630,\n",
       "         -1.2960,  4.4329,  0.7076, -6.7808,  5.5550, -1.9144, -9.4238,\n",
       "          3.4668, -3.7013, -9.7431,  3.2958,  6.6731, -7.1547, -6.1022,\n",
       "         -5.5303,  7.3839,  0.2262,  8.5676,  9.2259,  1.3863,  5.1316,\n",
       "          0.9105, -3.8576,  6.6748, -6.6745, -5.7680, -9.2261,  3.5318,\n",
       "          3.6893,  9.6670, -3.5333,  1.7074, -8.5153, -9.2276,  0.8946,\n",
       "          3.1824, -9.8191,  6.6384, -6.7160,  7.8679, -7.1486,  7.5002,\n",
       "         -4.1689, -3.6218,  8.7904, -5.0326,  1.6076, -7.4129, -5.5294,\n",
       "         -2.7946,  3.7476,  4.7254, -3.3581,  8.5601,  3.4089, -4.4644,\n",
       "         -0.9522, -1.6481,  7.7402,  5.6557, -1.4130, -3.3988, -7.1260,\n",
       "          1.0278, -0.6869,  1.6015, -5.8986, -9.9347, -1.4118,  8.1598,\n",
       "         -6.4395, -1.0196,  6.0142, -1.7922,  6.4562, -5.3934,  2.6868,\n",
       "         -0.3957]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [[ 8.8795,  6.7022, -4.2698,  ...,  7.1807,  0.1996, -4.1866],\n",
       "         [ 9.5981,  7.1148,  5.9013,  ...,  0.3150,  0.9368,  3.2636],\n",
       "         [-3.8729, -5.8204, -7.9619,  ..., -9.8043, -5.1225,  1.3209],\n",
       "         ...,\n",
       "         [-7.2495, -8.4514, -4.1570,  ..., -4.1812, -8.7157, -7.9210],\n",
       "         [ 5.3420, -3.6215, -1.0683,  ...,  8.7945,  4.8316, -8.4945],\n",
       "         [ 5.9799,  5.7587,  2.7072,  ...,  8.0207,  9.4027, -1.4508]]), Parameter containing:\n",
       " tensor([[-6.6973e-02, -4.2289e-02,  3.6512e-02,  ...,  4.9242e-02,\n",
       "           8.8212e-02, -2.8084e-02],\n",
       "         [-2.9391e-02,  4.9484e-02,  8.3532e-02,  ..., -5.9818e-02,\n",
       "           6.3659e-02,  2.0329e-02],\n",
       "         [ 7.9014e-02,  9.5963e-02,  7.1521e-02,  ...,  3.3392e-02,\n",
       "           9.1261e-02,  1.1879e-02],\n",
       "         ...,\n",
       "         [ 1.3541e-02, -9.8692e-02,  9.4808e-02,  ...,  2.7855e-02,\n",
       "          -3.9871e-03, -7.7073e-02],\n",
       "         [ 7.8820e-02, -8.9897e-03,  8.7255e-02,  ...,  2.5004e-03,\n",
       "          -8.8577e-02, -2.4226e-02],\n",
       "         [-2.8636e-02,  6.7902e-02, -3.5401e-02,  ..., -4.8619e-02,\n",
       "          -7.0731e-02, -6.4116e-02]]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [ 8.6733,  7.3550, -4.7464,  7.7949,  8.2048,  7.1014, -6.0653,\n",
       "         -8.1308, -5.1383, -0.7527, -6.5007,  1.6481, -1.9463,  2.5979,\n",
       "         -9.7399, -0.0999,  0.7816, -0.0879,  7.8883, -9.7039,  6.6938,\n",
       "         -9.7848, -5.7216, -4.7146,  3.8725, -8.2563,  7.1953, -5.1609,\n",
       "         -3.4127,  9.7190, -2.7808, -0.1896, -2.9053,  0.7078, -5.4958,\n",
       "          6.2742, -7.7871,  9.4596, -0.4289,  7.6050,  2.3500, -1.3171,\n",
       "          8.3474,  7.3804,  8.2026, -8.7075,  4.2039, -1.6799, -6.1016,\n",
       "          4.0495,  4.1915, -0.0932,  9.7068,  3.8076, -7.8759, -9.8504,\n",
       "          5.5568, -8.9533,  2.3493,  7.3791, -8.1767, -5.8153, -6.3066,\n",
       "          6.5030, -3.4470, -8.3163, -5.1126, -9.1736,  1.6662, -1.1722,\n",
       "         -3.1399,  3.9945,  2.3833,  0.6521, -6.2763, -4.3967,  2.6057,\n",
       "         -4.8302,  5.9263,  3.0653,  3.0233,  5.4811, -5.0985, -5.8159,\n",
       "         -8.4623, -6.3405,  5.4320, -7.0763, -3.2895,  2.9740,  8.3372,\n",
       "          5.1316, -0.3689, -7.7353,  1.8055,  5.9198, -3.8281,  0.1133,\n",
       "          0.5154,  0.3264, -9.7439,  1.5422,  5.9686,  4.6413, -0.2131,\n",
       "          8.6747, -8.6474, -7.3841, -4.9031,  2.1024,  1.2863, -1.8010,\n",
       "          9.1577, -2.4543,  2.6350, -5.0342,  5.1856, -9.2227,  0.9983,\n",
       "          9.9929, -5.6674, -8.9087,  0.3041, -1.2166,  0.8478, -1.1087,\n",
       "         -2.7564,  8.9657,  4.2907,  6.9358,  2.9162, -1.7329, -7.9370,\n",
       "         -8.7952,  7.4969,  7.0975, -4.9527,  2.0138,  7.8651,  4.1330,\n",
       "         -7.1161,  7.2679, -8.6936, -0.8949,  3.0561, -4.1361,  8.0690,\n",
       "         -0.7026, -3.7576, -4.8794, -0.3638,  1.8154,  6.4868,  2.0865,\n",
       "          7.3378,  6.8864, -5.0096,  3.2794, -3.5148, -7.7166, -0.5511,\n",
       "         -9.0502, -3.2328, -2.8069, -8.1983, -1.2977,  1.0208,  7.7180,\n",
       "         -8.4797,  8.7557,  2.9815, -9.3729, -2.7094,  9.2028, -5.0185,\n",
       "          4.6411, -2.3995,  5.0051,  4.5158,  3.9986, -5.6784, -6.8096,\n",
       "          3.0898,  2.8201,  4.4512,  4.4237, -2.6456, -0.4799, -9.9946,\n",
       "         -5.6992,  6.3580, -5.7472,  6.6652,  4.0268,  2.4735, -2.9685,\n",
       "          1.1394,  9.3372, -0.7336, -5.5547, -8.6074, -4.8948, -6.7637,\n",
       "         -8.4783,  5.6052,  5.7370, -6.5509,  0.8014,  0.0701,  3.9478,\n",
       "         -6.9840, -2.1808,  6.4847, -2.7766,  3.4646,  8.6250,  1.4813,\n",
       "          9.4562, -3.3512, -6.9255,  0.7773,  0.1025, -6.2618, -2.4778,\n",
       "          7.3230,  5.1733,  8.3746,  9.1661,  8.6545,  5.4536,  7.7011,\n",
       "         -0.7015,  4.8632,  0.4977,  7.9936, -5.4572, -7.5998,  1.6000,\n",
       "         -7.4568, -6.7234, -5.8962, -8.9072,  3.1343,  8.1043, -8.2101,\n",
       "          7.6215, -3.5220,  1.0289,  8.3524,  3.6819,  5.5665,  8.5996,\n",
       "          9.1415, -7.9524, -6.2060,  4.5942,  0.0929,  0.7240,  9.8576,\n",
       "          3.7545, -4.2278, -6.3610, -6.7468,  1.3623,  3.0732,  8.4371,\n",
       "          2.0024,  2.4102,  3.0310, -5.0772,  2.0426, -5.5794,  1.5222,\n",
       "         -8.4230, -5.2636, -8.6390,  6.7648,  6.8716,  1.5101, -7.5664,\n",
       "         -0.9941, -9.2406,  2.6221,  3.6237, -6.4656, -3.8197,  4.6699,\n",
       "         -8.1301,  6.3940,  6.1646, -6.9235,  4.6175,  3.2361, -2.7654,\n",
       "          6.6950, -2.6501,  5.8673,  1.2171,  3.6172,  9.2412,  0.2227,\n",
       "          0.9860, -3.2373, -5.6709,  0.2378,  5.7954,  4.9591, -8.5686,\n",
       "         -8.7900,  1.8742, -4.7927, -0.6049,  2.8381,  8.3818,  3.7046,\n",
       "          7.0869,  8.9267, -5.6103,  8.3363, -7.1946,  0.0793, -8.5285,\n",
       "          6.0948,  2.1959,  3.1737, -0.3698,  4.2071,  5.8884,  6.4328,\n",
       "          3.4702,  5.2552,  3.1756,  6.7154, -2.3079, -3.1272,  9.8239,\n",
       "          3.9581,  3.4527, -9.2887, -0.2226,  2.6535, -3.5037, -7.9360,\n",
       "         -4.6579,  8.0902,  2.1865,  3.0489,  7.9628,  0.7294,  0.7873,\n",
       "          5.6129,  9.5124,  7.1625, -8.5154, -4.2849,  3.1766,  6.6155,\n",
       "         -9.5980,  9.0728, -1.7241,  2.2435, -9.7280,  3.4752, -5.3137,\n",
       "          5.1176, -6.4445,  3.8882, -1.3484,  2.3420, -4.8948,  2.2485,\n",
       "         -3.0826, -5.6709, -3.8097,  3.5705, -7.9102,  7.5782, -8.7041,\n",
       "         -5.4377,  5.5927, -0.8856,  7.2381, -6.8928, -0.7785,  9.9656,\n",
       "          3.0277, -6.0210, -5.8727,  1.9588, -6.1890,  0.7839,  4.0791,\n",
       "          9.8832,  9.9929, -4.8321,  5.7925, -7.0154, -8.1211,  7.0538,\n",
       "         -3.8816]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [ 5.2453, -4.1763, -2.6440,  5.2278,  8.7134, -1.7289,  7.4553,\n",
       "          3.0034, -5.2835,  4.2822,  8.1646, -1.6802, -5.7891,  1.2011,\n",
       "         -0.7960, -5.7374, -9.8667, -4.6948, -6.0313,  5.2870, -4.1230,\n",
       "         -8.5821,  4.1554,  1.2945,  5.6860, -1.2292, -0.1756,  5.1919,\n",
       "         -2.8306,  3.6532, -6.6036,  1.0590, -6.3315,  2.7782,  9.9205,\n",
       "         -0.7367,  1.0271,  8.0705, -1.1609,  0.0987, -3.6525, -3.8422,\n",
       "         -5.7262, -2.5459,  5.5596, -4.0829, -4.0983,  9.0572,  9.1893,\n",
       "          6.0116, -6.6727,  1.3911, -0.1062,  2.8999,  0.7172, -8.9524,\n",
       "         -6.4756,  4.7376,  0.8133, -1.0029, -8.2857, -2.0141,  5.4687,\n",
       "         -0.0818, -0.9710,  5.2507, -1.7622, -2.9306,  4.9403, -0.1488,\n",
       "          6.1851, -3.8461,  8.7511, -4.3206,  7.0576,  6.1696,  2.4415,\n",
       "         -9.1777,  5.5954,  2.7734,  6.6249, -3.2210,  8.6290,  2.9846,\n",
       "         -6.7528, -7.1851, -9.5435,  9.2084,  3.5231, -4.3688, -3.0995,\n",
       "         -6.5892, -0.9406,  0.2483, -9.0247,  2.9089,  9.2131,  8.0864,\n",
       "          8.2223, -4.8859,  1.7033,  7.2508, -1.6653,  2.2673, -9.0153,\n",
       "          1.7913, -6.0548, -1.6513,  0.7218, -6.3128,  8.3348, -5.7643,\n",
       "         -2.6503, -9.6592, -0.6962, -7.4252,  1.8116,  1.8668,  9.3129,\n",
       "         -4.6843,  2.4022,  1.9105,  9.3566, -2.4781,  5.1592, -1.5995,\n",
       "         -3.5733,  0.6243,  5.4357,  3.9718,  0.5514,  5.8027,  2.6243,\n",
       "         -8.0499, -5.1554,  9.2330, -9.0254,  6.4101, -1.0932,  1.8867,\n",
       "          3.2954,  3.3091,  6.8852, -9.1626,  0.9180,  9.0478,  3.9012,\n",
       "          7.4430,  2.7107,  2.8221, -4.8247, -3.1951, -3.7914,  0.3069,\n",
       "         -1.3597, -3.6169,  4.2227, -5.4819,  4.2756, -7.6699,  1.3737,\n",
       "          0.0794, -7.3223, -3.1748, -6.3613,  6.0363, -1.2145, -3.4343,\n",
       "         -4.0928,  5.3379,  6.7241,  8.5953,  3.7358,  7.8846,  9.1765,\n",
       "         -4.2605,  4.1287,  1.6480, -2.1186, -3.6232,  6.7109,  4.3037,\n",
       "         -5.7111,  5.7374, -1.4291, -5.2291, -5.3382,  3.4748, -2.0157,\n",
       "          1.0464, -3.9345, -8.5319, -1.8470, -7.3912, -2.1955,  3.0300,\n",
       "         -9.7252, -4.6055,  3.6272, -6.3837,  5.0441, -6.0757, -3.5097,\n",
       "         -9.3439,  4.7728,  1.6598,  3.3011,  5.5788, -6.8649, -7.4094,\n",
       "         -2.0860, -0.0010, -3.8877,  2.8904,  9.6773, -1.0021,  5.7904,\n",
       "         -7.0739,  5.6578,  1.1888, -3.3150,  0.1256,  7.8530, -2.0035,\n",
       "         -2.5238, -8.0529, -5.8536, -8.5023, -2.5573,  2.0122, -8.0916,\n",
       "         -7.7719,  7.1855, -6.6929,  5.1734,  5.8652,  9.5703, -7.6957,\n",
       "          7.5373,  1.7492,  1.7714, -8.6811,  3.8621,  4.6243, -6.1688,\n",
       "          9.6172,  3.7343,  5.7028,  4.0203, -6.6098,  7.6028,  3.4267,\n",
       "          8.4935, -5.9265, -1.4188,  2.9690,  0.6971,  8.3949, -2.1454,\n",
       "          5.6424,  9.4658, -8.1449, -1.0320, -8.8881,  2.2029, -7.2830,\n",
       "         -0.8382, -4.9011,  1.9306, -8.1151, -8.6439,  1.4233,  9.5985,\n",
       "         -8.8246,  7.7545, -9.2273,  6.3886,  4.4535,  1.2386, -0.1714,\n",
       "         -5.0837,  5.0608,  8.5139, -5.6536, -8.1560,  6.1038, -8.3698,\n",
       "          9.0428,  9.0987, -7.7327,  3.2821,  8.7172, -9.0875,  0.0849,\n",
       "          3.5328,  0.8682,  9.0249, -6.0080, -4.2065, -7.8651, -9.9746,\n",
       "         -9.9250,  7.9840, -0.4621,  8.8077, -5.7492, -7.6909,  5.1283,\n",
       "         -9.3456,  1.4363, -3.1627,  3.8582,  6.7981, -5.1056,  1.8287,\n",
       "          3.4640, -4.1551, -3.1541, -3.4353,  4.2280, -7.5935,  2.1545,\n",
       "         -1.6229,  7.7280,  2.6349,  8.8568, -8.4393,  7.6544,  3.6566,\n",
       "          1.1563, -2.6665, -0.6594, -1.3491, -3.9990,  5.2664,  2.2423,\n",
       "          9.9034,  1.4893, -4.2570, -2.6611,  9.9473, -3.0748,  0.9587,\n",
       "          3.7238, -7.2129, -3.8618,  9.1473, -2.9568, -4.8775,  1.5946,\n",
       "          0.4598,  0.5189,  4.8385,  0.5893, -6.9841, -1.9561,  0.0925,\n",
       "         -5.3312,  9.6797, -3.1881,  3.7969, -5.6140,  4.3884,  6.9683,\n",
       "         -0.0263,  5.7093, -9.6695, -3.9732,  6.8982,  2.7023, -4.3916,\n",
       "         -2.4930,  0.3525,  2.0655, -3.9460,  2.2314, -6.2739,  4.5948,\n",
       "          0.4968,  6.5389, -5.6581,  1.3712,  2.5261,  3.6898, -7.3164,\n",
       "         -7.6029,  0.2209, -2.6625, -7.8913, -8.4572, -6.9265,  1.6397,\n",
       "          8.0957, -3.6414, -7.5823,  5.4206,  9.1697,  3.3520, -6.8636,\n",
       "         -7.4860]), Parameter containing:\n",
       " tensor([[ 5.2767e-02,  2.6316e-02,  1.7453e-02,  ..., -5.5936e-02,\n",
       "           1.7629e-02,  2.4326e-02],\n",
       "         [ 5.6828e-02, -3.8130e-02,  5.0129e-02,  ...,  5.4329e-02,\n",
       "           3.8570e-02,  4.3825e-02],\n",
       "         [ 4.6830e-02,  4.9759e-02, -5.2082e-02,  ..., -3.6562e-02,\n",
       "          -3.2842e-02,  2.4350e-02],\n",
       "         ...,\n",
       "         [-4.8553e-02, -3.7148e-02,  5.7997e-03,  ..., -5.3584e-02,\n",
       "          -9.8606e-03, -2.7201e-02],\n",
       "         [ 3.0535e-02,  6.2151e-03, -6.1545e-03,  ...,  5.5221e-02,\n",
       "          -4.4199e-02, -4.9378e-02],\n",
       "         [-2.4247e-02, -5.4027e-02, -1.9925e-02,  ..., -4.5851e-02,\n",
       "           2.9504e-02,  1.3915e-03]]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [-3.2797,  4.1779,  4.1912,  0.9835, -0.6414,  4.8246,  1.0298,\n",
       "          2.8321,  2.0987, -2.2547, -2.2125,  4.3111,  0.2145, -2.7648,\n",
       "         -4.3212,  1.8013,  2.1794,  0.6298,  1.8510,  0.8533, -5.7389,\n",
       "         -2.6706, -3.0752, -3.7967,  1.4514,  4.5157,  0.2364,  0.3754,\n",
       "          0.6229,  3.6982, -4.3217, -4.6129,  4.9902, -3.4975, -1.0816,\n",
       "         -3.8545, -0.5992,  2.8900, -1.5471,  3.7062,  4.5747, -1.3383,\n",
       "         -3.8110, -3.6694, -2.7555,  2.4119,  3.0828, -1.3647, -0.7000,\n",
       "         -0.1415, -0.9402, -2.4165,  0.3416, -4.0816,  2.4843, -0.7189,\n",
       "          2.9091,  5.3097, -5.2427,  5.4939,  0.0596,  3.2566, -3.9901,\n",
       "         -5.3646,  4.2095, -4.4389,  2.9337, -4.3202,  4.2347,  3.4753,\n",
       "         -1.3924,  3.8156,  0.7049,  2.5905,  2.5643, -4.1671,  4.7437,\n",
       "         -0.8284,  0.1747, -3.4176,  5.5614, -4.1966, -2.8090, -0.1430,\n",
       "         -0.1288,  3.5586,  0.0531, -2.2011,  0.3737, -0.3599, -3.5147,\n",
       "         -3.3231, -4.0275, -2.8036, -3.6540,  4.9856,  3.9096, -5.2036,\n",
       "          1.0754,  4.4171]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [[ 0.3485,  9.1803, -0.1841, -0.2157,  2.5339,  2.5855,  8.7397,\n",
       "          -8.5932,  1.5360, -9.0122,  5.3860,  5.8055,  8.1900,  8.3025,\n",
       "           5.5470,  0.9276,  8.0070,  1.7674,  6.6407, -8.1703, -6.7426,\n",
       "          -7.6387, -3.2062, -3.8623, -7.9140, -4.7114,  3.9147, -5.9446,\n",
       "           7.6138, -0.4828, -8.6802, -9.0865,  3.8914,  2.2473, -1.9932,\n",
       "          -1.0047,  1.2007, -1.2423, -6.3418,  0.6590,  0.4069,  8.9473,\n",
       "           8.8880, -3.4673,  0.5339, -9.2542,  6.6206, -2.3403, -3.9362,\n",
       "          -1.4028, -2.5907, -6.7764, -1.5114, -1.1256, -9.1892,  4.8700,\n",
       "          -8.7792,  7.4127, -7.3299, -1.9750, -8.2851, -1.5866, -1.8917,\n",
       "          -3.1994, -9.6475,  6.2506,  6.9776, -0.4950, -4.8328,  7.0403,\n",
       "           5.3968,  2.3849, -2.9409,  4.8731, -0.6210, -7.6733, -8.2035,\n",
       "           3.2117,  0.9644, -0.4253, -0.8891, -6.5961, -0.3171, -4.9506,\n",
       "           4.7846, -6.2633, -3.2578, -5.6290,  3.2410,  5.5002, -8.7786,\n",
       "           4.6961, -6.3253,  2.5309, -3.1805, -6.6827,  1.2797,  9.8561,\n",
       "          -9.0870, -9.8872]]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [-5.4719]), Parameter containing:\n",
       " tensor([[ 9.5758e-02, -7.1506e-02, -5.6319e-02,  ..., -3.5278e-02,\n",
       "           4.1863e-02, -4.2381e-02],\n",
       "         [-6.3454e-02,  2.9945e-02,  3.3218e-02,  ...,  2.0197e-02,\n",
       "           6.2376e-03,  7.8962e-02],\n",
       "         [-7.2328e-02, -7.9003e-02,  4.0422e-02,  ..., -5.9357e-02,\n",
       "          -8.2361e-03, -4.1697e-02],\n",
       "         ...,\n",
       "         [-7.0333e-02,  3.3865e-02,  1.6829e-02,  ..., -9.6098e-02,\n",
       "          -2.2646e-02, -3.8883e-02],\n",
       "         [ 1.4998e-02,  2.0354e-02,  7.2454e-02,  ..., -6.6533e-03,\n",
       "           4.9836e-02,  7.4543e-02],\n",
       "         [-1.5273e-02, -1.7087e-02,  3.4712e-02,  ...,  4.0621e-02,\n",
       "          -8.7836e-02, -1.7870e-02]]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [[ 7.2478,  7.9579, -4.3981,  ..., -5.6345,  8.3217,  9.9248],\n",
       "         [ 2.0497,  9.4539,  1.4143,  ...,  0.6025, -5.1552,  7.5663],\n",
       "         [-0.5842, -7.6192,  4.1048,  ...,  1.7294, -0.0322,  5.2247],\n",
       "         ...,\n",
       "         [ 0.2417, -5.7713,  3.8743,  ...,  8.5397,  0.4542, -1.4259],\n",
       "         [ 6.1572,  9.6114,  8.3123,  ...,  3.9121, -1.7286, -5.9225],\n",
       "         [ 3.0810,  3.9078,  8.1799,  ..., -4.5136, -7.5042,  8.3825]]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [ 2.3341, -8.3513,  8.6931, -3.4285,  2.0728, -6.1296,  1.8542,\n",
       "          0.2513,  7.4450, -4.0452,  9.0566, -6.2331,  8.8255,  9.5840,\n",
       "         -5.9951, -2.8462, -3.9032,  1.3337, -7.4114, -8.0890, -4.0197,\n",
       "         -3.2237, -3.3028,  2.5572, -1.4688, -7.7270,  9.5606,  3.6831,\n",
       "         -3.0479,  9.2808, -3.4397,  6.8006,  4.3553, -1.8402, -2.3415,\n",
       "          4.7339, -8.4618,  3.1518,  8.7623, -1.6380, -0.3828,  6.4932,\n",
       "         -5.6162,  7.8333,  8.9427, -0.5024, -8.6950,  8.1754,  8.6348,\n",
       "          0.3467,  8.3292, -8.3014,  0.4916, -5.8440, -6.5390,  0.6610,\n",
       "          4.2735, -9.9738, -1.2703, -7.4513,  2.7126, -1.0770, -8.0249,\n",
       "         -2.7720, -5.5536, -0.2492, -5.2706,  9.4292, -8.2742, -8.0495,\n",
       "         -7.6771,  4.0560, -7.4742,  1.7701,  9.7713, -4.5539, -9.6367,\n",
       "          2.9450,  6.7694,  7.8573, -6.1477, -4.9661,  3.1649,  6.2675,\n",
       "         -5.1700,  6.8670, -7.7428, -4.8533, -0.3806, -8.5058, -1.1016,\n",
       "         -1.9051,  9.2196, -9.2441, -0.6803,  2.0351,  3.2333, -3.4929,\n",
       "          4.9267,  6.5189,  6.5033, -7.6404, -0.6356,  3.0158, -6.7459,\n",
       "         -9.7978,  8.1884,  6.5244, -6.0692,  0.4061, -1.9594,  0.0741,\n",
       "          3.6383,  9.9376,  5.4050, -2.2257, -9.3284,  1.5064, -3.6376,\n",
       "         -2.7915,  8.0559,  4.1328,  1.0463, -6.7584,  3.2389,  0.6289,\n",
       "          0.0676,  5.3137,  1.8057, -0.0706,  2.4520, -5.5850, -3.7482,\n",
       "          2.3057,  1.0422,  4.2473,  0.1173, -0.6290, -3.7340, -5.1914,\n",
       "          2.2924,  8.0755,  3.6151, -6.4671, -7.6574,  2.0871,  4.6030,\n",
       "          6.1403,  1.4112,  4.0759, -1.8870, -6.9814,  0.1563, -3.9568,\n",
       "          0.4575, -7.9176,  5.4231,  0.1552,  2.4243,  9.5110,  8.0184,\n",
       "         -8.6212,  9.5374,  0.8050,  6.6806,  4.2181, -7.8622,  0.7637,\n",
       "         -9.2265,  8.1460,  6.7520, -5.2753, -8.2585, -3.5011,  9.9903,\n",
       "         -5.6401, -1.3994, -3.7258, -9.5708, -5.5846,  0.1678,  6.0611,\n",
       "          2.5454,  2.7137,  7.3654, -7.5734, -1.1584, -1.8816,  2.1338,\n",
       "         -5.2607,  7.3708,  0.1486, -6.2897, -0.4552,  3.9171, -2.5653,\n",
       "          9.2036,  9.7110, -8.9729,  4.5009, -7.2444, -8.2973,  4.7805,\n",
       "          1.0590, -5.1957,  5.0259, -1.6350,  3.2263, -4.5796, -2.2475,\n",
       "         -9.5585,  6.9110,  2.5913, -7.7131, -2.1939, -3.1716, -7.3060,\n",
       "         -3.4259,  7.8040,  5.8970, -3.9544,  8.0456,  1.8399,  3.8956,\n",
       "         -0.1934,  1.8998,  6.6510, -4.6971,  4.9126, -1.9679, -4.5234,\n",
       "         -5.3217, -8.3734,  0.7968,  9.0717,  7.8351, -5.3367, -2.1748,\n",
       "          8.1937,  7.5601,  0.1876,  7.2298,  9.9074,  6.1340,  8.2490,\n",
       "         -1.5552, -1.4960, -3.5564,  0.0925, -2.0345,  8.5564, -2.2648,\n",
       "          7.3052,  9.5434, -5.6532, -7.3105, -0.7418, -5.5353, -3.0836,\n",
       "         -8.9205,  1.1365,  4.6664,  1.1867,  1.5843, -1.7605,  7.2738,\n",
       "          9.9350,  2.3437, -4.3359,  4.4264, -6.5366, -0.2884,  3.2346,\n",
       "          5.2961, -7.5142, -9.5636,  2.5422, -4.5302, -7.4668, -4.0622,\n",
       "         -3.3747, -2.1583, -3.0362, -4.9492, -8.6342,  3.4376, -9.3472,\n",
       "         -5.7098,  6.6289,  7.0572, -3.5668,  1.3118, -1.6341,  5.3428,\n",
       "          3.7221,  9.8928, -6.5965,  0.4544,  2.9628, -7.5929,  6.7583,\n",
       "         -7.9247,  6.7378,  7.8764, -9.4759, -8.8566,  1.4562,  3.7450,\n",
       "         -4.1435,  5.2326, -9.7018, -4.2847, -6.4728, -3.3013, -9.4119,\n",
       "          9.8402,  1.1362,  3.0345, -3.7298,  1.4867,  8.7709, -9.6200,\n",
       "          2.8823, -1.4072, -7.4079, -8.3159, -7.3661, -6.1233, -5.0731,\n",
       "         -5.7181,  9.7271, -6.2139,  0.0012,  3.9264,  6.5986,  4.9882,\n",
       "          0.2585, -9.9621, -6.3188, -8.7371,  9.8023, -2.7627,  4.3611,\n",
       "         -0.0678, -8.6781, -2.9395,  2.4561, -4.3419, -7.6082,  5.2681,\n",
       "          1.7486, -5.5542,  4.8619,  6.9372,  2.1495, -5.1705, -8.7325,\n",
       "         -8.0340,  9.4673,  7.8314,  3.2628,  3.1446,  6.1261,  2.3358,\n",
       "         -8.9780,  0.0640,  7.3494,  4.3836, -1.8022,  0.6498,  2.5333,\n",
       "         -6.3332,  2.2184, -2.9126,  7.2467,  9.3197, -1.4961, -8.1291,\n",
       "         -4.2885,  6.7702,  9.5091, -5.5426, -7.6212, -6.9311,  1.6015,\n",
       "         -1.5587, -9.3086,  7.9188,  2.9067, -0.1612,  0.0333, -6.7732,\n",
       "          0.0494,  3.3377,  7.7202,  4.6531,  0.6654, -2.9266,  0.1501,\n",
       "         -7.4224]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [ 6.6277,  7.0587, -4.8545,  1.2382,  0.7894,  7.9758, -6.2527,\n",
       "          6.4647,  3.9318,  8.0141, -1.3484,  9.3978,  9.9602, -1.2970,\n",
       "         -1.5192, -1.1102,  0.6158, -9.3935,  2.3792,  1.0726,  1.2633,\n",
       "          6.1812,  2.0458,  2.0867,  5.0283, -8.9131, -3.6084, -2.8108,\n",
       "          7.7833, -6.0841,  5.7730, -1.2775, -1.7955,  9.9348,  0.3430,\n",
       "         -3.2715, -4.7581,  9.2231,  1.1962, -2.0783,  9.7235,  2.1001,\n",
       "         -7.9452,  4.7203,  5.9138, -3.5807, -4.2495,  1.0793, -4.6731,\n",
       "          7.4232, -6.2490,  9.9456, -3.1985, -3.1104, -1.4558, -3.5018,\n",
       "         -3.1887,  0.4432,  1.5518,  7.5966,  9.9718, -2.4671,  3.7435,\n",
       "         -1.3359, -6.0704,  1.8331,  9.2897, -6.3594, -6.0544, -3.4624,\n",
       "          7.1710,  6.5212,  3.4786,  7.3040,  3.3243,  5.5913,  2.9563,\n",
       "          0.0695,  2.5969,  9.2436,  5.4590, -2.3117,  6.8004,  9.7477,\n",
       "          8.0190,  1.0843, -2.7505,  6.0114, -6.0768, -6.6540, -9.4162,\n",
       "          9.0741,  8.6991,  6.9076, -8.8261,  2.1206,  5.3140,  4.8668,\n",
       "          8.0026,  6.2798, -6.2181, -3.4366,  7.9684,  6.9085, -4.8144,\n",
       "          9.6794,  2.0576, -8.3206,  5.9953, -8.1794,  2.1437,  6.2948,\n",
       "         -7.1374, -9.6323, -5.1039, -9.0215,  6.0757,  3.3672,  8.2903,\n",
       "         -6.6390, -5.7072, -9.7069, -3.5467,  6.3146, -2.6246, -7.0567,\n",
       "         -0.6974,  5.7158,  9.4423, -3.6988,  7.8442, -8.4718, -0.3377,\n",
       "         -6.4377,  5.4406, -0.7944,  0.8628, -4.9746, -8.1751, -7.1989,\n",
       "          8.3475, -1.3269,  0.1818, -9.4363, -8.7767,  7.3224,  5.6349,\n",
       "         -5.3028,  5.3312,  4.9503, -6.1295,  3.5750,  0.3533, -6.7817,\n",
       "          8.7068, -7.8147, -4.0732, -7.3257, -3.3621,  0.4959, -9.2135,\n",
       "          9.3179,  3.0021,  6.0460, -8.1895, -8.2418, -4.9601,  7.7947,\n",
       "         -4.5343, -8.1484, -5.1447,  7.2747, -4.8670, -7.3886, -3.5576,\n",
       "          4.5366,  9.8692, -4.7610, -0.1288,  7.7832,  0.9181, -5.8466,\n",
       "         -7.3887, -0.9130,  4.6717, -3.9157, -4.3606,  9.0525, -8.0079,\n",
       "          1.1572,  9.4139,  6.2073, -3.4797,  5.5425,  8.5994, -8.5879,\n",
       "          3.5671, -0.0135,  6.2454,  8.1265,  1.2222,  9.6659,  8.2341,\n",
       "         -4.8982,  1.3138,  4.6762, -7.6507, -6.5248,  8.1892, -3.1417,\n",
       "         -2.4791,  8.3631,  9.4407, -1.8165, -3.3106,  2.3804, -9.1170,\n",
       "          7.2058, -7.2552,  5.5409,  2.8168,  4.2716, -8.2084, -6.4696,\n",
       "         -7.5348,  7.0387, -6.7500, -4.7219,  2.7113, -7.8383,  0.4580,\n",
       "          2.1528,  4.9122,  3.4424, -0.4928,  9.7802,  7.5985, -3.9001,\n",
       "         -1.8327,  2.4417, -1.8834,  4.0446, -0.0176, -2.9875, -9.1462,\n",
       "         -7.5858, -4.7730, -2.4291,  4.9687,  0.4587, -1.4735,  8.4391,\n",
       "         -9.8325,  9.6928, -3.5642,  1.4773, -2.0567, -4.5359,  0.8443,\n",
       "          4.6882, -2.0775,  3.7367, -2.8442, -0.1202, -1.5779,  5.4270,\n",
       "         -2.4814, -2.1677, -1.6597,  5.9341, -0.7162,  7.3026,  4.1370,\n",
       "         -7.8135, -9.5228, -5.9873,  8.5803, -4.5777,  1.9892,  4.6325,\n",
       "          0.2067, -0.3608, -3.0919,  5.7911,  7.3966, -0.9395, -1.3628,\n",
       "          8.7800,  0.2815, -1.8589,  8.2264, -5.9466, -0.2251,  9.2661,\n",
       "          4.6842,  4.2662,  9.5264, -1.1352,  1.3652, -9.5817,  4.4175,\n",
       "         -2.4149, -1.3142, -4.9738, -3.4421,  4.7665, -4.2475, -1.5139,\n",
       "         -9.5622,  0.9179,  3.7877,  9.6901, -0.8974,  5.0047, -4.9237,\n",
       "          3.6215,  3.5112, -8.6290, -5.3106, -4.1619,  9.5087, -0.1631,\n",
       "         -0.9165, -2.3986, -9.6465,  8.1133, -3.4332,  1.1252,  4.3674,\n",
       "          8.3215,  3.8659, -3.4029, -3.0340,  9.9251,  0.5256, -7.4276,\n",
       "          5.8485, -3.3378, -2.4350, -0.0846,  1.1121, -4.0297, -7.7547,\n",
       "         -0.6389, -2.6570,  9.1164, -3.8419,  2.0262, -9.5960, -6.6698,\n",
       "         -1.1939, -2.2259, -4.9383, -3.5473, -6.1625, -6.6250, -7.1618,\n",
       "         -5.3142,  3.0957, -5.8732, -1.8980, -3.5885,  9.0411,  4.1047,\n",
       "          2.2905,  5.5544,  0.3701,  6.2789,  3.5922, -3.8873, -8.4758,\n",
       "          7.8478, -9.9104,  4.1710,  3.4910, -9.1504,  1.7114, -4.3239,\n",
       "         -6.5184, -2.5950, -2.9978,  6.8422,  4.8579, -8.8963,  8.0205,\n",
       "         -6.9048, -2.5522,  1.2389, -9.0813,  7.9878,  9.5611, -1.6738,\n",
       "         -5.6703, -6.1183,  8.2455, -1.2998, -1.4307, -2.2651, -1.4433,\n",
       "         -1.9729]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [[-6.9657,  6.6740, -3.0185, -3.6968, -4.0360,  5.1343,  9.3789,\n",
       "           0.8608, -0.0993, -0.4238,  4.5599, -0.5932, -7.5914, -5.3391,\n",
       "          -3.8917, -0.3299, -9.9688,  8.8722, -2.0955,  6.7844,  2.0572,\n",
       "           6.7784,  5.5089,  6.3244, -0.7522,  1.2553,  9.9271, -2.2757,\n",
       "           3.5019, -1.7070,  4.2904, -5.0664,  4.8413,  8.4095, -3.1722,\n",
       "           6.2647, -2.1928,  3.9131, -9.6379, -5.7744, -7.2285,  9.8470,\n",
       "           8.5695, -5.6185,  9.2854,  4.0606,  3.2887, -4.7173, -4.3426,\n",
       "          -1.3555, -0.5292,  2.8474, -3.6219, -6.3121,  5.7966,  8.5223,\n",
       "          -1.3890,  7.4457,  5.7490, -9.9390,  8.9761, -5.6068,  5.5980,\n",
       "           2.1623, -3.0885,  2.8697,  2.0626, -9.0048,  0.9193,  8.7668,\n",
       "          -1.6629,  6.7655,  8.6687, -5.5523, -7.7378,  4.8589,  1.0110,\n",
       "           2.1094,  8.6383,  9.7628, -1.0197, -3.0215, -7.4820,  5.3942,\n",
       "          -2.3056, -7.9611,  0.4628, -4.6771,  8.3693,  1.0802,  2.6278,\n",
       "           1.5412,  3.3941,  9.7228, -0.7772,  8.5772, -5.0935, -2.1744,\n",
       "           4.1665,  7.9967],\n",
       "         [-3.7171,  2.5405,  6.5039,  7.7912, -6.5297,  1.9793,  5.8594,\n",
       "          -8.6740,  6.1532, -7.4951, -1.2811,  4.5603,  8.5636, -3.3028,\n",
       "          -8.3707,  7.2935, -5.3963, -2.3182,  4.9297,  0.1571, -9.8999,\n",
       "          -6.3877,  4.9127, -7.8698,  1.0009, -3.4587,  1.4151,  8.2988,\n",
       "          -0.6153, -1.4415, -9.0732,  1.8536,  3.7030,  7.7678,  7.7709,\n",
       "          -4.0333,  1.7646, -0.1168,  6.4929,  6.5667,  7.3844,  4.0268,\n",
       "           7.0523, -9.7046, -9.1568,  0.0282, -1.3197,  2.9042, -2.4756,\n",
       "           3.2558, -4.0592, -7.9452,  4.4667, -1.4470,  0.3703, -3.0408,\n",
       "           8.7114, -2.4735, -2.4292,  3.0515,  1.1102,  9.2608, -8.4528,\n",
       "           4.6930, -1.1550,  7.9739,  8.0656, -0.8153,  7.3097,  9.3449,\n",
       "           6.4573,  1.9752,  2.1697,  1.6322, -5.3632, -9.3658,  3.4620,\n",
       "          -7.3278, -0.0463,  0.7469, -7.6607,  4.8503, -4.6927,  2.8841,\n",
       "          -5.2880,  7.5608, -3.0407, -0.0532, -2.2325,  6.1090,  5.5552,\n",
       "          -6.7737, -7.9881, -7.5026, -2.4653,  1.2548, -0.7806,  7.6981,\n",
       "          -5.0878,  3.1598],\n",
       "         [-3.5576,  3.2707,  7.7982,  7.8632, -0.3986,  3.3717, -3.1448,\n",
       "           4.5154, -8.6194, -1.9463,  4.5264,  0.1261,  9.7803, -9.7281,\n",
       "          -7.2494,  1.3743,  6.4163,  0.8663, -0.7616, -3.2496, -9.7459,\n",
       "           1.9127,  2.0873,  9.4484, -3.2073,  4.3096,  0.9983,  1.7166,\n",
       "           8.5290,  4.4853,  3.9463,  5.2959, -4.1461, -5.1401,  3.3473,\n",
       "          -4.7013, -4.0735,  0.4982,  2.1882,  2.3320,  3.6606, -9.2535,\n",
       "           4.9104,  1.3808, -3.5468, -7.7425, -0.4055, -3.9622, -1.5586,\n",
       "          -5.1157,  3.1581, -6.8094,  0.5238,  1.4839, -4.2844,  6.5151,\n",
       "          -2.4353,  4.3766,  8.9905, -3.2790,  8.5052,  3.0600, -9.9623,\n",
       "          -4.2807, -9.9599,  8.7932,  0.1385, -8.2144,  5.4633,  1.8599,\n",
       "          -3.1844, -9.7647, -8.6038,  7.5793, -3.4012, -1.1575,  2.3905,\n",
       "          -9.9930, -8.0796,  5.1799, -4.8158,  8.3051, -3.4272,  1.8603,\n",
       "          -9.9224, -5.9408,  2.1635,  3.6246, -8.8634,  5.5128,  1.1793,\n",
       "          -7.6702,  3.3667,  1.8751, -9.9794,  0.3140,  0.4452, -5.9001,\n",
       "           4.7557, -2.9881],\n",
       "         [ 4.3412, -1.8519, -7.7715,  9.3485, -0.4664, -3.9118, -4.0668,\n",
       "          -9.3835, -2.6326,  7.8622,  5.5681, -0.0504, -2.9950, -5.1552,\n",
       "           4.0591,  5.9337,  6.5462,  3.4979, -4.4834, -0.4225, -0.9859,\n",
       "           9.6390, -8.9452,  6.1228, -6.5594, -8.0856,  1.4465, -6.9974,\n",
       "          -8.9976, -9.5589,  1.3500, -7.1610,  8.0880, -3.8054,  5.4348,\n",
       "           9.3258, -3.8050, -4.9722,  6.8639, -4.1202, -1.9065, -2.7725,\n",
       "           3.3646,  0.2077,  5.8091,  8.8367, -4.5669, -3.6709, -2.8774,\n",
       "           7.9437,  7.2518,  8.7483,  6.6339, -0.1380,  0.8440, -6.5468,\n",
       "          -1.7801, -4.2562,  4.1798, -0.7986,  0.9814,  8.1398, -3.2410,\n",
       "           1.9438, -0.8407,  1.6604,  5.5035, -5.0334,  0.4144,  1.4519,\n",
       "          -0.0602,  0.8249, -9.9882, -5.2079, -3.0807, -5.5363, -9.2378,\n",
       "           2.5425, -1.1831, -6.9061, -4.6442, -9.4104,  5.0483, -2.8652,\n",
       "          -7.4602,  7.8941, -4.3292, -2.7152, -1.8555, -8.7720, -0.9354,\n",
       "          -3.1212, -9.5003,  1.6793,  8.0694,  1.4430, -4.3710, -1.6821,\n",
       "           8.8157,  2.6896],\n",
       "         [ 7.7750,  9.9513,  8.6302,  8.7417,  5.2921,  9.9206,  0.6846,\n",
       "          -3.6696,  5.9629,  3.2836,  1.1241,  3.9685,  3.2729,  4.7528,\n",
       "           3.6559,  1.6954,  0.9380, -2.8876,  5.0362, -1.5161,  4.1142,\n",
       "           3.2398,  4.7999, -3.5680, -5.3504, -8.5314,  6.3533, -1.9192,\n",
       "          -6.2730,  2.0853, -8.9539, -5.8919, -2.3757, -9.3671,  8.6728,\n",
       "           1.5276, -7.8200,  8.0757,  9.2701,  2.1927,  9.3518,  5.0738,\n",
       "           2.4902, -0.5016,  5.1923,  6.2176,  1.2075,  0.1383,  5.6524,\n",
       "          -7.7232,  1.1587,  5.6591, -1.6120,  9.5562,  4.6781, -7.0523,\n",
       "          -7.7199, -4.0453,  2.8471,  4.5695, -8.0648,  3.4005, -6.7304,\n",
       "          -1.3911,  9.1281,  7.6977, -1.4171, -9.1898,  0.7619, -3.4609,\n",
       "          -4.0194, -3.5069, -6.5182,  0.5814, -7.7226, -6.4485, -6.7714,\n",
       "          -1.1792,  2.5145, -9.9915,  8.5087,  5.7920,  3.3548,  4.5153,\n",
       "           0.7845, -7.9842, -2.1622,  0.5572, -3.9325, -8.0779,  7.4600,\n",
       "          -6.3526, -1.5176,  4.2935, -8.4111, -1.1258,  1.1146,  1.0782,\n",
       "          -2.4656, -6.7420],\n",
       "         [ 5.0068, -1.0413,  7.5990, -1.7457,  9.4396,  7.5856,  2.2542,\n",
       "           2.9400,  9.4133, -9.8901, -1.4763, -7.6742, -7.7325,  5.0544,\n",
       "          -5.4525,  9.2437,  2.9842,  1.9361, -7.8657, -8.7146, -4.7535,\n",
       "          -2.6638,  5.1722,  2.5378,  9.9478, -3.3703,  5.9291, -6.3336,\n",
       "          -9.9452, -1.5759, -0.2670,  1.5178, -9.2897, -0.7553,  2.1074,\n",
       "           5.6913, -4.7663,  0.6991, -2.0268,  2.1820, -3.3390,  7.6904,\n",
       "           6.7588,  2.4316,  5.9578,  3.8015,  0.1882,  1.4519,  7.9543,\n",
       "           6.7917,  3.0525,  9.9504,  6.5185, -8.4787,  0.2096,  6.8491,\n",
       "          -4.1136,  1.8094, -8.0597, -3.6094,  6.1280, -2.0817,  5.3989,\n",
       "           2.5148, -1.7155, -5.0449,  4.1042,  9.1061, -7.5809, -9.4083,\n",
       "          -1.1252,  3.3070,  4.2245, -4.9500,  4.1283, -2.4820,  4.1077,\n",
       "          -1.0918,  8.5472,  7.0835, -8.2038,  5.0610,  9.8003, -1.8467,\n",
       "          -4.6658, -5.9026, -2.4125,  9.8717, -0.9475, -8.3611, -8.0802,\n",
       "           1.9001,  9.2655, -3.6149, -4.5049, -9.4403,  3.7023,  7.5324,\n",
       "          -4.9568,  3.2594],\n",
       "         [-3.8560, -0.2632,  5.3420, -9.8683,  3.3508,  0.1127,  6.3610,\n",
       "          -6.8820,  1.9658, -5.4028,  9.4682, -7.8279,  7.5407, -7.5764,\n",
       "          -3.4038, -0.4716,  3.5206, -8.1901,  7.5081, -2.7938, -9.8932,\n",
       "           7.7715,  5.4929, -2.0378, -6.4121, -9.1072, -3.8500,  2.4772,\n",
       "           7.0149,  6.1063, -7.3218,  1.9136,  3.9526,  5.2644, -3.1725,\n",
       "           5.3052,  4.1753, -5.1018, -4.4317,  3.4131, -7.6166,  4.1192,\n",
       "          -3.1461, -7.2379, -2.7908, -3.5930, -3.5271,  6.6250,  2.1240,\n",
       "           2.9614, -6.1410, -6.5554,  2.6503,  3.5489, -3.2665,  0.2264,\n",
       "           8.4656,  0.5311,  6.5747,  5.9975,  3.7636, -3.2841, -0.0834,\n",
       "           7.1107, -2.9476, -5.8537, -0.4623, -7.2553,  3.4321, -8.0631,\n",
       "          -4.2715,  2.2060, -7.5373,  7.0098,  4.2981,  4.6449, -8.6135,\n",
       "          -6.2958, -2.4015,  5.9249, -0.4654, -7.0055,  4.1288,  7.0317,\n",
       "          -5.6347, -9.6138,  5.2519,  1.4011, -7.4872,  5.3048, -3.7027,\n",
       "          -8.2529, -3.5851, -5.9563, -8.1111,  0.6825,  7.0778,  2.6394,\n",
       "           3.1312,  4.1489],\n",
       "         [-8.1957, -3.6216, -6.7749,  9.9855, -3.7920, -3.9218, -4.9373,\n",
       "          -5.7650, -0.0374,  6.3336,  9.2457, -8.4878,  0.9889,  6.1606,\n",
       "           4.9370,  3.5291, -0.1854,  9.2904,  6.1774,  7.0521,  9.5228,\n",
       "          -4.8116,  0.0237,  6.0918, -3.3081, -0.0684,  1.7393,  3.3148,\n",
       "          -7.3794, -1.6649, -7.9712,  9.0348,  3.7786,  8.4161,  4.5996,\n",
       "           8.9456,  5.3529, -8.3864, -3.5724,  9.0579, -5.5047, -4.2549,\n",
       "          -8.0474, -4.1290,  0.3396,  5.9903,  9.6524,  2.7501,  7.2492,\n",
       "           4.0385, -5.8177, -9.1517,  4.8461, -1.5265,  3.9163, -7.7153,\n",
       "          -6.4655,  8.9447,  3.9504, -4.3049,  7.9728,  4.9886,  1.5860,\n",
       "          -0.7936,  8.6183, -0.0974, -1.3187,  8.7433,  6.9381, -8.2158,\n",
       "          -0.3285,  9.7847, -1.3550, -4.5629,  4.0115,  8.9481,  8.1579,\n",
       "          -7.0841,  2.7846, -6.2508, -7.1188, -4.2029, -3.9681,  7.4296,\n",
       "           4.5872, -1.8689,  5.9649, -3.0622, -3.8945, -6.8449,  3.4244,\n",
       "          -0.3625, -3.9747, -2.2595,  1.6042,  4.0253,  0.3973,  3.5570,\n",
       "           8.2691, -4.7411],\n",
       "         [-5.4692,  8.6773,  8.3165,  7.7361, -9.0812, -6.6333, -6.9682,\n",
       "           6.3874, -8.3898, -1.7424,  9.6556, -5.2698, -4.7636,  6.8636,\n",
       "           9.3895, -0.0238,  6.2749, -5.9494,  7.8818, -1.6638,  6.4422,\n",
       "           3.8783,  1.0785,  1.3620,  9.3667,  6.5339, -7.9447, -0.0295,\n",
       "           6.4212, -0.4174, -3.2895,  9.4075, -3.3595, -7.6378,  6.9559,\n",
       "           5.1602, -0.8393, -9.4484, -3.3178, -5.2601, -2.0159, -9.8267,\n",
       "           3.2717,  0.8223,  4.9183, -9.5838, -5.1777,  6.3490,  3.4223,\n",
       "          -3.3344, -7.0886, -5.9183, -5.5672, -5.4233,  6.9217, -8.8332,\n",
       "          -4.6411,  9.1682,  4.0104,  4.4488,  3.4843, -1.7248, -0.6492,\n",
       "           0.4240, -2.2025,  7.0012, -2.3064, -6.1878, -1.9829, -7.6942,\n",
       "           4.4541, -5.4404,  5.1148,  2.6754,  6.8039,  2.9889,  7.5326,\n",
       "          -1.7088,  6.4609,  8.5275,  0.0781,  8.6313,  9.5354,  1.4357,\n",
       "           9.7988,  2.5043, -6.9319,  6.4750,  8.0398, -3.8717, -6.0917,\n",
       "           1.6111, -8.0818,  2.7707, -8.9797,  1.9220,  5.6804,  6.3570,\n",
       "           1.3936, -9.5886],\n",
       "         [ 2.8921, -8.0803, -7.5643, -7.7414, -8.5019,  0.9450,  3.5739,\n",
       "          -7.2317, -1.7579,  7.8846, -9.5600,  0.5358,  3.6008, -8.2181,\n",
       "           9.6056,  4.2498,  3.5511,  0.5531, -4.1063, -6.6438,  6.7092,\n",
       "           1.7180,  8.2275, -0.2710,  5.0112,  4.6757, -5.9475, -0.6932,\n",
       "          -6.8890,  4.7082, -6.8832, -3.3912,  2.1471, -6.4194,  7.9755,\n",
       "           3.0643, -1.1617, -0.9630, -0.5235,  4.5298,  2.9650, -7.9336,\n",
       "           0.0386, -0.1326,  0.5661,  9.9866, -2.2493,  3.4397, -9.2012,\n",
       "           2.7012, -4.7730,  6.8176, -7.7397, -5.8631,  5.6124,  4.0895,\n",
       "          -8.6109,  5.4520,  5.2800, -5.3049,  6.4922, -7.8225,  2.8121,\n",
       "           0.6901, -5.5065,  1.5223,  0.7754, -9.4630, -3.6130, -5.9229,\n",
       "           6.1098,  0.2192, -2.8206,  2.0012,  0.9296, -3.8225,  4.9538,\n",
       "          -4.8152,  7.8443, -3.4171,  2.2422,  8.2783, -8.7675, -4.2562,\n",
       "           3.0327, -1.9394, -1.9922,  7.4849,  7.2506, -3.7682, -2.9068,\n",
       "           0.5190,  2.9956, -0.2339,  0.9373,  1.0497,  2.5496, -6.5427,\n",
       "           4.9100, -6.3315]]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [-9.4138, -4.7425, -2.0950, -5.1947, -7.6718,  7.1785, -6.7183,\n",
       "          7.7111, -8.8909, -2.9229]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [[ 8.5910, -0.3493, -8.1702, -6.7434, -4.6210, -9.2022, -7.0491,\n",
       "           5.5752,  2.4963,  1.5759, -3.5798,  0.2425,  9.9002, -3.9226,\n",
       "          -1.1819, -1.0326, -7.6256,  8.4234, -9.3167, -8.7514,  8.2464,\n",
       "           0.3660, -8.6072,  3.4679,  7.8911,  6.2621, -6.9116,  4.2203,\n",
       "          -3.5743, -1.1976, -2.5913,  4.7702,  9.8614, -6.2242,  2.5194,\n",
       "           6.0792, -2.4077,  7.5921,  0.1194, -6.4820, -0.7657, -5.3797,\n",
       "          -1.7239,  5.0506, -1.6083,  8.1716,  5.7489, -2.2004, -2.1801,\n",
       "           5.6489,  5.8681,  1.4064,  8.7202, -7.0631,  3.5911,  9.7941,\n",
       "          -9.2267, -5.2907,  6.3932, -7.9379,  3.0399,  7.9494, -1.5295,\n",
       "           7.0438, -3.6641,  2.4564, -1.4506,  5.0566, -1.7678, -3.8499,\n",
       "          -8.7334, -4.8144, -9.7076,  8.3585, -0.6323, -9.1895,  0.0718,\n",
       "           3.6889,  0.6734, -4.4115,  6.0857, -7.2974,  0.5300, -9.6336,\n",
       "           3.8235,  8.9022,  8.0619,  4.9762,  8.1521,  6.1037,  1.9285,\n",
       "          -5.5971,  7.4800,  7.7873,  9.9564, -8.5181, -3.2323, -7.5829,\n",
       "          -0.5930,  9.5278],\n",
       "         [-3.5852,  5.1042, -6.2831,  3.5680,  7.2235,  2.9374,  5.9362,\n",
       "          -3.8577,  3.8464,  3.9351,  2.3639,  7.9804, -0.3834,  2.6092,\n",
       "           7.5485, -2.1889, -4.9584,  5.0699, -1.1566,  1.7547, -6.8056,\n",
       "           4.7735,  9.1157, -3.5400,  1.8334, -3.4975,  3.1037, -6.7724,\n",
       "           8.0427, -8.9893,  5.0798, -7.7288,  9.0751,  4.1157,  1.0087,\n",
       "           8.0146, -2.4093, -4.9266,  8.6726,  5.4374, -1.3038,  0.0865,\n",
       "          -7.7154,  0.5168, -2.0832, -7.7283, -9.5133, -2.7759, -1.8920,\n",
       "          -8.3619,  9.3233, -6.1298,  0.1249,  7.6906,  8.8892, -7.6976,\n",
       "          -8.3059, -3.1908, -1.0344,  8.8359, -6.6764, -8.1729,  9.9264,\n",
       "           6.1464, -2.2574, -2.0825, -6.0888, -1.7681,  1.9802,  1.9546,\n",
       "           8.7913,  6.8466,  0.8436,  1.0976, -8.9619,  3.0016,  7.7175,\n",
       "          -3.0617, -7.6119,  0.2810,  1.8232,  4.3149, -5.7773, -7.5510,\n",
       "          -3.5797, -1.3447,  7.8257, -7.2800, -1.0078,  8.4846,  3.9370,\n",
       "          -4.1147, -1.1865, -3.0563, -9.2196,  0.7722,  6.4367, -8.4071,\n",
       "          -1.9785,  9.2881],\n",
       "         [ 0.0448, -8.6804, -9.9348,  2.4565, -6.5628,  6.7891,  9.8543,\n",
       "          -0.1909, -0.8633,  1.2066, -3.5989, -3.8091, -0.1233, -9.8635,\n",
       "          -2.9200, -6.5250, -7.5781,  7.0102, -8.6494, -8.0465, -2.5551,\n",
       "          -6.4356, -0.9446,  9.3307,  7.0144,  3.2742, -6.4269,  7.1344,\n",
       "           3.8553,  3.9715,  7.8570,  4.7722, -6.7432,  5.7826,  9.9788,\n",
       "          -9.2765, -9.3916,  3.3498, -5.7309, -7.3180,  0.6814,  2.2794,\n",
       "           7.9835, -9.4810, -0.7339, -5.4579,  0.1218, -4.4985, -1.6739,\n",
       "           5.6554,  7.0132, -8.4878, -5.9566,  7.8720,  4.4226, -7.3631,\n",
       "           1.4482, -2.3942,  7.2509,  5.7345, -6.7743,  2.5556, -5.5947,\n",
       "          -3.2150,  5.8337, -0.8868, -3.1786,  7.4235,  2.6388, -9.8090,\n",
       "          -0.4156, -4.3136,  9.2169, -2.5608, -3.6751, -8.7879, -2.6719,\n",
       "           8.7508, -1.2345,  2.3664, -6.4017,  8.0885, -6.0150, -2.9756,\n",
       "          -3.3844, -6.6538, -5.9956, -4.5174, -7.1963, -2.3097, -8.0768,\n",
       "           0.4641,  9.6499,  4.9551, -8.0058, -6.2418, -2.3167, -9.8355,\n",
       "          -2.2320,  5.6382],\n",
       "         [ 7.3257, -5.4597,  2.2640, -4.0619,  0.4486,  6.3143,  0.3730,\n",
       "          -2.0501, -3.8449, -1.9461, -2.2206,  7.0757,  2.6394,  2.8554,\n",
       "          -5.9404, -5.0036,  4.5684,  6.0358, -5.2870,  2.4103, -7.6615,\n",
       "           9.2071,  2.2660, -1.7551, -5.7093, -7.1817,  0.6994, -7.4230,\n",
       "          -1.6856,  6.0399, -0.4195, -0.9969, -6.5201, -9.0839, -1.2759,\n",
       "          -9.8930,  1.8197, -4.9268,  8.4883,  6.1499,  7.5980,  6.7898,\n",
       "          -0.4328, -8.1357,  1.3907, -2.3781,  9.0265,  1.0125,  1.1750,\n",
       "          -5.1846, -9.0320,  8.3366,  2.8289,  3.1204,  2.4472, -2.2460,\n",
       "          -3.4691,  1.7129,  6.1294, -1.5841, -0.1653, -9.5806,  4.1252,\n",
       "           4.9746, -8.7028,  7.6599,  4.7105, -0.1351, -4.1015, -3.6010,\n",
       "           1.2043,  6.5882,  5.9870,  4.7623,  8.7609,  3.4635,  1.5330,\n",
       "           6.8317, -2.4850,  2.0988,  0.2291,  2.4592,  3.5871,  7.0639,\n",
       "          -4.0828, -6.2081, -0.3765, -9.6246, -6.9698, -3.9277, -9.7207,\n",
       "           3.6079,  4.1889, -5.6142, -0.6558,  6.7447,  0.6543, -0.5092,\n",
       "          -6.4303, -2.5886],\n",
       "         [-1.9354,  5.9173,  8.5122,  6.0118, -3.4881, -7.6767, -6.4927,\n",
       "          -4.4321, -5.6930,  0.9491,  8.5815,  1.0178,  6.8448, -8.9292,\n",
       "          -3.4978,  1.4722, -9.8845, -4.6518,  7.6632, -5.1000, -5.6335,\n",
       "          -6.3456, -7.1846, -7.8194,  5.1292,  8.7216,  4.9931, -8.7190,\n",
       "           5.6545, -0.6120, -7.7898, -5.0588,  5.8982, -0.9861,  6.2428,\n",
       "          -8.3239, -0.5211, -8.4339, -5.9250, -1.4393, -8.1559,  3.1098,\n",
       "           0.9530, -1.0277,  2.4883,  3.4270,  3.2104,  7.6169, -8.5041,\n",
       "           8.1117,  5.2037, -5.4143, -6.0405, -9.2618, -8.7643,  8.4757,\n",
       "           2.9329, -0.4600,  4.4310,  1.8176, -7.9775,  2.2440, -8.0772,\n",
       "           4.1380,  3.5229, -1.8375, -7.4154,  4.9269, -7.9085, -5.3238,\n",
       "          -4.2293,  3.2036,  2.8136,  5.4485, -8.4648,  2.3510, -3.1553,\n",
       "           2.3276,  1.4430, -2.5642,  7.6467,  5.1820, -6.3166, -0.7412,\n",
       "           5.9097,  7.5765, -0.7356, -7.3679, -8.7838,  1.2418, -1.7310,\n",
       "           9.7757,  5.6251, -7.8254,  0.3298, -9.4991,  9.9239,  7.4565,\n",
       "          -1.2608,  7.1404],\n",
       "         [ 1.2131, -0.7930, -4.4418,  4.3857, -7.8664,  4.2596, -1.6939,\n",
       "           2.4083, -9.9824,  7.2242, -4.6474, -2.0946, -0.8551,  6.4767,\n",
       "           7.4691, -0.2699,  7.2895,  0.1614, -1.0602,  1.1616, -5.7327,\n",
       "           1.2411,  5.3567, -1.6356, -0.4226,  4.2951,  9.7535,  8.3052,\n",
       "          -8.5449,  7.7049,  5.9607, -3.5662,  1.2360,  2.0798, -5.6848,\n",
       "          -2.7506, -0.3820, -5.9293,  3.5376, -0.7021,  4.4119, -4.2006,\n",
       "          -1.2025,  0.2333,  8.0999, -6.2929, -8.5265, -9.5615, -5.7417,\n",
       "           0.8297, -9.3709, -5.3323, -5.9460, -5.2897, -9.9879, -2.5538,\n",
       "           2.8489,  1.3289, -0.1994, -1.0868, -2.1043,  6.3118,  0.0408,\n",
       "          -2.1080,  2.1013, -8.0954,  4.9739,  3.1443,  4.0207,  3.9284,\n",
       "          -0.3989, -3.2856,  3.5672, -2.8982, -8.4546,  9.1366, -3.6712,\n",
       "          -5.4440,  0.6506, -9.0870, -6.8784,  6.7580,  7.2890,  9.6692,\n",
       "           4.9677, -4.7106, -1.8040,  9.7326,  8.4932, -4.4774, -9.3213,\n",
       "           5.3492,  9.6336, -5.3791,  2.8390,  4.7013,  5.8422, -1.0096,\n",
       "           5.8020, -6.0586]]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [-3.0753, -2.9350, -4.2926,  6.6621,  7.0794, -2.4320]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [[ 4.1489, -6.4625,  7.9306, -8.9824, -5.0025, -8.4744, -9.8698,\n",
       "           7.5990, -4.7203,  6.1489, -6.6464,  0.0780, -3.3915, -0.9093,\n",
       "          -1.6831,  4.1127, -7.4381,  0.5833,  9.8620,  3.0408, -1.7288,\n",
       "           1.7153, -3.9662, -1.2375, -5.3935,  4.7789,  1.2867,  1.2366,\n",
       "           1.2435, -8.2183, -5.5650,  2.8270,  3.9842, -8.4316,  1.9089,\n",
       "           3.8735,  3.5310, -3.6518,  2.9698, -4.8820, -7.9946, -1.3970,\n",
       "          -0.3265, -8.5396, -7.7783, -5.3755,  6.4881, -4.1536,  9.4368,\n",
       "          -1.2963, -3.6494, -7.4121, -6.9114, -7.8761, -8.9003, -9.4485,\n",
       "          -8.2572,  1.6927, -0.8499,  8.3771,  4.6278,  5.8012, -3.0531,\n",
       "          -6.7755, -9.5368, -9.3782,  0.7165,  0.0973,  7.4527, -7.5632,\n",
       "           3.7591, -3.5625, -3.6118, -9.8421,  9.0976, -8.1208, -2.0333,\n",
       "          -1.1833, -7.8970, -7.8847,  2.7573,  1.6355, -9.5559,  9.1657,\n",
       "          -2.5087,  0.5314, -2.8676, -1.3416, -7.5553, -9.0256,  2.5795,\n",
       "           4.4340,  3.2719, -1.3112, -9.0957, -6.2398, -7.1134, -4.5032,\n",
       "           6.4586,  9.8672],\n",
       "         [-1.7161, -9.0037,  3.3201, -6.9357,  2.8161,  7.1545, -2.7308,\n",
       "          -5.0923,  4.9055,  0.4446, -7.7981, -8.7613,  7.6138, -4.1767,\n",
       "           7.5649, -4.1402,  1.5528, -0.0977,  2.7550, -7.0550,  7.4794,\n",
       "          -1.9514, -9.4470,  1.5112,  0.1433, -6.1897, -5.6574,  4.3679,\n",
       "           4.5292,  8.3762,  3.9852,  1.8927,  6.8134, -2.5628,  2.4686,\n",
       "           0.0826, -8.2481,  2.4874, -1.5943,  2.9893, -9.4513, -5.6858,\n",
       "           5.2875, -2.8737,  7.4364,  7.8571, -6.7314,  4.2036,  0.4856,\n",
       "           0.9323,  2.0555,  3.1464,  1.2764, -3.7323, -8.1342,  3.4049,\n",
       "           1.5731, -6.7142,  0.9057,  9.2118, -5.3334, -8.2003, -5.6015,\n",
       "           0.4321,  9.8871,  3.8470, -8.0338,  0.2188, -8.0213,  0.9534,\n",
       "           6.2094,  8.3991, -2.0949, -3.6709, -9.5764, -2.4224, -2.3247,\n",
       "          -0.5274, -0.1488,  7.1616,  9.5417, -9.5194,  1.3296, -6.5476,\n",
       "          -4.5986, -0.0961,  2.5258, -5.3026,  8.2625, -3.2272,  8.6059,\n",
       "           7.5273,  8.0623,  9.5602,  6.2680, -3.9774,  4.5954, -6.1907,\n",
       "           9.1503,  5.3428],\n",
       "         [ 7.6370,  4.4123,  3.1989,  0.2763, -1.2935,  8.8047, -1.5154,\n",
       "           0.0660, -5.0959, -0.7730, -7.9380,  4.0380, -3.6320,  4.5212,\n",
       "          -9.1014, -4.1343, -7.0475, -6.8886,  5.5016,  5.6599,  0.4750,\n",
       "          -4.9439,  4.9930, -7.7034, -5.6454,  1.8580, -7.1034,  7.1318,\n",
       "           2.0911,  9.9752,  5.5668,  1.8246, -2.1932,  8.6957, -0.5829,\n",
       "           5.2743,  8.3842,  3.4252, -0.8650,  1.0475, -4.0754,  5.3322,\n",
       "          -9.6148, -6.5814,  6.4940, -7.8442, -5.7802,  9.9369,  4.2686,\n",
       "           6.4986, -1.9808, -6.0330, -6.1664,  4.4172,  5.5227,  3.8905,\n",
       "           7.7700, -8.5973, -0.8160,  6.0499, -2.4686,  0.3941,  7.2898,\n",
       "           9.6528, -4.0293,  2.7636,  9.1519,  6.6680, -1.4392,  9.7370,\n",
       "          -0.9384, -4.6877,  8.3049,  0.3907, -7.1631, -1.4260,  6.7463,\n",
       "           3.6472,  6.8965,  1.3846,  0.2393,  9.9446, -5.2630,  8.8794,\n",
       "           4.0799, -2.4886, -8.1994,  9.3232,  4.0295, -3.0087,  4.3387,\n",
       "          -1.5178,  9.6476,  9.1388,  3.4536,  7.4258,  7.8008,  5.5728,\n",
       "          -4.0263,  9.2275],\n",
       "         [-4.6537,  5.4480,  1.5566,  5.3032,  1.3038, -7.2796, -2.1419,\n",
       "           2.6392,  5.4759,  5.9493,  0.5561, -1.3398, -4.7308, -0.5939,\n",
       "          -6.9795, -3.7070,  6.0706,  1.1646,  8.2924, -9.9369, -7.1220,\n",
       "          -4.0480,  8.6912,  2.6188, -9.0994,  3.5397, -6.1860, -1.5407,\n",
       "           9.2083, -9.3447, -8.7494, -4.9428,  9.6582, -1.7518, -0.5258,\n",
       "          -1.2019,  6.6521, -6.1900,  2.7410, -3.9562, -0.7411, -6.2728,\n",
       "           4.7683, -8.4951, -8.5419, -9.3887, -9.2473,  0.1498, -5.1351,\n",
       "           1.9868,  5.7726,  4.0968, -3.9483,  1.8342, -7.3632, -0.7133,\n",
       "           3.0794,  3.4057,  3.7363, -9.8333, -4.2253, -5.7229,  4.9721,\n",
       "           9.7793, -4.6237,  5.9758,  8.1010, -2.9178, -4.0529,  4.7605,\n",
       "          -6.0976,  9.0418, -9.8341, -0.2922,  3.9650, -4.6731, -1.6343,\n",
       "           7.4208, -8.2280, -3.2951, -4.3743, -2.7321,  2.3243,  9.1272,\n",
       "           5.7970,  6.4156, -5.6514,  4.5095,  8.6775, -6.6975, -6.0654,\n",
       "           5.0413,  1.0469, -4.3566, -1.9637, -0.2341, -2.1392, -0.1194,\n",
       "          -3.9042, -4.6081],\n",
       "         [-3.4608, -7.2939, -4.1794,  7.2650,  1.2517,  5.5819,  6.0382,\n",
       "          -8.2138,  6.8363,  7.7959, -5.3324, -4.9654,  2.7826, -1.0274,\n",
       "          -7.5927,  8.7139, -3.2579,  0.4818, -7.6691,  1.8600, -8.1081,\n",
       "          -0.4533,  3.0091,  7.8169,  1.9523, -4.9699,  0.0682, -0.1188,\n",
       "           5.6748,  5.6273, -2.7463,  6.4515,  2.9729,  2.9590, -6.2876,\n",
       "           8.2950, -6.8499, -1.1325, -1.1106,  8.3304, -9.7003, -0.7569,\n",
       "           1.0656,  5.3522, -3.1122,  8.9664, -8.7884, -6.2229,  4.8601,\n",
       "          -5.0615, -9.5522,  5.7574, -5.6713,  8.8050,  3.7818,  6.9570,\n",
       "           2.0732, -7.7032,  7.5865,  2.4813, -3.2597, -1.9386,  8.7784,\n",
       "           5.5792, -6.8388,  4.6842,  8.9480,  8.1701, -8.0086, -5.8886,\n",
       "          -7.6663, -4.1901,  6.9589,  1.9652,  2.9256, -1.2886,  2.6933,\n",
       "           6.7019, -4.7838,  9.0416,  9.4841, -6.3255, -2.8275,  0.1901,\n",
       "          -5.2215, -5.4058, -0.3994, -5.6555,  7.3206,  8.4883, -7.1893,\n",
       "          -5.5230, -5.4762,  7.9210, -8.2008, -8.2980,  5.7753, -1.0347,\n",
       "          -9.4905,  0.4126],\n",
       "         [ 7.1745, -6.5502, -7.2763,  8.6016,  3.7470, -4.2041,  7.3007,\n",
       "          -9.2243, -2.5590, -3.7352,  5.0127,  3.1033,  5.9176, -7.0789,\n",
       "           1.7352, -7.0933, -7.2771, -0.6210,  1.3263,  0.3585, -5.3459,\n",
       "          -5.3554, -0.1578,  7.1040,  9.9904,  0.3451, -1.7571,  4.2380,\n",
       "          -7.9263,  7.3958,  8.3340, -9.4373, -3.4298, -6.0518,  0.1687,\n",
       "          -1.1010,  1.3057,  2.7561, -4.7515, -8.6528,  7.7906,  9.0066,\n",
       "           3.7073, -9.4214, -2.1188, -7.7781, -2.5012, -8.3919, -4.9799,\n",
       "           1.1566,  9.2647,  7.2919,  1.1063, -4.1343,  6.2217, -8.9748,\n",
       "           1.4201,  9.5764, -7.5486,  0.1222,  1.7272,  9.6817, -1.9710,\n",
       "          -9.5798, -2.8724,  4.2634, -8.5555,  5.4746, -0.2522,  2.2620,\n",
       "           9.9202,  7.3004, -2.7563,  4.8516, -9.0608,  8.4778,  7.7646,\n",
       "           9.8969,  9.6961,  2.3280,  3.5507,  7.6010,  2.9105,  2.0411,\n",
       "           8.8364,  4.6969,  5.7912,  7.8275,  9.1672, -4.9048, -6.2382,\n",
       "          -8.1284,  8.4574, -2.4781, -4.7533,  7.0873, -1.0514,  8.1382,\n",
       "           4.6985, -5.0830]]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [ 4.6765, -4.8858,  7.8500, -9.1679,  0.1960, -2.0788]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [[ 8.1328, -8.8358, -7.9964, -4.9974, -6.8977, -9.7922,  5.1118,\n",
       "          -5.7106, -7.5031, -5.5378, -1.7368, -3.0997,  1.8495, -8.3968,\n",
       "          -8.6742,  8.5604, -9.1470,  5.7399, -6.2738, -4.1350,  5.6406,\n",
       "          -8.7188,  6.6649, -2.5916,  0.6077, -5.9474, -0.0017, -1.2836,\n",
       "           0.5958,  4.9058,  4.7312,  6.2222, -8.5552, -8.9553, -8.3707,\n",
       "           1.1507, -1.9208,  6.4890,  7.8175, -5.3458,  0.0456, -5.4017,\n",
       "           1.9543, -8.7378, -2.2498,  2.6569, -9.2880,  4.6726,  9.8723,\n",
       "           9.6539, -6.9466, -4.5416,  5.8136,  7.3443, -9.9700,  8.4017,\n",
       "          -6.7347, -8.0398, -2.2588, -3.5887,  1.0355,  5.3023,  6.3968,\n",
       "           3.6795, -2.5227,  9.8308,  4.7447, -7.2476, -3.8840,  7.7556,\n",
       "           4.0291,  3.6607,  8.2638, -5.3668,  5.2236,  4.7042, -2.6844,\n",
       "          -1.1509, -8.4213, -8.9712, -6.1280, -6.5090, -6.7582, -9.9977,\n",
       "           7.7991,  6.3093, -2.7283,  9.7564,  1.5235, -0.4822,  2.1235,\n",
       "           4.2506,  0.0446,  0.9123, -4.3795, -0.9055, -1.2925,  3.5310,\n",
       "           0.0916, -6.2586],\n",
       "         [ 4.0077,  4.6927, -9.3208,  0.5134, -4.6206,  4.8587, -5.9070,\n",
       "          -7.1337, -0.9074,  4.6671, -3.7423, -5.6487,  5.5351,  1.1818,\n",
       "          -7.0563, -3.0359,  3.7232, -7.7746,  8.2511, -5.1193,  9.5831,\n",
       "          -7.1134, -7.7673,  8.2017,  8.2349,  0.5800, -5.3436,  5.3472,\n",
       "           2.1479,  2.8941, -3.7511, -9.2200,  4.8396,  5.6265,  1.7698,\n",
       "          -6.3270,  0.4589, -3.8141,  1.3597, -0.3784,  3.6905, -5.2105,\n",
       "           8.5322, -6.9199,  6.7711, -2.0743, -1.9344,  6.5905, -4.8016,\n",
       "          -9.4818,  4.8214,  3.9845, -5.5595,  4.3507,  0.1084,  3.9876,\n",
       "           0.6918,  6.0382, -7.5666,  8.5853, -7.2284, -4.8196,  2.9818,\n",
       "           7.8263, -4.1926, -4.9203, -0.6211,  9.5589,  3.7380, -4.5391,\n",
       "           7.6401,  4.2420,  0.1400,  9.6105,  7.0523, -1.9974,  6.8536,\n",
       "          -7.8815,  6.2521,  1.0441, -5.7337, -4.2181,  3.0732,  8.0702,\n",
       "          -8.0244, -9.7793, -4.5927,  0.5102,  5.2139,  2.1119, -9.0979,\n",
       "          -1.0606,  0.1156,  3.0870, -3.0423,  3.7571, -8.8903,  2.4032,\n",
       "          -5.6897, -1.1204],\n",
       "         [ 0.2066,  9.2781, -5.2548,  9.8500, -2.4939,  2.5776, -8.8166,\n",
       "           6.9321,  3.1895, -0.2952,  7.0926,  8.8738, -2.6788, -0.9778,\n",
       "           7.3676,  7.9605,  0.6270,  9.3981, -5.3566,  2.6735,  5.6261,\n",
       "          -8.9762,  5.5161, -8.7261,  6.5660, -7.6018, -6.2432,  0.7619,\n",
       "          -9.4718, -3.2742,  4.9713,  4.4560,  2.1282, -0.4654, -6.5107,\n",
       "           4.8510, -4.2362,  6.2419,  3.8978, -3.8727,  7.1647,  3.0643,\n",
       "           3.4081, -2.9158,  1.5812, -9.8665,  0.4980, -0.7666,  4.4978,\n",
       "           0.2546,  0.8250,  6.6165,  2.8542,  2.0985,  8.2609,  1.6870,\n",
       "          -6.1162, -1.5026, -8.8091, -3.6284, -6.1677,  5.7042,  0.8020,\n",
       "           1.7711, -1.1306,  4.1369, -8.0290, -5.5059, -5.3460,  6.3427,\n",
       "           7.4518, -2.8497,  8.7963,  7.0273, -3.8968,  8.1561, -5.8256,\n",
       "          -9.9630,  5.5674, -1.4896, -7.6995, -6.9240,  4.0912,  5.9780,\n",
       "           7.5644, -4.6468,  5.8083,  1.9494, -9.0159,  1.5938, -3.2140,\n",
       "          -1.4585,  8.3327, -5.9403, -7.7935, -7.6156,  7.8171, -7.9955,\n",
       "           9.2700,  8.9721],\n",
       "         [-0.5292, -0.8193, -9.3087, -3.3754, -3.1911,  1.1320,  0.4787,\n",
       "           2.4052, -8.6769,  9.1936, -8.3025, -5.1792, -9.4950,  9.4880,\n",
       "           6.6036,  6.3209,  5.7007,  6.6637,  0.3759, -2.9125, -6.3942,\n",
       "          -7.4817,  3.3101, -6.3363,  6.0816, -2.8573, -6.5642, -4.1732,\n",
       "           8.0713, -1.2521, -4.1226, -0.6519,  4.2794,  9.6295,  2.8593,\n",
       "           0.8924,  5.2002, -3.9533,  5.6792,  4.8745,  2.4648, -7.7593,\n",
       "          -3.5193,  2.8410, -0.9493,  3.7112, -6.4141,  5.4626,  5.3155,\n",
       "           4.2873, -9.5863, -5.9937,  6.6713, -2.8713,  9.2544, -4.4088,\n",
       "          -2.9766, -8.3306, -9.9645,  9.9183,  5.9452, -9.7514,  6.9486,\n",
       "           8.6570, -2.2340, -7.4900, -3.2775, -3.6958,  2.8059, -4.1636,\n",
       "          -2.8805,  3.3270,  7.8977,  7.9342, -2.1194, -3.6196,  7.4484,\n",
       "           3.4913,  2.3402, -8.2225, -2.8443,  3.6211, -0.9395,  2.4997,\n",
       "          -9.9290, -6.9711, -7.7483, -3.8645, -6.1101,  6.1309, -6.7749,\n",
       "           4.2731, -2.8803, -5.6978, -3.3392, -3.7190, -7.5243,  3.5473,\n",
       "           4.3801, -9.3635],\n",
       "         [-3.4940, -2.7282,  6.2454, -6.5729, -3.7623, -8.6434,  2.0138,\n",
       "           8.2938, -7.0975,  7.7327, -8.2709,  6.0812, -0.1952,  7.7432,\n",
       "          -4.2944,  0.4177,  1.7689, -5.8186,  6.6680, -3.3303, -3.6552,\n",
       "          -7.8265, -4.6180,  6.0985,  6.5537, -3.4356, -1.6693, -8.5932,\n",
       "          -2.8280,  3.3464,  1.8361, -9.2861,  2.3970, -6.1297, -0.2805,\n",
       "          -7.8755, -9.0630,  4.9183,  6.3885, -8.8517, -7.3730,  1.9803,\n",
       "          -4.4473,  2.7429, -4.8263, -3.0140,  9.6134, -2.9135,  8.9704,\n",
       "           6.8045, -6.7199,  3.1043, -4.0059,  2.8779, -4.4662, -8.3050,\n",
       "           1.3867,  0.5777, -0.4233,  3.2044,  6.1485,  2.3062,  9.0639,\n",
       "           7.3874,  1.3091, -2.4806, -6.1181, -2.9246, -0.6264,  1.3175,\n",
       "           2.9832, -7.5098, -9.5133,  5.7530,  0.0440,  0.4473,  4.8117,\n",
       "           4.1007,  3.0387,  4.6373,  2.6503,  8.2920,  3.0002, -6.6406,\n",
       "          -3.0414,  2.5443,  7.4586,  8.4277, -5.5671, -5.5915,  0.5911,\n",
       "          -5.6839, -0.5855,  6.1833,  8.4472, -5.9633, -5.5776, -7.2939,\n",
       "          -0.7831,  9.3911],\n",
       "         [ 2.9945, -2.6311, -1.6615,  3.0147,  6.8717, -2.9540,  5.4020,\n",
       "          -3.6317, -2.8521,  0.5412, -8.2962, -9.5017, -5.3751, -8.2732,\n",
       "          -6.6894, -4.0622, -5.2526, -5.4416, -4.6921,  1.0264, -0.8049,\n",
       "           2.1768, -6.9019,  3.6636,  6.2339,  1.3718, -9.8905,  2.9185,\n",
       "          -2.0254,  2.9672, -0.4451, -7.7883, -9.6698, -7.8374,  5.1713,\n",
       "          -9.8794, -5.7340, -7.9402, -2.0268,  3.8815, -3.9234,  8.6151,\n",
       "          -4.2584,  0.9031, -3.7342,  7.4716,  1.8462,  2.7431,  4.2408,\n",
       "          -5.5873,  2.7683, -2.3232, -6.1992,  2.8482,  8.8441, -6.4004,\n",
       "          -5.3907,  5.9401,  6.8816,  1.1022, -8.2241, -9.8275,  7.5508,\n",
       "           2.8368, -4.5281, -3.2412,  6.6204,  0.7404,  3.7146, -9.0319,\n",
       "           3.0351,  8.1131,  0.9731, -1.2471, -2.8031, -3.8047, -6.7963,\n",
       "          -0.1058, -3.8480, -7.5843,  3.8432,  2.7059, -1.8890, -6.2988,\n",
       "           7.3864,  8.4278, -8.9999,  1.3726,  8.5616, -0.7001, -0.6852,\n",
       "          -0.7018,  5.9959, -7.0773,  2.9842,  1.5346, -1.7239, -4.7751,\n",
       "          -8.7610, -5.1272]]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [-6.2495, -6.8857,  7.8221, -6.7409,  2.7063, -7.7782])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Agents[1].parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIjkU1IQEZF8SgoiIpLv/wFlWvKDCO8y5QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e964e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_list = np.arange(len(r_list[0]))*ep_record\n",
    "plt.plot(epoch_list,r_list[0], label='Agent 1')\n",
    "plt.plot(epoch_list,r_list[1], label='Agent 2')\n",
    "plt.ylabel('Average reward in epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim([0,1])\n",
    "plt.legend()\n",
    "#plt.savefig('N_ep='+str(N_ep)+'_seed='+str(num_seed)+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dummy inputs\n",
    "x = torch.randint(0,max_item,[1,6]).long()\n",
    "y = torch.randint(0,num_vocab,[1,6]).long()\n",
    "z = torch.randint(0,max_item,[1,3]).long()\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    z = z.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0]),\n",
       " tensor([[ 1,  7,  5,  5,  9,  2]]),\n",
       " tensor([[ 3,  2,  2]]),\n",
       " tensor(1.00000e-02 *\n",
       "        6.7275),\n",
       " tensor([[-0.6393]]),\n",
       " tensor([[-13.1120]]),\n",
       " tensor([[-1.9442]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[0] = 4\n",
    "Agents[1]([x,y,z],True,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
