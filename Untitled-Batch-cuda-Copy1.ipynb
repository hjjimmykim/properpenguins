{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "All the logarithms used are base 2. <br>\n",
    "Assumes 2 self-interested agents alternating turns. <br>\n",
    "Baseline (1 for each agent) gets updated after each episode ends (see corpses). <br>\n",
    "Rewards only possible at the end of each game. <br>\n",
    "Uses same (numerical) encoder for both item context and proposal. Reference code uses 3 distinct ones. It also has max_utility = num_types instead of 10 for us.<br>\n",
    "Check how message policy works again; paper seemed to imply that each output of the lstm is a letter. (we take the hidden output and make a probability over letters out of it).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Network\n",
    "import torch\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "# cuda\n",
    "use_cuda = 0\n",
    "\n",
    "# Random seeds for testing\n",
    "num_seed = 0\n",
    "torch.manual_seed(num_seed)\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    torch.cuda.manual_seed(num_seed)\n",
    "np.random.seed(num_seed)\n",
    "\n",
    "# Utility functions\n",
    "from utility import truncated_poisson_sampling, create_item_pool, create_agent_utility, rewards_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Game setup\n",
    "num_agents = 2         # Number of agents playing the game\n",
    "num_types = 3          # Number of item types\n",
    "max_item = 5           # Maximum number of each item in a pool\n",
    "max_utility = 5       # Maximum utility value for agents\n",
    "\n",
    "# Turn sampling\n",
    "lam = 7                # Poisson parameter\n",
    "max_N = 10             # Maximum number of turns\n",
    "min_N = 4              # Minimum number of turns\n",
    "\n",
    "# Linguistic channel\n",
    "num_vocab = 10         # Symbol vocabulary size for linguistic channel\n",
    "len_message = 6        # Linguistic message length\n",
    "\n",
    "# Training\n",
    "alpha = 0.001          # learning rate\n",
    "N_ep = 100              # Number of episodes\n",
    "num_games = 128        # Number of games per episode (batch size)\n",
    "\n",
    "# Appendix\n",
    "lambda1 = 0.05         # Entropy regularizer for pi_term\n",
    "lambda2 = 0.0001        # Entropy regularizer for pi_utt\n",
    "lambda3 = 0.005        # Entropy regularizer for pi_prop\n",
    "smoothing_const = 0.7  # Smoothing constant for the exponential moving average baseline\n",
    "\n",
    "# Miscellaneous\n",
    "ep_time = 100         # Print time every ep_time episodes\n",
    "ep_record = 10        # Record training curve every ep_record episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class combined_policy(nn.Module):\n",
    "    def __init__(self, embedding_dim = 100, batch_size = 128, num_layers = 1, bias = True, batch_first = False, dropout = 0, bidirectional = False):\n",
    "        super(combined_policy, self).__init__()\n",
    "        # Save variables\n",
    "        self.embedding_dim = embedding_dim # Hidden layer dimensions\n",
    "        self.batch_size = batch_size       # Batch size (updated every forward pass)\n",
    "        self.log_p = torch.zeros([batch_size,1])                     # Store policy log likelihood for REINFORCE\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            self.log_p = self.log_p.cuda()\n",
    "        \n",
    "        # Encoding -------------------------------------------------------------\n",
    "        \n",
    "        # Numerical encoder\n",
    "        self.encoder1 = nn.Embedding(max_utility+1, embedding_dim)\n",
    "        # Linguistic encoder\n",
    "        self.encoder2 = nn.Embedding(num_vocab+1, embedding_dim)\n",
    "        \n",
    "        # Item context LSTM\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        # Linguistic LSTM\n",
    "        self.lstm2 = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        # Proposal LSTM\n",
    "        self.lstm3 = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        \n",
    "        # Outputs of the 3 LSTMS get concatenated together\n",
    "        \n",
    "        # Feed-forward\n",
    "        self.ff = nn.Linear(3*embedding_dim, embedding_dim)\n",
    "        \n",
    "        # Output of feed-forward is the input for the policy networks\n",
    "        \n",
    "        # Policy ---------------------------------------------------------------\n",
    "        \n",
    "        # Termination policy\n",
    "        self.policy_term = nn.Linear(embedding_dim, 1)\n",
    "        # Linguistic policy\n",
    "        self.policy_ling = nn.LSTM(embedding_dim, embedding_dim, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "        self.ff_ling = nn.Linear(embedding_dim, num_vocab)\n",
    "        # Proposal policies\n",
    "        self.policy_prop = nn.ModuleList([nn.Linear(embedding_dim, max_item+1) for i in range(num_types)])\n",
    "        \n",
    "    def forward(self, x, test, batch_size=128):\n",
    "        # Inputs --------------------------------------------------------------------\n",
    "        # x = list of three elements consisting of:\n",
    "        #   1. item context (longtensor of shape batch_size x (2*num_types))\n",
    "        #   2. previous linguistic message (longtensor of shape batch_size x len_message)\n",
    "        #   3. previous proposal (longtensor of shape batch_size x num_types)\n",
    "        # test = whether training or testing (testing selects actions greedily)\n",
    "        # batch_size = batch size\n",
    "        # Outputs -------------------------------------------------------------------\n",
    "        # term = binary variable where 1 indicates proposal accepted => game finished (longtensor of shape batch_size x 1)\n",
    "        # message = crafted linguistic message (longtensor of shape batch_size x len_message)\n",
    "        # prop = crafted proposal (longtensor of shape batch_size x num_types)\n",
    "        # entropy_loss = Number containing the sum of policy entropies (should be total entropy by additivity)\n",
    "        \n",
    "        # Update batch_size variable (changes throughout training due to sieving (see survivors below))\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Extract inputs ------------------------------------------------------------\n",
    "        \n",
    "        # Item context\n",
    "        x1 = x[0]\n",
    "        # Previous linguistic message\n",
    "        x2 = x[1]\n",
    "        # Previous proposal\n",
    "        x3 = x[2]  \n",
    "\n",
    "        # Encoding ------------------------------------------------------------------\n",
    "\n",
    "        # Initial embedding\n",
    "        x1 = self.encoder1(x1).transpose(0,1)\n",
    "        x2 = self.encoder2(x2).transpose(0,1)\n",
    "        x3 = self.encoder1(x3).transpose(0,1) # Same encoder as item context\n",
    "        \n",
    "            \n",
    "        # LSTM for item context\n",
    "        h1 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial hidden\n",
    "        c1 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial cell\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            h1 = h1.cuda()\n",
    "            c1 = c1.cuda()\n",
    "\n",
    "        for i in range(x1.size()[0]):\n",
    "            _, (h1,c1) = self.lstm1(x1[i].view(1,self.batch_size,self.embedding_dim),(h1,c1))\n",
    "        x1_encoded = h1\n",
    "        \n",
    "        # LSTM for linguistic\n",
    "        h2 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial hidden\n",
    "        c2 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial cell\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            h2 = h2.cuda()\n",
    "            c2 = c2.cuda()\n",
    "\n",
    "        for i in range(x2.size()[0]):\n",
    "            _, (h2,c2) = self.lstm2(x2[i].view(1,self.batch_size,self.embedding_dim),(h2,c2))\n",
    "        x2_encoded = h2\n",
    "        \n",
    "        # LSTM for proposal\n",
    "        h3 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial hidden\n",
    "        c3 = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial cell\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            h3 = h3.cuda()\n",
    "            c3 = c3.cuda()\n",
    "\n",
    "        for i in range(x3.size()[0]):\n",
    "            _, (h3,c3) = self.lstm2(x3[i].view(1,self.batch_size,self.embedding_dim),(h3,c3))\n",
    "        x3_encoded = h3\n",
    "\n",
    "        # Concatenate side-by-side\n",
    "        h = torch.cat([x1_encoded,x2_encoded,x3_encoded],2).squeeze()\n",
    "\n",
    "        # Feedforward\n",
    "        h = self.ff(h)\n",
    "        h = F.relu(h) # Hidden layer input for policy networks\n",
    "        \n",
    "        # Policy ------------------------------------------------------------------\n",
    "\n",
    "        # Termination -----------------------------------------------\n",
    "        p_term = F.sigmoid(self.policy_term(h)).float()\n",
    "\n",
    "        # Entropy\n",
    "        one_tensor = torch.ones(self.batch_size,1)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            one_tensor = one_tensor.cuda()\n",
    "        entropy_term = -(p_term * (p_term).log()) - ((one_tensor-p_term) * (one_tensor-p_term).log())\n",
    "        entropy_term = torch.sum(entropy_term)\n",
    "        \n",
    "        if test:\n",
    "            # Greedy\n",
    "            term = torch.round(p_term).long()\n",
    "        else:\n",
    "            # Sample\n",
    "            term = torch.bernoulli(p_term).long()\n",
    "            \n",
    "        # log p for REINFORCE\n",
    "        log_p_term = torch.zeros(self.batch_size,1)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            log_p_term = log_p_term.cuda()\n",
    "\n",
    "        log_p_term = ((term.float() * p_term) + ((one_tensor-term.float()) * (one_tensor-p_term))).log()\n",
    "        \n",
    "        # Linguistic construction ----------------------------------\n",
    "        h_ling = h.clone().view(1,self.batch_size,self.embedding_dim) # Initial hidden state\n",
    "        c_ling = torch.zeros(1,self.batch_size,self.embedding_dim) # Initial cell state\n",
    "        letter = torch.zeros(self.batch_size,1).long() # Initial letter (dummy)\n",
    "        entropy_letter = torch.zeros([self.batch_size,len_message])\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            c_ling = c_ling.cuda()\n",
    "            letter = letter.cuda()\n",
    "            entropy_letter = entropy_letter.cuda()\n",
    "        \n",
    "        # log p for REINFORCE \n",
    "        log_p_letter = torch.zeros([self.batch_size,1])\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            log_p_letter = log_p_letter.cuda()\n",
    "\n",
    "        message = torch.zeros(self.batch_size,len_message) # Message\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            message = message.cuda()\n",
    "        for i in range(len_message):\n",
    "            embedded_letter = self.encoder2(letter)\n",
    "\n",
    "            _, (h_ling,c_ling) = self.policy_ling(embedded_letter.view(1,self.batch_size,self.embedding_dim),(h_ling,c_ling))\n",
    "            logit = self.ff_ling(h_ling.view(self.batch_size,self.embedding_dim))\n",
    "            p_letter = F.softmax(logit,dim=1).float()\n",
    "\n",
    "            entropy_letter[:,i] = -torch.sum(p_letter*(p_letter+1e-8).log(),1)\n",
    "\n",
    "            if test:\n",
    "                # Greedy\n",
    "                letter = p_letter.argmax(dim=1).view(self.batch_size,1).long()\n",
    "            else:\n",
    "                # Sample\n",
    "                letter = torch.multinomial(p_letter,1).long()\n",
    "                \n",
    "            # Gather the probabilities for the letters we've picked\n",
    "            probs = torch.gather(p_letter, 1, letter)\n",
    "            log_p_letter = log_p_letter + (probs).log()\n",
    "                \n",
    "            message[:,i] = letter.squeeze()\n",
    "            \n",
    "        message = message.long()\n",
    "        entropy_letter = torch.sum(entropy_letter)     \n",
    "   \n",
    "        # Proposal ----------------------------------------------\n",
    "        p_prop = []\n",
    "        prop = []\n",
    "        \n",
    "        #prop = torch.zeros([self.batch_size,num_types]).long()\n",
    "        entropy_prop_list = [0,0,0]\n",
    "        \n",
    "        # log p for REINFORCE \n",
    "        log_p_prop = torch.zeros([self.batch_size,1])\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            log_p_prop = log_p_prop.cuda()\n",
    "\n",
    "        for i in range(num_types):\n",
    "            p_prop.append(F.sigmoid(self.policy_prop[i](h)))\n",
    "            \n",
    "            entropy_prop_list[i] = -torch.sum(p_prop[i]*p_prop[i].log())\n",
    "            \n",
    "            p_prop[i] = p_prop[i].view(self.batch_size,max_item+1)\n",
    "\n",
    "            if test:\n",
    "                # Greedy\n",
    "                #prop[:,i] = p_prop[i].argmax(dim=1)\n",
    "                prop.append(p_prop[i].argmax(dim=1))\n",
    "            else:\n",
    "                # Sample\n",
    "                #prop[:,i] = torch.multinomial(p_prop,1)\n",
    "                prop.append(torch.multinomial(p_prop,1))\n",
    "                \n",
    "            # Gather the probabilities for the letters we've picked\n",
    "            probs = torch.gather(p_prop[i], 1, prop[i].view(self.batch_size,1))\n",
    "            log_p_prop = log_p_prop + probs.log()\n",
    "              \n",
    "        prop = torch.stack(prop).transpose(0,1)\n",
    "        entropy_prop = sum(entropy_prop_list) # Entropy for exploration\n",
    "\n",
    "        # Combine -----------------------------------------------------------------\n",
    "        entropy_loss = torch.sum(lambda1*entropy_term + lambda3*entropy_prop + lambda2*entropy_letter)\n",
    "        self.log_p = log_p_term + log_p_letter + log_p_prop\n",
    "\n",
    "        return (term,message,prop, entropy_loss, log_p_term,log_p_letter,log_p_prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = combined_policy()\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    net = net.cuda()\n",
    "\n",
    "# Dummy inputs\n",
    "x = torch.randint(0,max_item,[128,6]).long()\n",
    "y = torch.randint(0,num_vocab,[128,6]).long()\n",
    "z = torch.randint(0,max_item,[128,3]).long()\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    z = z.cuda()\n",
    "\n",
    "blah = net([x,y,z],True)\n",
    "\n",
    "# Initialize agents\n",
    "Agents = []\n",
    "for i in range(num_agents):\n",
    "    Agents.append(combined_policy())\n",
    "    if torch.cuda.is_available() and use_cuda:\n",
    "        Agents[i] = Agents[i].cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start ----------------\n",
      "End ------------------\n",
      "Total runtime: 25.26877784729004s\n"
     ]
    }
   ],
   "source": [
    "baselines = [0 for _ in range(num_agents)] # Baselines for reward calculation\n",
    "\n",
    "# Initialize optimizers for learning\n",
    "optimizers = []\n",
    "for i in range(num_agents):\n",
    "    optimizers.append(optim.Adam(Agents[i].parameters()))\n",
    "    \n",
    "# Train rewards\n",
    "r_list = []\n",
    "for i in range(num_agents):\n",
    "    r_list.append([])\n",
    "\n",
    "print('Start ----------------')\n",
    "time_start = time.time()\n",
    "time_p1 = time.time()\n",
    "# Loop over episodes\n",
    "for i_ep in range(N_ep):\n",
    "    # Setting up games -----------------------------------------------------------------------\n",
    "    # Game setup\n",
    "    # Truncated Poisson sampling for number of turns in each game\n",
    "    N = truncated_poisson_sampling(lam, min_N, max_N, num_games)\n",
    "    # Item pools for each game\n",
    "    pool = create_item_pool(num_types, max_item, num_games)\n",
    "    if torch.cuda.is_available() and use_cuda:\n",
    "        N = N.cuda()\n",
    "        pool = pool.cuda()\n",
    "    # Item contexts for each game\n",
    "    item_contexts = [] # Each agent has different utilities (but same pool)\n",
    "    for i in range(num_agents):\n",
    "        utility = create_agent_utility(num_types, max_utility, num_games)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            utility = utility.cuda()\n",
    "        item_contexts.append(torch.cat([pool, utility],1))\n",
    "    \n",
    "    # Initializations\n",
    "    survivors = torch.ones(num_games).nonzero()               # Keeps track of ongoing games; everyone alive initially\n",
    "    num_alive = len(survivors)                                # Actual batch size for each turn (initially num_games)\n",
    "    prev_messages = torch.zeros(num_games, len_message).long() # Previous linguistic message for each game\n",
    "    prev_proposals = torch.zeros(num_games, num_types).long()  # Previous proposal for each game\n",
    "    if torch.cuda.is_available() and use_cuda:\n",
    "        survivors = survivors.cuda()\n",
    "        prev_messages = prev_messages.cuda()\n",
    "        prev_proposals = prev_proposals.cuda()\n",
    "    \n",
    "    rewards = [torch.zeros(num_games), torch.zeros(num_games)]       # Rewards for each game for each agent\n",
    "    # Keep track of sum of all rewards (from all games in a batch) for baseline updates (see corpses below)\n",
    "    reward_sums = []\n",
    "    for i in range(num_agents):\n",
    "        reward_sums.append(torch.zeros(1)) # Just a number\n",
    "        Agents[i].log_p = torch.zeros([num_games,1])\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            rewards[i] = rewards[i].cuda()\n",
    "            reward_sums[i] = reward_sums[i].cuda()\n",
    "            Agents[i].log_p = Agents[i].log_p.cuda()\n",
    "\n",
    "    # Play the games -------------------------------------------------------------------------\n",
    "    for i_turn in range(max_N): # Loop through maximum possible number of turns for all games\n",
    "        \n",
    "        # Losses for each agent\n",
    "        reward_losses = [torch.zeros([],requires_grad=True), torch.zeros([],requires_grad=True)]  \n",
    "        entropy_losses = [torch.zeros([],requires_grad=True), torch.zeros([],requires_grad=True)] # Exploration\n",
    "        for j in range(num_agents):\n",
    "            Agents[j].log_p = torch.zeros([num_alive,1])\n",
    "            \n",
    "            if torch.cuda.is_available() and use_cuda:\n",
    "                reward_losses[j] = reward_losses[j].cuda()\n",
    "                entropy_losses[j] = entropy_losses[j].cuda()\n",
    "                Agents[j].log_p = Agents[j].log_p.cuda()\n",
    "        \n",
    "        # Agent IDs\n",
    "        id_1 = i_turn % 2    # Current player\n",
    "        id_2 = int(not id_1) # Other player\n",
    "        \n",
    "        # Remove finished games (batch size decreases)\n",
    "        N = N[survivors].view(num_alive, 1)\n",
    "        pool = pool[survivors].view(num_alive, num_types)\n",
    "        prev_messages = prev_messages[survivors].view(num_alive, len_message)\n",
    "        prev_proposals = prev_proposals[survivors].view(num_alive, num_types)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            N = N.cuda()\n",
    "            pool = pool.cuda()\n",
    "            prev_messages = prev_messages.cuda()\n",
    "            prev_proposals = prev_proposals.cuda()\n",
    "        # Quantities different for each agent\n",
    "        for j in range(num_agents):\n",
    "            item_contexts[j] = item_contexts[j][survivors].view(num_alive,num_types*2)\n",
    "            #rewards[j] = rewards[j][survivors].view(num_alive)\n",
    "            #reward_losses[j] = reward_losses[j][survivors].view(num_alive)\n",
    "            if torch.cuda.is_available() and use_cuda:\n",
    "                item_contexts[j] = item_contexts[j].cuda()\n",
    "        \n",
    "        # Agent currently playing\n",
    "        Agent = Agents[id_1]             \n",
    "        item_context = item_contexts[id_1]\n",
    "        \n",
    "        # Play the game -------------------------------------------------------------\n",
    "        term, prev_messages, proposals, entropy_loss, lt,ll,lp = Agent([item_context, prev_messages, prev_proposals], True, num_alive)\n",
    "        entropy_losses[id_1] = entropy_loss\n",
    "        \n",
    "        # Compute reward loss (assumes 2 agents) ------------------------------------\n",
    "        # Games terminated by the current agent (previous proposal accepted)\n",
    "        \n",
    "        finishers = term.squeeze().nonzero()          # squeeze is for getting rid of extra useless dimension that pops up for some reason\n",
    "        num_finishers = len(finishers)\n",
    "\n",
    "        if len(finishers) != 0:\n",
    "            pool_12 = pool[finishers].view(num_finishers,num_types)\n",
    "            \n",
    "            share_2 = prev_proposals[finishers].view(num_finishers,num_types) # Share of other (previous proposal) \n",
    "            share_1 = pool_12 - share_2 # Share of this agent (remainder)\n",
    "            \n",
    "            # Zero reward if proposal exceeds pool\n",
    "            invalid_batches = torch.sum(share_2>pool_12,1)>0\n",
    "            share_2[invalid_batches] = 0\n",
    "            share_1[invalid_batches] = 0\n",
    "            \n",
    "            utility_1 = item_contexts[id_1][:,num_types:] # Recall that item context is a concatenation of pool and utility\n",
    "            utility_1 = utility_1[finishers].view(num_finishers,num_types)\n",
    "            utility_2 = item_contexts[id_2][:,num_types:]\n",
    "            utility_2 = utility_2[finishers].view(num_finishers,num_types)\n",
    "\n",
    "            log_p_1 = Agents[id_1].log_p[finishers].view(num_finishers,1)\n",
    "            log_p_2 = Agents[id_2].log_p[finishers].view(num_finishers,1)\n",
    "\n",
    "            # Calculate reward and reward losses\n",
    "            r1, rl1 = rewards_func(share_1, utility_1, pool_12, log_p_1, baselines[id_1])\n",
    "            r2, rl2 = rewards_func(share_2, utility_2, pool_12, log_p_2, baselines[id_2])\n",
    "         \n",
    "            #for i in range(num_finishers):\n",
    "                #print(r1[i], r2[i])\n",
    "                #if r1[i]==0:\n",
    "                #    print(share_2[i])\n",
    "                #    print(share_1[i])\n",
    "                #    print(utility_1[i])\n",
    "                #    print(utility_2[i])\n",
    "                #    print(pool_12[i])\n",
    "                #print(lt[i])\n",
    "                #print(lp[i])\n",
    "                #print(ll[i])\n",
    "                #print(baselines)\n",
    "            \n",
    "            # Add rewards and reward losses\n",
    "            rewards[id_1] = r1.squeeze()\n",
    "            rewards[id_2] = r2.squeeze()\n",
    "            reward_losses[id_1] = rl1\n",
    "            reward_losses[id_2] = rl2\n",
    "            reward_sums[id_1] = reward_sums[id_1] + rewards[id_1].sum()\n",
    "            reward_sums[id_2] = reward_sums[id_2] + rewards[id_2].sum()\n",
    "\n",
    "        prev_proposals = proposals # Don't need previous proposals anymore so update it\n",
    "        \n",
    "        # Gradient descent -----------------------------------------------------------\n",
    "        \n",
    "        for i in range(num_agents):\n",
    "            # optimize\n",
    "            loss = reward_losses[i] - entropy_losses[i]\n",
    "            \n",
    "            optimizers[i].zero_grad()\n",
    "            loss.backward()\n",
    "            optimizers[i].step()\n",
    "        \n",
    "        # Wrapping up the end of turn ------------------------------------------------\n",
    "        # Remove finished games\n",
    "        # In term and term_N, element = 1 means die\n",
    "        term_N = (N <= (i_turn+1)).view(num_alive,1).long() # Last turn reached; i_turn + 1 since i_turn starts counting from 0\n",
    "        # In survivors, element = 1 means live\n",
    "        survivors = (term+term_N) == 0\n",
    "\n",
    "        # Check if everyone's dead\n",
    "        if survivors.sum() == 0: # If all games over, break episode\n",
    "            # Baseline updates\n",
    "            for i in range(num_agents):\n",
    "                # Update with batch-averaged rewards\n",
    "                baselines[i] = smoothing_const * baselines[i] + (1-smoothing_const)*reward_sums[i]/num_games\n",
    "            break;\n",
    "            \n",
    "        # Reshape\n",
    "        survivors = ((term+term_N) == 0).nonzero()[:,0].view(-1,1)\n",
    "        num_alive = len(survivors) # Number of survivors\n",
    "\n",
    "        #print('i_turn = ' + str(i_turn))\n",
    "        \n",
    "    #print('i_ep = ' + str(i_ep))\n",
    "    if (i_ep % ep_time == 0) and (i_ep != 0):\n",
    "        time_p2 = time.time()\n",
    "        print('Runtime for episodes ' + str(i_ep-ep_time) + '-' + str(i_ep) + ': ' + str(time_p2 - time_p1) + 's')\n",
    "        time_p1 = time_p2\n",
    "        \n",
    "    if (i_ep % ep_record == 0):\n",
    "        for j in range(num_agents):\n",
    "            r_list[j].append(reward_sums[j]/num_games)\n",
    "    \n",
    "    #print('----------------')\n",
    "    \n",
    "print('End ------------------')\n",
    "time_finish = time.time()\n",
    "print('Total runtime: ' + str(time_finish-time_start) + 's')\n",
    "\n",
    "#for i in range(num_agents):\n",
    "#    torch.save(Agents[0].state_dict(),'saved_model_agent_' + str(i) + '.pt')\n",
    "    \n",
    "#Agents[0].load_state_dict(torch.load('saved_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-1.2434,  0.7392, -1.2891,  0.5243,  0.0413,  0.5524,  0.3027,\n",
       "          -1.8542,  2.0847,  0.2147, -0.4083,  0.6640,  0.4650, -0.5117,\n",
       "          -0.3007, -0.6512,  0.8171, -0.1765, -1.6775, -0.2017, -0.4943,\n",
       "           2.6301, -0.5356, -1.7895, -0.7340, -1.6677, -0.9648,  1.1681,\n",
       "          -1.1146, -0.9332,  0.7118, -1.1328,  0.2798, -0.4856, -0.1763,\n",
       "          -0.6043, -0.8246,  0.0432,  1.0800,  0.6672,  0.2117,  1.4400,\n",
       "           1.6802, -0.8136, -0.0293, -0.8887, -2.2676, -0.1542, -1.7502,\n",
       "          -0.5789,  0.0535,  1.8024, -0.7787,  1.1520, -0.3148,  0.0099,\n",
       "          -0.2665, -0.1109,  0.6388, -0.3715, -1.1392,  0.2028, -1.1968,\n",
       "          -0.8832,  0.1751, -1.0045,  0.2386,  0.9834, -0.2993, -0.4406,\n",
       "          -1.3448, -1.2522,  0.1973,  0.4426,  0.2202,  0.0397,  1.1438,\n",
       "           0.4814,  0.1610, -0.7312, -1.0072,  0.1790,  1.0765,  1.3188,\n",
       "           1.3561, -0.3485, -1.0101, -0.1361, -0.1185, -0.3661,  0.6578,\n",
       "          -0.0864,  0.0688,  1.0608, -2.5432, -1.0101, -1.5352, -0.6684,\n",
       "           1.1428, -1.2801],\n",
       "         [-1.4762,  0.5993,  0.9873, -0.5432,  0.6488,  0.0611, -1.0127,\n",
       "          -0.3660,  0.5070,  2.1252, -0.1065, -0.1407,  1.1916, -1.8838,\n",
       "           1.3256, -0.7890, -1.2966,  1.9158, -0.0602, -1.7895, -0.4295,\n",
       "           0.5474, -0.5038, -2.3002, -1.1601,  1.7874, -2.2600, -0.3023,\n",
       "           0.4888,  0.5245, -0.2453,  1.1323, -0.0848,  0.7126,  0.4899,\n",
       "           0.2027, -0.1085,  1.7144, -0.0465,  1.2946,  0.3326, -0.7408,\n",
       "          -0.8218, -2.5043,  1.0044,  0.2634,  1.1815, -0.1550, -1.2351,\n",
       "          -1.0056, -0.5870, -1.9378, -2.0527, -0.6932,  1.4417,  0.6000,\n",
       "          -0.8729, -0.5461, -1.5804,  0.1447,  0.3665, -0.7914, -0.2026,\n",
       "           0.0837, -0.7038, -0.1227, -0.3000,  0.3512,  0.9304, -1.4711,\n",
       "          -2.1426,  1.1462, -1.5173,  0.2293,  0.3646, -1.0206, -0.1004,\n",
       "          -0.3055,  1.5758,  0.9905, -0.9059, -0.2138, -0.1182,  0.7087,\n",
       "          -0.3370,  0.2625, -1.6948, -1.0212,  1.1202, -0.5890,  0.6979,\n",
       "          -1.5566, -0.5432, -0.8527,  0.0016,  0.1309, -0.2462, -0.4367,\n",
       "          -0.9675,  0.4667],\n",
       "         [ 0.6045,  1.5443,  0.0140,  0.1317,  1.6540,  1.0892, -0.6186,\n",
       "          -0.3780, -0.4477,  1.2819, -0.2568, -1.1997, -0.5845,  1.1429,\n",
       "          -0.4808, -1.0912, -1.6409, -0.7211,  0.0113, -1.4717, -0.4222,\n",
       "           0.0497,  0.5503, -0.6404,  0.2402,  1.2197,  0.5930,  0.4335,\n",
       "           0.3578, -1.1997,  1.2585, -0.8688,  0.8831,  0.5179, -1.1146,\n",
       "          -0.0141, -0.3430, -1.8749,  0.5167,  2.3173, -0.2778, -1.6959,\n",
       "           0.2591, -1.1349,  0.0184, -0.5923,  1.6118,  0.9435, -0.6248,\n",
       "           0.4191, -1.1653,  0.8225, -0.9644,  0.0648,  0.8342, -1.0326,\n",
       "           0.2145,  0.3208,  2.0877, -0.5062,  0.2185, -1.5009, -1.0968,\n",
       "           0.8451,  0.8234, -0.6288,  0.0799,  1.6989, -0.4553, -0.6217,\n",
       "           0.7777,  0.0949,  0.5698, -0.6759, -0.4332,  0.4411,  0.6158,\n",
       "           0.1877, -0.9096, -0.8810,  0.6043,  2.2000, -1.3314,  0.8798,\n",
       "           0.9756, -1.3167,  0.3194, -0.0751, -0.7317, -0.6907, -0.2463,\n",
       "           0.6198, -0.7941, -0.3952,  1.5535,  0.3397, -0.8991,  0.2374,\n",
       "           0.6723,  1.0171],\n",
       "         [ 0.0366,  0.3911,  0.4799,  2.0731,  0.4761,  0.5518,  1.6316,\n",
       "           0.7681,  1.4404, -1.0824,  2.0351,  0.4327, -0.7512, -0.7845,\n",
       "          -0.8686,  0.8328, -2.6866, -0.2910, -1.6419,  0.5750, -1.4272,\n",
       "          -2.0981,  2.2452, -0.2324,  0.9031,  0.2393,  1.1093, -0.7624,\n",
       "           1.0682,  0.7474, -0.2179,  0.3896, -0.1628,  0.6761,  0.6856,\n",
       "           0.7354, -1.1304, -0.0015,  0.7701, -1.5667,  1.3528, -0.9834,\n",
       "          -0.2060, -1.0206, -0.4808, -0.0168,  0.2716, -1.2327,  0.3664,\n",
       "          -0.6344, -1.1511, -0.4323, -0.3142,  0.6977, -0.1422,  0.2188,\n",
       "           0.2390,  0.2439,  0.4819,  1.7114, -0.2563, -0.2567, -0.4990,\n",
       "          -1.3966,  1.1799, -1.0174,  0.0820, -0.4169, -0.3532,  0.4031,\n",
       "          -1.8324,  1.5336,  1.2044,  1.3573, -0.5179, -0.4277, -0.6174,\n",
       "           0.9965, -0.8821, -0.1592, -0.4916, -1.0736,  1.0868,  0.4751,\n",
       "           0.7263, -0.2291,  0.0797, -0.7038, -0.5052,  0.7747,  2.3459,\n",
       "          -0.3441,  0.3281,  0.2789,  0.7114,  1.0173, -1.8523, -0.6715,\n",
       "          -0.8276, -0.3535],\n",
       "         [ 0.7565,  1.9242,  0.6966,  1.0137,  0.2372, -0.3485,  0.2036,\n",
       "          -0.1476, -1.4996, -1.1855,  1.2858,  0.8945,  0.5086, -0.6293,\n",
       "          -2.4646,  1.0090, -1.7786, -0.7557, -0.7484,  0.9490, -0.0593,\n",
       "           0.6281,  0.4120,  0.4800, -0.3676, -1.7044,  0.1381,  0.2795,\n",
       "           0.5847,  1.1873,  1.4306, -0.3698,  0.0944, -0.5345,  0.0886,\n",
       "           0.1435,  0.3066, -0.8280,  0.5951,  1.0175,  1.1748,  0.5968,\n",
       "           2.1479,  0.1644, -0.3805, -1.6947, -0.8580,  0.1242, -0.3547,\n",
       "           0.5531,  0.4744, -1.2892,  1.0757,  0.7175, -1.1938,  0.6186,\n",
       "           0.3798, -0.6297,  0.2401, -0.0607, -0.0564,  0.7572,  0.9328,\n",
       "          -0.6648,  0.7887, -0.3035, -0.2081,  0.8270, -0.4677,  0.7754,\n",
       "           1.0516, -0.5596,  1.4765, -0.1234,  0.5302,  1.6818, -1.2810,\n",
       "          -1.6064,  1.6486,  1.3389,  1.6386, -0.4611,  0.4174,  0.5721,\n",
       "           1.7698,  0.6658,  0.0289,  1.7188,  0.7733, -1.0095, -0.3949,\n",
       "           0.9952,  2.5426,  1.0329, -0.4917,  1.2111, -0.5433,  0.9027,\n",
       "           0.7403,  1.3036],\n",
       "         [ 0.3093, -0.9332,  0.9311, -0.3840, -0.6602, -0.1234,  0.2971,\n",
       "           1.2924, -0.6448, -0.5267, -1.0148, -0.6266, -0.6242, -0.3855,\n",
       "           1.1173,  1.3451,  0.0282, -1.5072, -1.2143, -0.3391, -1.0440,\n",
       "          -0.4671, -0.1649, -0.4441,  0.6332, -0.7264, -0.0270, -1.6376,\n",
       "          -1.3768, -1.3844, -0.7357,  1.6972, -1.0017, -0.7838, -0.6242,\n",
       "           0.0785,  1.6634, -0.5111, -1.3645,  0.1167, -0.3073,  0.1458,\n",
       "           1.3352, -0.5779,  0.9142,  1.0598, -0.1815,  0.7699,  0.0886,\n",
       "          -1.3491,  0.6909, -0.4719, -1.6398,  0.5873,  0.7532,  0.0522,\n",
       "          -1.0317, -1.4111, -0.1433, -1.4129,  0.4605,  1.7249, -1.2834,\n",
       "          -0.4062,  2.9295,  1.6529,  0.8728,  1.3452,  0.8733, -1.6661,\n",
       "           2.6751, -0.9119, -0.5681, -0.0576,  0.8645,  0.3808,  1.1573,\n",
       "          -0.1340, -0.6708, -0.8153, -0.3647,  0.0658, -0.7709, -0.5462,\n",
       "          -0.3331,  1.1873,  0.7840, -0.7891,  0.3061,  2.4772, -0.6824,\n",
       "           0.6059, -1.0669,  0.9081,  0.9585,  2.1727, -1.0946,  0.3149,\n",
       "           0.7293, -2.1948]]), Parameter containing:\n",
       " tensor([[-1.2169, -2.0501,  1.1898,  ...,  0.4052,  0.1157,  2.1912],\n",
       "         [ 0.2994, -2.0228,  0.5553,  ..., -1.0579, -0.5452, -0.8253],\n",
       "         [ 2.3678,  0.9370, -0.8505,  ..., -0.8381, -0.1221,  0.7448],\n",
       "         ...,\n",
       "         [-0.3073,  1.2418, -0.1313,  ..., -0.7878,  0.5706,  0.3445],\n",
       "         [ 0.3251,  0.5762,  1.0545,  ..., -0.6518, -0.5502,  0.1210],\n",
       "         [ 0.0871, -0.5456, -0.1299,  ...,  0.7757, -0.0841,  0.4174]]), Parameter containing:\n",
       " tensor([[ 6.8492e-02, -7.1968e-02, -8.2810e-02,  ...,  6.3415e-02,\n",
       "           7.8825e-02, -2.0085e-02],\n",
       "         [ 8.9855e-02, -1.6587e-02,  5.4413e-02,  ...,  5.8636e-02,\n",
       "           6.6838e-02,  1.1285e-01],\n",
       "         [-9.1113e-02, -6.5399e-02, -9.6030e-02,  ..., -7.3306e-02,\n",
       "          -9.0329e-02,  6.0532e-02],\n",
       "         ...,\n",
       "         [ 1.4703e-02,  4.0634e-02,  3.0003e-02,  ...,  2.4656e-02,\n",
       "           4.0499e-02,  3.4515e-02],\n",
       "         [-4.9691e-02,  2.0718e-02,  5.1418e-02,  ..., -6.2245e-02,\n",
       "           8.4133e-02,  3.1909e-02],\n",
       "         [-2.1778e-02, -4.4527e-02, -1.8557e-02,  ..., -2.6084e-02,\n",
       "          -1.0078e-02,  8.3194e-02]]), Parameter containing:\n",
       " tensor([[ 2.3230e-02,  7.0809e-02,  4.6124e-02,  ...,  4.8433e-02,\n",
       "          -8.3373e-02, -6.5622e-02],\n",
       "         [ 5.1106e-02, -1.8846e-02, -1.7895e-02,  ...,  4.0986e-02,\n",
       "          -3.7885e-02, -6.9913e-02],\n",
       "         [-4.2111e-02, -1.0080e-01,  4.7331e-02,  ...,  7.6181e-02,\n",
       "           2.1666e-02,  5.5555e-02],\n",
       "         ...,\n",
       "         [-2.7573e-02,  4.9984e-02, -2.6379e-02,  ...,  3.8797e-02,\n",
       "          -6.0224e-02,  9.6638e-03],\n",
       "         [ 1.1104e-01, -1.2526e-02, -7.2417e-02,  ..., -7.7595e-02,\n",
       "          -4.8416e-02, -8.5726e-02],\n",
       "         [ 3.5638e-02,  4.8163e-02,  5.9467e-02,  ...,  4.4306e-02,\n",
       "           9.9540e-03, -2.7608e-02]]), Parameter containing:\n",
       " tensor([ 0.0239,  0.0871, -0.0112, -0.0816,  0.0700,  0.0274,  0.0583,\n",
       "          0.0297,  0.0354, -0.0287, -0.0588,  0.0427,  0.0128,  0.0646,\n",
       "         -0.0327, -0.0372,  0.0558,  0.0282,  0.0139,  0.0251, -0.0775,\n",
       "          0.0062,  0.0692,  0.0571, -0.0590, -0.0884, -0.1080, -0.0039,\n",
       "         -0.0861, -0.0992, -0.0154,  0.0568, -0.0416,  0.0216,  0.0827,\n",
       "         -0.0890,  0.0351,  0.0445,  0.0780,  0.0143,  0.0637, -0.0798,\n",
       "          0.0017,  0.0603, -0.0925,  0.0357,  0.0606, -0.0030, -0.1178,\n",
       "         -0.0782,  0.0968, -0.0348, -0.0332, -0.0999,  0.0708, -0.0505,\n",
       "          0.0109,  0.0726, -0.0379,  0.0139,  0.0367,  0.0135, -0.0464,\n",
       "         -0.0522, -0.0193, -0.0740,  0.1037,  0.0257,  0.0120, -0.1020,\n",
       "          0.0380, -0.0220, -0.0072, -0.0946, -0.0208,  0.0550, -0.0032,\n",
       "          0.0171, -0.0044, -0.1010,  0.0577, -0.0071, -0.0820, -0.0122,\n",
       "          0.0590,  0.0719, -0.0423,  0.0223, -0.1147,  0.0514, -0.0334,\n",
       "         -0.0757, -0.0164, -0.0609,  0.0557, -0.0032,  0.0487,  0.0211,\n",
       "         -0.0910, -0.0255, -0.0123, -0.0707, -0.0553, -0.1079,  0.0298,\n",
       "          0.1178, -0.0603,  0.0661,  0.0880,  0.0723,  0.0758,  0.0555,\n",
       "         -0.0553, -0.1024, -0.0785, -0.0855, -0.0065,  0.0000, -0.0071,\n",
       "          0.0713, -0.1105,  0.0125,  0.0130,  0.0240, -0.0130, -0.1059,\n",
       "         -0.0332,  0.0423, -0.0714,  0.0203,  0.0323, -0.0337, -0.0936,\n",
       "          0.0075, -0.0946,  0.0107, -0.0892,  0.0969,  0.0614,  0.0644,\n",
       "         -0.0280,  0.0615,  0.0772,  0.0449,  0.0111, -0.0578,  0.0527,\n",
       "         -0.0244,  0.0203, -0.0767,  0.1008, -0.0680, -0.0011, -0.0149,\n",
       "          0.0821,  0.0374,  0.0710, -0.0651,  0.0014, -0.0613,  0.0213,\n",
       "         -0.0272, -0.1000,  0.0776, -0.0725, -0.0768,  0.0862,  0.0638,\n",
       "          0.0434,  0.0498, -0.0071, -0.0469, -0.0250,  0.0051,  0.0336,\n",
       "          0.0317,  0.0108, -0.0795, -0.0108,  0.0360,  0.0848,  0.0476,\n",
       "         -0.0587, -0.1119,  0.0757, -0.0115, -0.0488, -0.0508,  0.0298,\n",
       "         -0.0853, -0.0705,  0.0797, -0.0432,  0.0009, -0.0414, -0.0406,\n",
       "         -0.0835, -0.0508, -0.0123, -0.0895, -0.0244,  0.0772, -0.0248,\n",
       "         -0.0165,  0.0829, -0.0473, -0.0956,  0.0704, -0.0126,  0.0019,\n",
       "          0.0251,  0.0568,  0.0772,  0.0053,  0.0727,  0.0190,  0.1058,\n",
       "          0.0203,  0.0070,  0.0365, -0.0531,  0.0081,  0.0611, -0.0298,\n",
       "         -0.0369,  0.0636,  0.0057,  0.0342,  0.0725, -0.0843, -0.0766,\n",
       "         -0.0567, -0.0824,  0.0432, -0.0846,  0.0316,  0.0174, -0.0662,\n",
       "          0.0449,  0.0309, -0.0349,  0.0175,  0.0350,  0.0703,  0.0228,\n",
       "          0.0665,  0.0962, -0.0412, -0.0790,  0.0987,  0.0781, -0.0911,\n",
       "          0.0223,  0.0607,  0.0290, -0.0027,  0.0515,  0.0038,  0.0385,\n",
       "         -0.1000,  0.0442, -0.0108,  0.0650, -0.0006, -0.0197, -0.0552,\n",
       "         -0.0727, -0.0396, -0.0201, -0.1001, -0.0436,  0.0470, -0.0062,\n",
       "          0.0369, -0.0226,  0.0723,  0.0181,  0.0823, -0.0881, -0.0405,\n",
       "          0.0113,  0.0882,  0.0799, -0.0012, -0.0146,  0.0671, -0.0740,\n",
       "          0.0417, -0.0650, -0.0675,  0.0552, -0.0382, -0.0342, -0.0087,\n",
       "          0.0100,  0.0460, -0.0142,  0.0346,  0.0599, -0.0531,  0.0195,\n",
       "          0.0523,  0.0270,  0.0022,  0.0117, -0.0931, -0.0445,  0.0310,\n",
       "         -0.0221, -0.0769, -0.0984, -0.0794,  0.0631, -0.0499, -0.0425,\n",
       "         -0.1122,  0.0825, -0.0098,  0.0165, -0.0153, -0.0652, -0.0559,\n",
       "         -0.0672,  0.0897,  0.0651, -0.0109,  0.0509, -0.0589,  0.0528,\n",
       "         -0.0184, -0.0003, -0.0662, -0.0802,  0.0580,  0.0542, -0.0625,\n",
       "         -0.0311, -0.0586,  0.0874, -0.0634,  0.0223, -0.0695, -0.0554,\n",
       "         -0.0659,  0.0565,  0.0561, -0.0522, -0.0633, -0.0987, -0.0669,\n",
       "          0.0310, -0.0024, -0.0113, -0.1061,  0.0061, -0.0105, -0.0554,\n",
       "          0.0122, -0.0524, -0.0795,  0.0510,  0.0764, -0.0396,  0.0522,\n",
       "         -0.0795,  0.0315, -0.0131,  0.0019, -0.0278, -0.0504,  0.0405,\n",
       "          0.0331,  0.0782, -0.0547,  0.0749, -0.0427, -0.0772,  0.0195,\n",
       "         -0.0580, -0.0644,  0.0884, -0.0550, -0.0365,  0.0696, -0.0690,\n",
       "          0.0486, -0.0088, -0.0920,  0.0327,  0.0550,  0.0578, -0.0840,\n",
       "         -0.0488, -0.0579,  0.0407,  0.0928,  0.0371, -0.0883,  0.0646,\n",
       "          0.0148]), Parameter containing:\n",
       " tensor([-0.0235,  0.0775, -0.0644, -0.0819, -0.0999,  0.0143,  0.0450,\n",
       "          0.0230, -0.0514,  0.0420, -0.0783, -0.0189,  0.0449, -0.0773,\n",
       "         -0.0353,  0.0679,  0.0435, -0.0476,  0.0315, -0.0017,  0.0662,\n",
       "          0.0426, -0.1026,  0.0043,  0.0250, -0.0744, -0.1328, -0.0012,\n",
       "         -0.0357,  0.0086, -0.0642, -0.0599,  0.0961,  0.0688,  0.0012,\n",
       "         -0.1017, -0.0076,  0.0148,  0.0587, -0.0293, -0.0441, -0.0408,\n",
       "          0.0339,  0.0197,  0.0759, -0.0420, -0.0231, -0.0549,  0.0219,\n",
       "         -0.0746,  0.0647,  0.0925, -0.0831,  0.0473,  0.0300, -0.0442,\n",
       "         -0.0554, -0.0082, -0.0885, -0.0152,  0.0929,  0.0942,  0.0006,\n",
       "         -0.0561,  0.0741, -0.0370, -0.0694, -0.0543,  0.0380, -0.0803,\n",
       "         -0.0833,  0.0311,  0.0021, -0.0024, -0.0792,  0.0305,  0.0108,\n",
       "         -0.0700, -0.0790,  0.0424,  0.0547,  0.0719, -0.0235, -0.0391,\n",
       "         -0.0906,  0.0039,  0.0449, -0.0538, -0.0146, -0.0914,  0.0223,\n",
       "         -0.0308,  0.0273,  0.0036, -0.0337,  0.1091, -0.0578,  0.0797,\n",
       "         -0.0931, -0.0836, -0.0745, -0.0853, -0.0568, -0.0778,  0.0500,\n",
       "         -0.0671, -0.1019, -0.0643, -0.0242,  0.0698, -0.0072,  0.0711,\n",
       "         -0.1043, -0.0447,  0.0492,  0.0102,  0.0001, -0.0511, -0.1127,\n",
       "         -0.0707, -0.0925, -0.0811,  0.0555,  0.0381,  0.0714, -0.0189,\n",
       "          0.0493, -0.0813, -0.0887,  0.0428, -0.0336, -0.0520, -0.0603,\n",
       "          0.0664,  0.0443, -0.0958,  0.0685,  0.0487, -0.0611, -0.1095,\n",
       "          0.0253, -0.1108,  0.0817,  0.0501, -0.0139,  0.0388, -0.0890,\n",
       "          0.0509, -0.0185, -0.0930, -0.0105,  0.0125,  0.0192,  0.0442,\n",
       "          0.0443,  0.0412,  0.0447,  0.0854, -0.0995,  0.0969,  0.0598,\n",
       "         -0.0256,  0.0802,  0.0574, -0.0458, -0.0720, -0.0233, -0.0385,\n",
       "          0.0122, -0.0800, -0.0907,  0.0816, -0.1027, -0.0901,  0.0232,\n",
       "          0.1149,  0.0151,  0.0006, -0.0214,  0.0500,  0.0315,  0.0357,\n",
       "          0.0508, -0.0872, -0.0691, -0.0134, -0.0806,  0.0277,  0.0519,\n",
       "          0.0442,  0.0715, -0.0582, -0.0746,  0.0838,  0.0597,  0.1100,\n",
       "          0.0299, -0.0997, -0.0156, -0.0977, -0.0430,  0.1017,  0.0864,\n",
       "          0.0621,  0.0436, -0.0164, -0.0780, -0.0718, -0.0939,  0.0003,\n",
       "         -0.0886,  0.0375, -0.0756,  0.0511, -0.0046,  0.0763,  0.0433,\n",
       "          0.0231,  0.0638,  0.0679, -0.0010, -0.0082, -0.0352, -0.0382,\n",
       "         -0.0382, -0.0260, -0.0452,  0.1109,  0.0836,  0.0284, -0.0985,\n",
       "          0.0580,  0.0197,  0.0647,  0.0187,  0.0132, -0.0809, -0.0436,\n",
       "         -0.0614,  0.0123, -0.0730,  0.0721, -0.1003, -0.0939, -0.0557,\n",
       "         -0.0244, -0.0344, -0.0681,  0.0322,  0.0102,  0.1107,  0.0920,\n",
       "         -0.0961, -0.0052, -0.0519,  0.0093, -0.0097,  0.0074,  0.0534,\n",
       "         -0.0975, -0.1087, -0.0027,  0.0061,  0.0895, -0.0801, -0.0505,\n",
       "          0.0605, -0.0603,  0.0744, -0.0475,  0.1001,  0.0407, -0.0770,\n",
       "         -0.0582, -0.0521,  0.0043,  0.1145,  0.0295,  0.0261, -0.0746,\n",
       "         -0.0180,  0.0520, -0.0699, -0.0647, -0.0350, -0.0467, -0.0019,\n",
       "         -0.0143, -0.0323,  0.0917,  0.0527, -0.0225,  0.0157, -0.0446,\n",
       "          0.0055, -0.0034,  0.0781,  0.0893,  0.0199, -0.0931,  0.0244,\n",
       "          0.0011, -0.0637,  0.0788, -0.0263,  0.0128,  0.0179, -0.0611,\n",
       "         -0.0677, -0.0388, -0.0566, -0.0527,  0.0916, -0.0297,  0.0252,\n",
       "          0.0395, -0.0186, -0.0285,  0.0184,  0.0854, -0.0866, -0.0593,\n",
       "         -0.1095,  0.0273, -0.0242,  0.0174, -0.0008, -0.0652, -0.1152,\n",
       "          0.0752, -0.0817,  0.0965,  0.0891, -0.0232, -0.0751, -0.0115,\n",
       "         -0.0939,  0.0621, -0.0805, -0.0322, -0.0815, -0.0841, -0.0535,\n",
       "         -0.0577,  0.0320,  0.0517, -0.0349,  0.0783, -0.0261,  0.0764,\n",
       "         -0.0902, -0.0419, -0.0456, -0.0168, -0.0848, -0.0759, -0.0633,\n",
       "          0.0039,  0.0818,  0.0979, -0.0211,  0.0678,  0.0233,  0.0970,\n",
       "          0.0232,  0.0225,  0.0929,  0.0316, -0.0906, -0.0891,  0.0181,\n",
       "         -0.0549, -0.0757, -0.0609, -0.0703, -0.0849, -0.0882, -0.0950,\n",
       "         -0.0225,  0.0200,  0.0449, -0.0936, -0.0791, -0.1045,  0.0130,\n",
       "         -0.0362,  0.0318,  0.0992, -0.1052,  0.0775,  0.0593, -0.0426,\n",
       "         -0.0604, -0.0927,  0.0169, -0.0691, -0.0012,  0.0617, -0.0893,\n",
       "         -0.0214]), Parameter containing:\n",
       " tensor([[-7.6604e-02, -7.0138e-02,  8.7033e-02,  ..., -7.6219e-02,\n",
       "           1.0410e-01, -4.3225e-02],\n",
       "         [ 1.4264e-02, -8.1052e-02, -1.0407e-03,  ...,  6.6666e-03,\n",
       "          -4.2151e-02, -3.1615e-02],\n",
       "         [ 2.2609e-02, -7.8837e-02, -3.9978e-02,  ..., -3.5052e-02,\n",
       "          -8.9129e-03,  6.0832e-02],\n",
       "         ...,\n",
       "         [-7.2470e-02, -1.3668e-03, -5.9573e-02,  ..., -6.9070e-02,\n",
       "          -1.1194e-02, -8.1863e-02],\n",
       "         [-6.9140e-02, -5.1595e-02, -5.7531e-02,  ...,  1.4570e-02,\n",
       "          -3.9549e-02,  1.2041e-02],\n",
       "         [-5.6369e-02,  7.2296e-02, -7.9536e-02,  ..., -1.1071e-02,\n",
       "           6.7318e-02,  7.9383e-02]]), Parameter containing:\n",
       " tensor([[ 3.4639e-02, -3.3828e-02,  2.2253e-02,  ...,  6.3745e-02,\n",
       "           7.8933e-02, -9.2214e-02],\n",
       "         [ 2.3011e-02, -5.7609e-02,  4.5594e-02,  ..., -7.6413e-02,\n",
       "           4.6596e-02,  5.8810e-02],\n",
       "         [-3.5306e-02, -9.3447e-02,  2.4970e-02,  ..., -7.5722e-02,\n",
       "           1.6660e-02, -3.5608e-02],\n",
       "         ...,\n",
       "         [-1.4297e-02, -1.0323e-01,  1.1829e-01,  ...,  5.8530e-02,\n",
       "           5.6808e-03,  9.5201e-02],\n",
       "         [ 5.1522e-02, -2.9138e-02,  5.1905e-02,  ...,  7.9497e-03,\n",
       "          -5.7919e-02,  3.0628e-02],\n",
       "         [-6.3787e-02, -6.7210e-02, -3.0817e-02,  ..., -2.9265e-02,\n",
       "           6.6747e-02, -3.4644e-02]]), Parameter containing:\n",
       " tensor([ 0.0259,  0.0870, -0.0375,  0.0989, -0.0448, -0.0485,  0.0174,\n",
       "          0.0863, -0.0140,  0.0478,  0.0364, -0.1047,  0.0879,  0.1090,\n",
       "          0.0105,  0.0114, -0.0483,  0.0683, -0.0857, -0.0684,  0.0061,\n",
       "         -0.0760,  0.0542, -0.0605,  0.0500, -0.0315,  0.0586, -0.0726,\n",
       "          0.0518, -0.0661, -0.0495,  0.0833, -0.0044, -0.0278, -0.0771,\n",
       "          0.0830, -0.1067, -0.0032,  0.0260,  0.0313,  0.0979, -0.0639,\n",
       "         -0.0335,  0.0170,  0.0671, -0.0001,  0.0230, -0.0617,  0.0334,\n",
       "          0.0656, -0.0351,  0.0844, -0.0610,  0.0516,  0.0815, -0.1035,\n",
       "          0.0120, -0.0644, -0.0894,  0.0188, -0.0115, -0.0974, -0.0785,\n",
       "          0.0223,  0.0740,  0.0944,  0.0942,  0.0845, -0.0552, -0.0242,\n",
       "          0.0225,  0.0694,  0.0214, -0.0932, -0.0351,  0.0897, -0.1086,\n",
       "         -0.1072,  0.0402, -0.0646,  0.0995, -0.0816, -0.0570, -0.0094,\n",
       "         -0.0271, -0.0830, -0.0673, -0.0951,  0.0141,  0.0671, -0.0304,\n",
       "          0.0189, -0.0520, -0.0595, -0.0181,  0.0016, -0.0049, -0.0144,\n",
       "          0.0440,  0.0770,  0.0106,  0.0877, -0.0314, -0.0945,  0.0475,\n",
       "         -0.0486, -0.0991, -0.0949, -0.0400,  0.0493, -0.0356, -0.0771,\n",
       "         -0.0961, -0.0326, -0.0403, -0.0675, -0.0696, -0.0867,  0.0226,\n",
       "         -0.0915, -0.0134, -0.0657, -0.0151,  0.0737,  0.0770, -0.0673,\n",
       "         -0.0122, -0.0986,  0.0526,  0.0864,  0.0122, -0.0393, -0.0764,\n",
       "         -0.0564,  0.0702,  0.0858,  0.0754,  0.0786, -0.0655, -0.0724,\n",
       "         -0.0731, -0.0543,  0.0924, -0.0037,  0.0973,  0.0202, -0.0038,\n",
       "         -0.0544,  0.0718,  0.0866, -0.0816, -0.0497,  0.0263, -0.0990,\n",
       "         -0.0475, -0.0464,  0.0203, -0.0194, -0.0779,  0.0435, -0.0919,\n",
       "         -0.0820,  0.1034, -0.0921, -0.0602, -0.0787,  0.0227,  0.0683,\n",
       "          0.0586,  0.0721, -0.0359, -0.0607, -0.1157, -0.0106, -0.0523,\n",
       "          0.0275, -0.0854,  0.0911, -0.0462, -0.0776, -0.0134, -0.0101,\n",
       "         -0.0077, -0.0536, -0.0252, -0.0112, -0.1000,  0.0019,  0.0970,\n",
       "          0.0122,  0.0533,  0.0329,  0.0143, -0.0648, -0.0841, -0.0352,\n",
       "         -0.0459, -0.0026, -0.1011, -0.0668, -0.0960,  0.0792, -0.1168,\n",
       "          0.0675,  0.0492,  0.0756,  0.0183,  0.0586,  0.0055,  0.0264,\n",
       "          0.0336,  0.0012,  0.0483,  0.0589,  0.0682,  0.0114,  0.0524,\n",
       "          0.0513, -0.0150, -0.0480,  0.0558,  0.0652,  0.0711, -0.0244,\n",
       "         -0.0544,  0.0763, -0.0017, -0.0071, -0.0628,  0.0233, -0.0336,\n",
       "         -0.0968, -0.0210, -0.0547, -0.0792,  0.0309,  0.0584,  0.0079,\n",
       "         -0.0676, -0.0137,  0.0161,  0.0653, -0.0411, -0.0433,  0.0573,\n",
       "         -0.0089, -0.0707,  0.0012, -0.0753,  0.0119,  0.0896,  0.0347,\n",
       "          0.0065, -0.0417, -0.0742, -0.0456, -0.0755,  0.0640,  0.0456,\n",
       "         -0.0213, -0.0659,  0.0726,  0.0005, -0.0175,  0.0493, -0.0115,\n",
       "          0.0181, -0.0910, -0.0417,  0.0489,  0.0284, -0.0646,  0.0650,\n",
       "          0.0396,  0.0599,  0.1008,  0.0312, -0.0748,  0.0479, -0.0877,\n",
       "         -0.0616, -0.0028,  0.1125,  0.0182,  0.0619,  0.0790, -0.0164,\n",
       "          0.0653,  0.0097,  0.0597, -0.0758, -0.0950,  0.0145,  0.0240,\n",
       "          0.0501, -0.0264, -0.0112, -0.0234,  0.0800,  0.0170, -0.0457,\n",
       "         -0.0634,  0.0710, -0.0879,  0.0918, -0.0366, -0.0004, -0.0642,\n",
       "         -0.0763, -0.0825, -0.0232,  0.0744,  0.0742,  0.0628,  0.0015,\n",
       "          0.0716,  0.0885,  0.0333, -0.0776,  0.0783, -0.0804,  0.0360,\n",
       "         -0.0224, -0.0141, -0.0675, -0.0116, -0.0932, -0.0712, -0.0263,\n",
       "         -0.0565,  0.0872,  0.0224,  0.0302,  0.0787, -0.0643,  0.0442,\n",
       "         -0.0389,  0.0589, -0.0133,  0.0622,  0.0316, -0.0748,  0.0445,\n",
       "          0.0579, -0.0504,  0.0549, -0.0970,  0.0902, -0.0936, -0.0116,\n",
       "          0.0668, -0.0533, -0.0998,  0.0327, -0.0487, -0.0283, -0.0178,\n",
       "         -0.0949, -0.0998,  0.0543, -0.0946,  0.0560,  0.0116, -0.0007,\n",
       "         -0.0102, -0.0020, -0.0629, -0.0415, -0.0630,  0.0742, -0.0784,\n",
       "         -0.0938, -0.0575,  0.0303,  0.0795, -0.0103, -0.1007, -0.0267,\n",
       "         -0.0650, -0.0967, -0.0677, -0.0005, -0.0688,  0.0802,  0.0128,\n",
       "          0.0791,  0.0445,  0.0745, -0.0853,  0.0108, -0.0121,  0.0414,\n",
       "         -0.0743, -0.0668,  0.0756, -0.0645, -0.1116,  0.0291, -0.0700,\n",
       "          0.0033]), Parameter containing:\n",
       " tensor([ 0.0578,  0.0803, -0.0687,  0.0892,  0.0338, -0.0103,  0.0967,\n",
       "          0.0543,  0.0249,  0.0817,  0.0139, -0.0309,  0.0348,  0.0262,\n",
       "         -0.0385, -0.0609,  0.0858, -0.0367,  0.0906, -0.0441, -0.0019,\n",
       "         -0.0112,  0.0199,  0.0567, -0.0644,  0.1020, -0.0787,  0.0603,\n",
       "         -0.0477, -0.0512,  0.0420, -0.0945, -0.0331,  0.0274,  0.0857,\n",
       "         -0.0719,  0.0761, -0.0667,  0.0136,  0.0447, -0.0626, -0.0649,\n",
       "         -0.0850,  0.0100, -0.0442, -0.1112, -0.0744, -0.0480, -0.0440,\n",
       "         -0.0695,  0.0253,  0.0916,  0.0515,  0.0399, -0.0013,  0.0679,\n",
       "          0.0117,  0.0209,  0.0342, -0.0203,  0.0413,  0.0785,  0.0124,\n",
       "          0.0153, -0.0307,  0.0909, -0.0014, -0.0562, -0.0918, -0.0793,\n",
       "         -0.0857,  0.0028,  0.0060,  0.0049, -0.0983,  0.0799, -0.1110,\n",
       "          0.0098,  0.0792, -0.0971,  0.0262, -0.0684, -0.0741,  0.0142,\n",
       "         -0.0077, -0.0571,  0.0561, -0.1118, -0.0442, -0.0983,  0.0023,\n",
       "          0.0617,  0.0113, -0.0396, -0.0637,  0.0668, -0.0646, -0.0415,\n",
       "         -0.0829, -0.0585,  0.0792, -0.0649,  0.0577, -0.0569,  0.0276,\n",
       "          0.0588, -0.1068,  0.0766,  0.0337, -0.0353,  0.0057, -0.1028,\n",
       "          0.0028, -0.0325,  0.0539, -0.0995,  0.0562, -0.0288, -0.0480,\n",
       "          0.0457, -0.0724,  0.0417, -0.0191, -0.0968, -0.0467, -0.0270,\n",
       "         -0.0667, -0.0890,  0.0002, -0.0539,  0.0277, -0.0288,  0.0658,\n",
       "          0.0240,  0.0119,  0.0238,  0.0253,  0.0621, -0.0663, -0.0667,\n",
       "         -0.0449,  0.0381, -0.0304, -0.0662,  0.0743,  0.0466,  0.0449,\n",
       "          0.0280, -0.0360, -0.0602,  0.0349,  0.0677, -0.0229,  0.0276,\n",
       "         -0.0400, -0.0948,  0.0832,  0.0898,  0.0818, -0.0367,  0.0990,\n",
       "         -0.0348,  0.0506, -0.0336, -0.0126,  0.0427,  0.1061, -0.0216,\n",
       "         -0.0989, -0.0287,  0.0819, -0.0397, -0.0482, -0.0562, -0.0001,\n",
       "         -0.0704, -0.0342,  0.0664, -0.0021, -0.0006, -0.0991,  0.0758,\n",
       "          0.0324, -0.0318, -0.0201,  0.0467, -0.0719, -0.0447,  0.0330,\n",
       "          0.0748, -0.0088,  0.0955,  0.0018,  0.0671, -0.0642, -0.0054,\n",
       "          0.0156, -0.0839,  0.0852,  0.0636,  0.0711, -0.0128,  0.0449,\n",
       "          0.0905, -0.0859, -0.1027,  0.1008,  0.0998,  0.0015, -0.0987,\n",
       "          0.0814,  0.0634,  0.0541,  0.0329,  0.0875,  0.0786, -0.0893,\n",
       "         -0.0360,  0.0337, -0.1094, -0.0203,  0.0602, -0.0501,  0.0593,\n",
       "         -0.0776, -0.1006,  0.0776, -0.0095,  0.0238,  0.0763,  0.0686,\n",
       "         -0.0465, -0.0918,  0.1067,  0.0494,  0.0640,  0.0978,  0.0953,\n",
       "         -0.0269,  0.0382,  0.0811, -0.0856,  0.0743, -0.0458, -0.0276,\n",
       "         -0.0746, -0.0636, -0.0520,  0.0862, -0.0259,  0.0610, -0.0021,\n",
       "          0.0105, -0.0178,  0.0740, -0.0337, -0.0536,  0.0366,  0.0347,\n",
       "         -0.0217, -0.1068, -0.0209, -0.0784,  0.0473, -0.0014,  0.0378,\n",
       "          0.0166, -0.0718,  0.0122,  0.0691, -0.0419, -0.0435,  0.0762,\n",
       "         -0.0180,  0.0063, -0.0661, -0.0863, -0.0558,  0.0506,  0.0103,\n",
       "         -0.0966,  0.0915, -0.0271, -0.0387, -0.0558,  0.0046, -0.0468,\n",
       "         -0.0286,  0.0544, -0.0229, -0.0007, -0.0206, -0.0488,  0.0181,\n",
       "          0.0923,  0.0412, -0.0220, -0.0070,  0.0537, -0.0639,  0.1106,\n",
       "          0.0334, -0.0260, -0.0568,  0.0052,  0.0244, -0.0244, -0.0376,\n",
       "          0.0560, -0.0750, -0.0402, -0.0861, -0.0360, -0.0658,  0.0918,\n",
       "          0.0311, -0.0116, -0.0955,  0.0443, -0.0760,  0.0021, -0.0681,\n",
       "         -0.0708,  0.0078, -0.0555, -0.0537,  0.0266,  0.0561, -0.0344,\n",
       "         -0.0339,  0.0582,  0.0969,  0.0690, -0.0541,  0.0891, -0.0347,\n",
       "          0.0004, -0.1091,  0.1122, -0.0556,  0.0381,  0.0665,  0.0908,\n",
       "         -0.0081,  0.0320,  0.0423, -0.0374, -0.0906,  0.0498,  0.0022,\n",
       "          0.0910, -0.0141, -0.0818,  0.0413,  0.0119, -0.0992,  0.0153,\n",
       "         -0.0055,  0.0558,  0.0037,  0.0246, -0.0161,  0.0082,  0.0326,\n",
       "         -0.0771, -0.0578,  0.0316,  0.0723, -0.0535,  0.0647, -0.0103,\n",
       "          0.0522, -0.0645,  0.0209, -0.0519, -0.0055, -0.0296, -0.0161,\n",
       "          0.0640,  0.0389, -0.0530, -0.0881, -0.0371, -0.0152,  0.0834,\n",
       "         -0.0430, -0.0434, -0.1071,  0.0485,  0.0265,  0.0604, -0.0679,\n",
       "          0.0963, -0.0569, -0.0627, -0.0306,  0.0473, -0.0246, -0.0050,\n",
       "         -0.0058]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [[ 8.1089, -3.2526, -5.0342,  ...,  8.3842, -5.9065,  7.4996],\n",
       "         [-0.4061,  0.5875, -4.8117,  ..., -0.4822,  3.7740,  4.2318],\n",
       "         [-6.2393,  4.5203,  5.5604,  ..., -9.3533, -8.7711,  6.7113],\n",
       "         ...,\n",
       "         [ 5.7845,  2.2373, -1.7581,  ...,  7.5710,  6.2945,  7.3202],\n",
       "         [ 0.6089, -2.4909, -9.6903,  ...,  5.3638, -1.0447, -7.4556],\n",
       "         [ 2.8435,  2.3322, -9.9023,  ..., -4.1509,  4.6841,  3.3056]]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [[ 0.0289, -9.2388, -2.7697,  ...,  3.7265,  7.4056, -7.2529],\n",
       "         [-3.8474,  0.0260, -5.2023,  ..., -2.2760,  0.7926, -2.2798],\n",
       "         [ 5.2844,  8.4921,  7.4326,  ...,  7.6248,  8.8788, -2.4832],\n",
       "         ...,\n",
       "         [ 7.5972, -3.7258, -9.7206,  ..., -6.2599, -0.9862,  6.3521],\n",
       "         [ 3.0360, -2.1432,  2.7521,  ..., -8.0970, -7.5317, -8.0158],\n",
       "         [ 9.4780,  2.5204,  3.9605,  ...,  9.3295,  1.7193,  2.0979]]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [-2.0595,  9.8159, -9.0939, -1.2249,  8.2528,  5.0410,  0.2210,\n",
       "          2.1543, -3.5500,  8.1928, -1.9054,  8.2123,  5.4995, -2.5731,\n",
       "          2.8665, -0.6538,  8.3866,  3.0280,  3.9128, -2.1742,  8.5389,\n",
       "         -0.8584,  9.9270, -5.1127,  8.5558,  8.7461, -7.4120,  9.7643,\n",
       "         -4.1880,  7.2269, -9.0685, -2.4135, -8.9277,  9.5051, -5.0040,\n",
       "         -8.4353,  0.4425, -8.4910,  4.3618, -6.8033,  1.1184,  6.8683,\n",
       "          5.0428, -1.9263,  4.6503, -0.7769, -1.1145,  3.5286, -2.0654,\n",
       "         -4.6404, -2.0910,  5.5677,  1.9741, -2.2095,  4.0528, -7.1120,\n",
       "         -8.0313, -1.0063,  1.4740,  9.2399,  2.2588, -4.3819,  3.6229,\n",
       "         -9.3798, -4.3168, -2.1573, -4.9670,  5.2720,  0.5227,  4.2444,\n",
       "          7.5281, -3.8222, -7.3130, -7.1504, -1.5996,  0.4443, -2.1434,\n",
       "          3.2910, -5.7901, -0.8647, -8.9355, -6.2242, -3.2976,  7.5097,\n",
       "         -0.5212, -8.6226,  4.5219,  3.6492,  2.7357, -6.1787,  4.2176,\n",
       "          7.3092, -2.3681, -5.5308,  8.5563, -3.6006, -5.8223, -7.8686,\n",
       "          9.9829,  0.4467, -4.1343,  9.1240, -1.3707,  4.1270, -8.4779,\n",
       "         -5.4934,  7.9779,  3.8203, -5.9489, -9.5673, -4.4093, -9.7901,\n",
       "          7.2212, -4.2925,  8.4999, -0.1979, -5.4787, -1.2281,  7.5031,\n",
       "         -1.8212, -5.4545, -1.1825,  8.1973, -7.3934, -9.3735, -8.2557,\n",
       "          9.5907,  0.0094,  6.8170, -7.2418,  8.9871, -6.3777,  8.4697,\n",
       "         -1.3506, -3.4803,  0.3572, -9.8659,  9.7725,  2.3446, -7.6184,\n",
       "         -0.0759,  7.8898,  2.8782, -2.0830,  5.4369, -1.8430,  9.8200,\n",
       "         -1.7475, -5.3223, -6.4997,  9.5100, -1.6407,  5.2253, -8.1110,\n",
       "         -8.2790, -8.4360,  8.5427,  1.3312, -2.0169,  8.2265, -3.4138,\n",
       "          3.5244, -8.7929, -4.3564,  4.6698,  9.8563, -4.4161, -2.6785,\n",
       "          4.7352, -2.4356,  2.7931,  1.3804,  5.6269,  4.1216,  1.8598,\n",
       "         -2.9473,  6.4562, -6.9617,  4.0911,  0.6041, -9.6858, -4.2259,\n",
       "          6.1663, -1.9740, -6.2587,  6.5328, -7.0592, -1.9064,  4.6794,\n",
       "         -5.1457,  4.9028, -1.1730, -2.4686, -5.4393, -2.8796, -4.4212,\n",
       "         -6.1490, -0.0137,  5.8060, -0.2571,  4.9346,  4.3863, -0.5795,\n",
       "         -8.1028,  1.3572, -0.2643,  8.8116, -5.6071, -1.6906, -7.9218,\n",
       "          1.8240, -3.3683, -0.8844,  6.0641, -1.5149,  2.4480, -8.0748,\n",
       "         -9.9991,  3.1418,  6.6258,  6.6944,  6.2663,  7.4268,  5.2436,\n",
       "         -3.2244, -2.7986,  1.3372,  8.6857,  3.0865, -9.6401, -5.4483,\n",
       "          7.7323,  4.7834,  3.1930,  1.2575, -2.0181,  1.4608, -6.0763,\n",
       "          5.6025, -9.5956,  9.5244,  2.0512,  4.4422, -1.3190, -6.3544,\n",
       "         -5.9039,  5.1397,  6.8875, -4.9423,  7.4699, -0.7369, -0.8023,\n",
       "         -0.3062,  5.0479,  3.2547,  5.5197, -7.4828, -1.4201,  8.7153,\n",
       "          4.0514, -2.0695,  1.5118, -0.1022,  6.7395,  8.1976,  8.6128,\n",
       "         -7.6969, -3.0561,  7.8595,  1.7298, -5.8619, -5.1970,  3.5213,\n",
       "          1.5628, -8.5722,  0.8499,  0.9299, -9.2672, -6.6466,  2.9761,\n",
       "         -2.3238,  3.9721, -4.2762,  8.4082,  8.0187, -8.2546, -1.4901,\n",
       "         -9.6888, -5.9214, -7.6241, -5.8389,  3.5223, -8.0277, -4.0994,\n",
       "         -1.2968,  2.3608,  5.4769,  9.6585, -4.0144,  4.2121,  3.2430,\n",
       "          2.7585, -3.2207, -5.2774, -8.0872, -2.1439, -2.9521, -7.6086,\n",
       "         -4.2926, -7.3532,  5.9503,  9.7962, -7.5407, -6.4954,  7.1314,\n",
       "          7.5853, -7.9329, -1.6584,  6.6482, -9.9054, -7.6136, -6.4350,\n",
       "         -8.0522, -6.8536, -7.6750, -2.5771, -7.4409, -3.6152, -5.1710,\n",
       "          6.0902,  8.6535,  0.2933,  1.9100, -3.7132,  8.3516,  5.5671,\n",
       "         -0.7182, -7.3598, -6.9991, -5.5458,  5.7658, -9.9031,  2.7330,\n",
       "          5.5999, -3.4931,  1.7174, -1.8904,  1.7291,  2.0705,  8.7542,\n",
       "          7.8828,  7.1231,  0.6042,  3.1300,  7.5735,  7.6466, -9.6220,\n",
       "         -6.4480, -5.8253, -5.1354,  7.5797,  5.2985, -8.6765, -5.9898,\n",
       "         -4.8250,  6.3804,  1.8531,  3.9604, -5.1613, -8.2137, -9.1147,\n",
       "         -4.6804,  2.2776, -6.3003, -2.1330,  7.9287, -6.6873, -9.9689,\n",
       "          3.4856, -6.4544, -3.9784,  6.2652, -7.1402, -4.1390,  2.8277,\n",
       "          2.7651,  5.9869,  8.8012,  2.7921,  2.4421,  7.8807,  2.1834,\n",
       "         -2.7351,  2.4268, -3.5439,  8.6126,  7.3345, -0.8446, -0.5434,\n",
       "          0.5490]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [ 7.3999, -8.8253,  4.6585,  8.6014, -3.1130,  0.1467,  2.9439,\n",
       "         -4.9898,  6.8348,  6.8688, -4.6490, -3.4751, -1.7681, -7.3612,\n",
       "          0.5028, -7.6583, -6.9232, -2.8801,  1.5934, -5.4184,  3.2050,\n",
       "         -7.2072,  7.3303, -8.7610, -9.1442,  8.8351,  3.3302,  2.2124,\n",
       "         -3.8361,  7.0364,  1.3697,  8.4967, -2.1301,  6.3535, -4.8746,\n",
       "          8.1279,  9.1119,  3.5925, -8.1315,  7.3222,  6.7298,  1.1059,\n",
       "         -5.6797, -2.3465,  3.1233, -6.8131, -0.3283,  9.6497, -4.1140,\n",
       "          3.8767, -3.7059,  9.8005, -5.5189,  1.9639,  0.0457, -8.6894,\n",
       "         -4.2934, -2.9530,  8.9667,  1.7518, -6.5270, -6.8530, -2.3651,\n",
       "          5.0536, -4.7441, -5.6697,  8.8736,  0.9631,  1.7823, -7.0596,\n",
       "         -0.9723,  7.6802, -5.9674, -0.4604,  1.8573, -7.1102,  7.9371,\n",
       "          1.5568,  7.1713,  4.3042,  0.8472,  7.3925,  1.3600, -7.3043,\n",
       "         -9.1437, -6.3277, -7.2554,  8.1310, -2.3297, -5.4835, -1.8063,\n",
       "         -1.1593, -5.6944, -2.2266,  6.8362,  6.7661,  3.6391,  4.1578,\n",
       "          7.3344,  9.0514,  4.2033,  7.5645,  7.8410,  0.8344,  4.6851,\n",
       "         -3.5981, -2.6358,  9.7144,  3.8008, -5.0403,  1.2724, -7.8634,\n",
       "         -4.0393,  9.3686, -7.4668, -3.1013,  5.3439, -3.5585,  9.1920,\n",
       "          3.9301,  6.5536, -9.4275,  6.4082,  5.6995, -3.4315,  9.8644,\n",
       "         -3.4523, -8.7353,  1.4212,  9.6139,  6.3623,  9.8158, -9.2468,\n",
       "         -2.8094, -3.8785, -8.3632, -9.1223, -1.2587,  4.6679,  7.7212,\n",
       "         -8.9563,  1.9814, -4.8819, -0.2091,  9.9305, -1.2085,  4.7667,\n",
       "         -7.0617, -9.8463, -1.5636, -0.1088,  3.0518, -0.4769, -6.2081,\n",
       "          4.5794,  1.3589, -1.0278, -2.3794, -6.3520,  1.8490, -2.1229,\n",
       "         -1.2676,  4.0944,  0.1985,  6.2731, -2.6658,  6.2398,  2.3387,\n",
       "          9.8692, -3.4971,  6.2453,  3.3140, -8.7536,  9.3421,  1.3837,\n",
       "         -3.0878,  7.3781, -2.6305,  8.7149,  4.1507, -0.5210,  9.3099,\n",
       "         -5.7583, -2.3409, -1.9285,  0.1755,  7.5030, -2.4323, -4.8324,\n",
       "         -9.2613,  5.5427,  5.9776, -5.0203, -0.6765,  0.1997, -9.1651,\n",
       "          7.0238, -7.4754, -2.8875, -4.8639,  4.8454, -3.5600,  0.8732,\n",
       "          7.0082, -1.3825,  2.6321, -5.7561,  4.7034,  7.0775,  9.2685,\n",
       "          5.4381,  7.1379,  0.0996, -8.3596,  5.0618, -9.2217,  0.3104,\n",
       "          8.8106, -4.4334, -7.2989, -7.2445,  9.4186,  4.0164,  0.3951,\n",
       "         -3.1164,  6.4040, -1.9668, -8.4934, -6.7777,  5.0377,  5.6068,\n",
       "          4.6168,  0.2203, -0.6396, -1.3871,  7.9783,  7.1219, -2.0903,\n",
       "          4.7123,  4.4267,  6.0478,  1.6023,  5.9909,  2.2462, -9.3411,\n",
       "          6.9711, -7.4486,  2.0580,  6.5508,  2.7019,  2.0563, -8.2528,\n",
       "         -0.0231, -1.2642,  5.9334, -5.2609, -0.2190,  5.9556,  0.9455,\n",
       "          3.8549,  6.6791, -7.7295,  3.9073,  5.0381,  1.9867, -3.8474,\n",
       "          6.6672,  5.3235, -5.7530, -4.8426, -6.6473, -5.0496,  1.4717,\n",
       "         -8.8010,  2.9305,  8.4956,  7.6528, -0.0859,  1.2361, -6.5319,\n",
       "          9.1399,  5.3133, -4.3493,  7.7204,  9.5643, -7.4146,  4.9076,\n",
       "          1.0705, -1.0044, -6.2458,  0.8208, -0.6782, -0.2929, -6.5209,\n",
       "          2.7549,  2.9378,  0.6514,  8.9202,  1.3379, -5.9792,  8.2582,\n",
       "          1.9566,  4.9640, -3.8827, -7.9123, -8.1519,  2.3266,  5.7375,\n",
       "         -6.8914, -7.1692,  6.7277, -3.7751, -0.3152,  7.2804, -3.1277,\n",
       "          0.5939, -1.2063, -2.5353, -1.0809, -3.7782, -3.3205,  5.3134,\n",
       "          6.4691,  6.7448, -2.0423,  6.3074,  5.5375,  3.3386, -4.2493,\n",
       "          0.8625,  3.6666, -2.4889, -7.6753, -8.3506,  4.3770, -0.0156,\n",
       "         -7.4798, -3.0190, -5.2787, -0.0167,  5.4043,  6.2373, -7.7994,\n",
       "          3.8294,  7.6538,  4.6504,  7.7404,  4.0857,  6.2723,  4.2667,\n",
       "         -1.1538, -1.9949, -4.4474, -8.5693,  8.4686, -2.6838, -5.3456,\n",
       "         -1.6491, -2.7309, -1.7734, -4.6247, -7.1821,  1.6037,  3.0778,\n",
       "          1.3982,  7.7993, -5.1071, -0.9103,  9.7604,  1.7663,  4.7298,\n",
       "         -2.6096, -3.2358, -2.1870, -1.5423,  2.0742,  9.3277, -9.2634,\n",
       "          8.2372, -2.3360,  7.7485,  8.3505, -3.8250, -0.8141,  5.3011,\n",
       "         -1.0220,  3.1489,  8.0967, -3.9889,  7.6268, -2.1349, -8.4346,\n",
       "         -7.2392, -3.4184, -3.2127,  3.2910, -2.4219,  0.9774,  9.0936,\n",
       "          3.1722]), Parameter containing:\n",
       " tensor([[ 2.2092e-02, -9.7773e-03, -3.9022e-02,  ...,  4.4060e-02,\n",
       "          -4.5047e-02,  2.3048e-02],\n",
       "         [ 1.6588e-02,  4.3004e-03, -2.7604e-02,  ...,  4.4992e-02,\n",
       "           4.2222e-02,  2.0130e-02],\n",
       "         [ 4.6215e-02,  1.3170e-02,  2.5931e-02,  ...,  5.5720e-02,\n",
       "           6.6301e-03, -4.9029e-02],\n",
       "         ...,\n",
       "         [ 4.2498e-02, -2.6950e-02,  5.6923e-02,  ..., -4.0493e-04,\n",
       "           3.7304e-02, -1.1478e-02],\n",
       "         [-2.7762e-02, -2.1757e-02, -1.1559e-02,  ..., -6.3955e-03,\n",
       "           2.2975e-02,  2.4268e-02],\n",
       "         [ 5.7394e-03, -1.0600e-02, -2.7224e-02,  ...,  4.5979e-02,\n",
       "           3.7213e-02,  1.4144e-02]]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [ 1.1221,  3.9157, -5.5437, -1.2160,  1.4456,  2.7206, -1.8317,\n",
       "          0.9148, -0.7966, -5.8476, -5.9520,  5.5524, -2.9231, -2.5870,\n",
       "          1.6655, -4.2521,  3.8840, -4.5530, -4.1804,  1.1260,  2.2311,\n",
       "         -5.3541,  1.5375, -0.2792, -5.7907,  4.2148,  0.4456,  3.5807,\n",
       "         -4.0389,  2.5780, -5.2190,  5.2438,  6.7270, -4.3239,  2.9755,\n",
       "          0.3478,  4.9026,  0.3190, -4.7168, -3.9216, -4.3709,  2.4160,\n",
       "         -6.3446,  1.6037,  3.0835, -2.1711, -5.8857, -4.4534, -3.7183,\n",
       "          2.8174,  5.5002,  1.0738, -5.0481,  0.2045,  2.1896,  0.6712,\n",
       "          4.6162,  2.8277, -1.6745,  3.7228,  1.3246,  2.3330, -0.4037,\n",
       "          1.2380,  3.3047, -4.3798,  0.0755, -5.7407,  3.6136, -2.4906,\n",
       "         -3.4098,  0.5147, -2.3546,  3.4652,  5.8689,  0.5871,  3.1604,\n",
       "          4.4926,  1.0266,  3.9627,  2.8529,  3.7288,  6.0593,  5.0880,\n",
       "         -2.3333,  3.1030, -6.1953,  8.0336,  0.6728,  4.3199,  1.7462,\n",
       "          1.0232,  4.1597,  4.0579,  3.6353, -4.7108,  5.1576,  3.3904,\n",
       "         -2.0260,  3.6854]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [[ 5.1565,  7.6491, -3.0523, -8.7479,  5.7395, -3.4506, -2.0181,\n",
       "           7.0355, -2.2082,  0.7843,  3.0694,  0.5726, -2.8564,  3.1228,\n",
       "          -1.2728,  3.7831,  0.1789,  0.7136,  9.0241,  6.4964,  4.4790,\n",
       "           2.3460, -5.3444,  9.1051, -4.3441, -8.0184, -5.1513, -0.2977,\n",
       "           1.3644,  3.7388,  5.1209, -9.7216,  0.3625, -8.0368,  5.3373,\n",
       "           2.7594,  6.0179, -8.7395,  6.9683, -7.6751,  4.0476,  2.9524,\n",
       "           5.9543, -2.8175,  3.0816, -2.2473,  5.9234, -5.9193, -3.5462,\n",
       "          -6.6326, -0.3447,  0.9031, -0.5287,  1.1194, -2.3778, -4.5068,\n",
       "           3.1965,  6.5832, -0.8996, -1.9375, -8.1412, -0.5329,  1.9637,\n",
       "          -7.2309, -2.3861, -5.5412, -3.5411,  8.1738, -3.2067, -7.1992,\n",
       "           6.4375,  4.2202,  3.0893,  2.2679,  4.1589, -2.7489,  0.0016,\n",
       "          -4.9713, -3.4911, -4.8838,  2.4627, -6.9580,  1.6516,  0.3130,\n",
       "          -5.8743, -2.0448,  5.7684,  0.1333,  6.5747,  2.1519, -1.8325,\n",
       "          -2.6370,  7.2259,  4.9500,  3.7798, -6.7908, -3.7048, -0.7735,\n",
       "           6.6681, -5.5224]]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [ 2.5299]), Parameter containing:\n",
       " tensor([[ 8.5851e-03,  7.4225e-02, -2.9045e-02,  ..., -2.6698e-02,\n",
       "           3.2509e-02, -3.4864e-02],\n",
       "         [ 6.7141e-02,  9.3832e-02, -6.2831e-02,  ...,  4.9756e-03,\n",
       "          -6.0770e-02,  2.5238e-02],\n",
       "         [-4.4502e-02,  1.0011e-01,  5.3912e-02,  ...,  1.1588e-02,\n",
       "           1.1542e-01,  8.5323e-03],\n",
       "         ...,\n",
       "         [-4.9948e-02,  1.1975e-02, -6.6641e-02,  ...,  9.0063e-02,\n",
       "          -2.3723e-02,  1.1663e-01],\n",
       "         [-6.4443e-02,  4.2658e-02, -6.6975e-02,  ..., -4.0053e-02,\n",
       "           2.8229e-02, -3.0266e-02],\n",
       "         [ 1.0200e-02,  4.6490e-02,  1.0794e-02,  ...,  8.7890e-02,\n",
       "          -1.0115e-01, -3.5544e-02]]), Parameter containing:\n",
       " tensor([[ 2.4332e-02,  7.6425e-02, -1.4215e-02,  ..., -6.0225e-02,\n",
       "           2.0480e-02,  2.3055e-02],\n",
       "         [ 5.5987e-02, -2.8765e-02, -8.5927e-02,  ...,  9.2675e-03,\n",
       "           1.0590e-01,  5.4131e-02],\n",
       "         [-5.8766e-02,  1.0554e-01,  1.4545e-02,  ..., -7.8323e-02,\n",
       "           3.7572e-02, -1.5359e-02],\n",
       "         ...,\n",
       "         [-1.3215e-02, -6.0789e-02,  1.3203e-02,  ...,  2.3092e-02,\n",
       "          -7.1825e-02, -3.8607e-02],\n",
       "         [-1.1222e-02,  2.8036e-02, -7.0750e-02,  ..., -7.4291e-02,\n",
       "           6.6213e-02, -3.7160e-02],\n",
       "         [-6.7121e-02,  7.3530e-02, -7.7808e-03,  ..., -2.3980e-02,\n",
       "          -8.0998e-02,  7.9168e-02]]), Parameter containing:\n",
       " tensor([ 0.0386, -0.0393, -0.0321,  0.0059, -0.0816, -0.0168, -0.0266,\n",
       "         -0.0335,  0.0507, -0.0117, -0.0713, -0.0827,  0.1039,  0.0588,\n",
       "          0.0227, -0.0777, -0.0316,  0.0831, -0.0288,  0.0699, -0.0475,\n",
       "         -0.0034,  0.0438, -0.0441,  0.0777,  0.0897,  0.0159, -0.0212,\n",
       "         -0.0095, -0.0697,  0.0464, -0.0700,  0.0439, -0.0389,  0.0607,\n",
       "          0.0105, -0.0367,  0.1052,  0.0828,  0.0634,  0.0556,  0.0997,\n",
       "         -0.0634, -0.0236,  0.0621,  0.0333,  0.0688, -0.0702,  0.0798,\n",
       "          0.0872,  0.0275, -0.0625,  0.0079, -0.0044, -0.0117, -0.0709,\n",
       "         -0.0594, -0.0773, -0.0815,  0.0665, -0.0523, -0.0659,  0.1167,\n",
       "          0.0456, -0.0327, -0.0975,  0.0547, -0.0139, -0.0649,  0.0176,\n",
       "         -0.0546, -0.0033,  0.0792,  0.0063,  0.0547, -0.0314,  0.0272,\n",
       "         -0.0122,  0.0214, -0.0148,  0.0637,  0.0707, -0.0001,  0.0223,\n",
       "         -0.0546,  0.0956, -0.0736,  0.0535,  0.0852, -0.0370,  0.0486,\n",
       "         -0.0641,  0.0291,  0.0738, -0.0968,  0.0723, -0.0628, -0.0429,\n",
       "          0.0642,  0.0537,  0.0480,  0.0633, -0.0621, -0.0856,  0.0616,\n",
       "          0.0966, -0.0269,  0.0451, -0.0145,  0.0347, -0.0076,  0.0642,\n",
       "          0.0462,  0.0799, -0.0784,  0.0080,  0.0989,  0.1154,  0.0490,\n",
       "         -0.0275,  0.0797,  0.0628, -0.0222, -0.0477,  0.0384,  0.0573,\n",
       "          0.0244,  0.0690, -0.0150,  0.0694,  0.0015, -0.0052,  0.0483,\n",
       "          0.0914,  0.0288, -0.0918,  0.0147, -0.0804, -0.0924, -0.0328,\n",
       "          0.0992, -0.0614,  0.0183,  0.0593, -0.0717,  0.0423,  0.0634,\n",
       "         -0.0421, -0.0155, -0.0390, -0.0759,  0.0430, -0.0186,  0.0286,\n",
       "          0.0812, -0.0570,  0.0298, -0.0056, -0.0011,  0.0362, -0.0374,\n",
       "          0.0218,  0.0698, -0.1005,  0.1125,  0.0752, -0.0256,  0.0549,\n",
       "          0.1009, -0.0266, -0.0148,  0.0126, -0.0553, -0.0203, -0.0036,\n",
       "         -0.0564, -0.0567,  0.0674, -0.1138, -0.0470,  0.0508, -0.1174,\n",
       "          0.0094, -0.0193,  0.0059, -0.0006, -0.0379,  0.0190, -0.0535,\n",
       "         -0.0931,  0.0226,  0.0740,  0.0479, -0.0440, -0.0486, -0.0154,\n",
       "          0.0366, -0.0486, -0.0046, -0.0171,  0.0495, -0.0846, -0.0259,\n",
       "         -0.0854, -0.0834,  0.0542,  0.0286, -0.0550, -0.0854, -0.0188,\n",
       "         -0.1005, -0.0579, -0.0795, -0.0301,  0.0932, -0.0216, -0.0307,\n",
       "          0.0772, -0.0319,  0.0600,  0.1116,  0.0043, -0.0612,  0.1065,\n",
       "         -0.1056,  0.0572, -0.0018,  0.0618, -0.0229,  0.0688, -0.0480,\n",
       "         -0.0019,  0.0240,  0.0775,  0.1007,  0.0450,  0.0005,  0.0333,\n",
       "         -0.0788, -0.0385,  0.0671,  0.0724, -0.0482, -0.0440,  0.0164,\n",
       "          0.0766, -0.0228,  0.0496,  0.0015,  0.0511, -0.0363,  0.0387,\n",
       "         -0.0643, -0.0600,  0.1075,  0.0035, -0.0786, -0.0491,  0.0011,\n",
       "         -0.0246, -0.0924, -0.0454,  0.0075, -0.0612, -0.0176, -0.0244,\n",
       "         -0.0390,  0.1004, -0.0189, -0.0132, -0.0807, -0.0647, -0.0782,\n",
       "         -0.0694,  0.0583, -0.0448, -0.0307, -0.0159,  0.0946, -0.0758,\n",
       "         -0.0416,  0.0373,  0.0436,  0.0643,  0.0122, -0.0635, -0.1134,\n",
       "          0.1039, -0.0758,  0.0254, -0.0611, -0.0643,  0.0172,  0.0282,\n",
       "          0.0737, -0.0927,  0.0818, -0.0488,  0.0197,  0.0403,  0.0364,\n",
       "          0.0851, -0.0731,  0.0915,  0.0819, -0.0724, -0.0538, -0.0138,\n",
       "         -0.0143, -0.0617, -0.0270, -0.0215, -0.0582,  0.0401,  0.0276,\n",
       "         -0.0783, -0.0637,  0.0001, -0.0062, -0.0683,  0.0412,  0.0785,\n",
       "         -0.0434, -0.0223,  0.0464,  0.1122, -0.0413, -0.0661,  0.0935,\n",
       "          0.0359, -0.0056,  0.0162, -0.0440,  0.0614,  0.0513, -0.0663,\n",
       "          0.0822,  0.0138,  0.0696, -0.0401,  0.0545, -0.0061,  0.0371,\n",
       "         -0.0241,  0.0689, -0.0399,  0.0215,  0.0013, -0.0355, -0.0647,\n",
       "          0.0242,  0.0918,  0.0572, -0.0440,  0.0831,  0.0120, -0.0767,\n",
       "          0.1058,  0.0072,  0.0571, -0.0214,  0.0417, -0.0755, -0.0316,\n",
       "          0.0939, -0.0020,  0.0103,  0.1122,  0.0494, -0.0506, -0.0008,\n",
       "         -0.0100,  0.0281,  0.0497,  0.0745, -0.0281, -0.0160,  0.0531,\n",
       "         -0.0386, -0.0214, -0.0302,  0.0716, -0.0905,  0.0927,  0.0857,\n",
       "          0.0779,  0.0800,  0.0102,  0.0205, -0.0635, -0.0021,  0.0610,\n",
       "          0.0798,  0.0260, -0.0219,  0.0687, -0.0688, -0.0017, -0.0510,\n",
       "         -0.0170]), Parameter containing:\n",
       " tensor([ 0.0935, -0.0447, -0.0295, -0.0841,  0.0983,  0.0318, -0.0213,\n",
       "          0.0287, -0.0168,  0.1052,  0.0663, -0.0549, -0.0809, -0.0303,\n",
       "          0.0889, -0.0191, -0.0455, -0.0267, -0.0344,  0.0274, -0.0222,\n",
       "          0.0461,  0.0751, -0.0594, -0.0331,  0.0468,  0.0797,  0.0330,\n",
       "         -0.0783,  0.0591, -0.0849,  0.0401, -0.0572, -0.0286, -0.0703,\n",
       "          0.1085, -0.0650,  0.0233, -0.0467,  0.0403, -0.0221, -0.0417,\n",
       "          0.0727,  0.0153,  0.0400,  0.0891, -0.0098,  0.0004,  0.0545,\n",
       "         -0.0605, -0.1033, -0.0702, -0.0121,  0.0835,  0.0074, -0.0712,\n",
       "          0.0478, -0.0170,  0.0075,  0.0457, -0.0221, -0.0800, -0.0754,\n",
       "          0.0237, -0.0315,  0.0241, -0.0236, -0.0397,  0.0565, -0.0945,\n",
       "          0.0927,  0.0780, -0.0405,  0.0838,  0.0431,  0.0366,  0.0060,\n",
       "         -0.0682,  0.0039, -0.0092, -0.0265,  0.0452, -0.0796, -0.0873,\n",
       "         -0.0502, -0.0637, -0.0116,  0.0622, -0.0613, -0.0329,  0.0507,\n",
       "         -0.0611,  0.0765,  0.0329,  0.0355, -0.0097, -0.0624,  0.0463,\n",
       "          0.0670, -0.0055, -0.0481,  0.1115,  0.0113,  0.0259,  0.0064,\n",
       "          0.0660,  0.0279, -0.1039,  0.1149,  0.0033,  0.0390, -0.0184,\n",
       "         -0.0661,  0.0285,  0.0972,  0.0104,  0.1083, -0.0144, -0.0769,\n",
       "         -0.0867, -0.0016, -0.0399, -0.0199, -0.0628, -0.0497,  0.0317,\n",
       "         -0.0328, -0.1174,  0.0319, -0.0201, -0.0455,  0.0553, -0.0115,\n",
       "         -0.0703,  0.0396, -0.0558,  0.0609, -0.0920,  0.0233, -0.0462,\n",
       "         -0.0525, -0.0955,  0.0026, -0.0693, -0.0836,  0.0275, -0.0170,\n",
       "         -0.0460,  0.0992, -0.0619, -0.0556,  0.0261,  0.0462, -0.0489,\n",
       "         -0.0108,  0.0462, -0.0130, -0.0434,  0.0657, -0.0375,  0.0002,\n",
       "         -0.0675,  0.0938,  0.0097,  0.0640,  0.0420, -0.1075, -0.0357,\n",
       "         -0.0702,  0.0664, -0.0020, -0.0470, -0.0765, -0.0109,  0.0609,\n",
       "         -0.0991,  0.1041,  0.0888, -0.0965,  0.0049,  0.0167,  0.0782,\n",
       "         -0.0225, -0.0370, -0.0645,  0.0767,  0.0380,  0.0635, -0.0194,\n",
       "         -0.0314, -0.0745,  0.0201,  0.0704,  0.0406, -0.0191,  0.0268,\n",
       "         -0.0104,  0.0288,  0.0749,  0.0090, -0.0049,  0.0130, -0.0649,\n",
       "         -0.0369, -0.0462,  0.0783,  0.0787, -0.0143,  0.0002, -0.0472,\n",
       "          0.0171, -0.0119,  0.0466, -0.0130,  0.0452, -0.0738, -0.0615,\n",
       "         -0.0554,  0.1104, -0.1087,  0.0902,  0.0484, -0.0950,  0.0190,\n",
       "         -0.0411,  0.0924,  0.0686, -0.0349,  0.0988, -0.0083,  0.0064,\n",
       "         -0.0630,  0.0286, -0.0159,  0.0839, -0.0561, -0.0649,  0.0399,\n",
       "         -0.0989, -0.0406, -0.0789, -0.0329, -0.0001, -0.0956, -0.0678,\n",
       "         -0.0739,  0.0530,  0.0682,  0.0512, -0.0779, -0.0746,  0.0314,\n",
       "         -0.0856, -0.0138,  0.0215,  0.0305,  0.0153, -0.0543, -0.0476,\n",
       "          0.0107,  0.1023,  0.0192,  0.0211,  0.0084, -0.0401, -0.0729,\n",
       "          0.0101, -0.0169,  0.0797, -0.0559,  0.0151, -0.0683, -0.0437,\n",
       "          0.0358, -0.0362,  0.0120,  0.0769, -0.1040,  0.0516, -0.0113,\n",
       "          0.0747, -0.0263, -0.0797, -0.0260, -0.0544, -0.0164,  0.0548,\n",
       "          0.0428, -0.0393, -0.0567,  0.0209, -0.0552,  0.1100,  0.1001,\n",
       "          0.0458,  0.0543, -0.0698,  0.0575,  0.0991,  0.0632,  0.0335,\n",
       "          0.1127, -0.0222, -0.0968, -0.0765,  0.0786,  0.0371, -0.0615,\n",
       "          0.1125,  0.0325,  0.0922,  0.0418,  0.0591,  0.1081, -0.0667,\n",
       "         -0.0377, -0.0021,  0.0838,  0.0358, -0.0763, -0.0741, -0.0722,\n",
       "          0.0294, -0.0764, -0.0653,  0.0511, -0.0448, -0.0284,  0.0745,\n",
       "          0.0677, -0.0217, -0.0179, -0.0423, -0.0796,  0.0631,  0.0867,\n",
       "         -0.0811,  0.0650,  0.0608, -0.0637, -0.0774,  0.0836,  0.0849,\n",
       "         -0.0931, -0.0426, -0.0857,  0.0086, -0.0045,  0.0386,  0.0547,\n",
       "         -0.0454,  0.0401, -0.0160, -0.0155,  0.0541,  0.0357, -0.0476,\n",
       "         -0.0324,  0.0754, -0.0282,  0.0715, -0.0035,  0.0375,  0.0708,\n",
       "          0.0131, -0.0880, -0.0415,  0.0051,  0.0733, -0.0054, -0.0366,\n",
       "         -0.0579, -0.0200,  0.0863,  0.0543, -0.0102,  0.0469, -0.0873,\n",
       "         -0.0611, -0.0756,  0.0363, -0.0745,  0.0228, -0.0691,  0.0694,\n",
       "          0.0201, -0.0364,  0.0325,  0.0475,  0.0466,  0.1005, -0.0828,\n",
       "          0.0489,  0.0105, -0.0731,  0.1095,  0.0607,  0.0910,  0.0515,\n",
       "          0.0853]), Parameter containing:\n",
       " tensor([[-0.0955,  0.0678,  0.0523, -0.0281,  0.0609,  0.0393, -0.0727,\n",
       "          -0.0116,  0.0967,  0.0560,  0.0086,  0.0798, -0.0774, -0.0928,\n",
       "          -0.0924, -0.0008,  0.0086, -0.0761, -0.0485, -0.0498, -0.0308,\n",
       "           0.0344,  0.0484, -0.1017, -0.0486,  0.0618,  0.0198,  0.0132,\n",
       "           0.0265,  0.0050, -0.0876,  0.0781, -0.0812, -0.0026,  0.0177,\n",
       "           0.0201,  0.0285,  0.0858,  0.0292,  0.0994,  0.1136, -0.0124,\n",
       "           0.0354,  0.0664, -0.0448,  0.0329, -0.0984,  0.0109, -0.0932,\n",
       "          -0.0024, -0.0818, -0.0806,  0.0556, -0.0349,  0.0848, -0.0445,\n",
       "           0.0264,  0.0809,  0.0702, -0.0663, -0.0396,  0.1087, -0.0012,\n",
       "           0.0372, -0.0447, -0.0938,  0.0284, -0.0101,  0.0630,  0.0105,\n",
       "           0.0571,  0.0530, -0.0042, -0.0233,  0.0416, -0.0625, -0.0481,\n",
       "           0.0742, -0.0347,  0.0182, -0.0307,  0.0245,  0.0512,  0.0224,\n",
       "           0.0627,  0.0829,  0.0360, -0.0118, -0.0463, -0.0312,  0.0359,\n",
       "           0.1109, -0.0273, -0.0800, -0.0301, -0.0660,  0.0743,  0.0009,\n",
       "          -0.0157,  0.0621],\n",
       "         [-0.0963, -0.0265,  0.0161,  0.0399,  0.0768,  0.0410, -0.0027,\n",
       "           0.0012, -0.0035,  0.0713, -0.0041, -0.0749, -0.0553,  0.0619,\n",
       "           0.0318, -0.1119, -0.0294,  0.0795,  0.0496, -0.0456,  0.0564,\n",
       "           0.0817, -0.0775, -0.0934,  0.0048,  0.0554,  0.1094, -0.0802,\n",
       "          -0.0770,  0.0663, -0.0179, -0.0840, -0.0071, -0.0707, -0.0888,\n",
       "           0.0207, -0.0787,  0.0769, -0.0607,  0.0926,  0.0365,  0.0687,\n",
       "          -0.0410, -0.0495, -0.0658, -0.0220,  0.0595,  0.0265, -0.0352,\n",
       "           0.0610, -0.0702,  0.1058,  0.0162, -0.0309, -0.1042,  0.0841,\n",
       "           0.0229,  0.0189,  0.0318, -0.0745, -0.0071,  0.0756,  0.0532,\n",
       "           0.0407, -0.0725, -0.0136, -0.0556, -0.0316, -0.0725, -0.0500,\n",
       "          -0.0221,  0.0715, -0.1057, -0.0674,  0.0075,  0.0443,  0.0520,\n",
       "          -0.0645, -0.0301,  0.0220, -0.0591,  0.0130,  0.0092,  0.0185,\n",
       "           0.0421, -0.0707,  0.0977, -0.0115, -0.0017,  0.0527, -0.0399,\n",
       "           0.0563, -0.0892, -0.0560, -0.0856,  0.0785,  0.0905,  0.0190,\n",
       "          -0.0390, -0.0151],\n",
       "         [-0.0201, -0.0738,  0.0418, -0.0749,  0.0345, -0.0367, -0.0105,\n",
       "           0.0663,  0.0878,  0.0214, -0.0063,  0.0712,  0.0659, -0.0353,\n",
       "          -0.0443, -0.0112,  0.0779,  0.0404, -0.0441,  0.0835, -0.0811,\n",
       "          -0.0807,  0.1099, -0.0218, -0.0677, -0.0427, -0.0451,  0.0048,\n",
       "          -0.0526, -0.0093,  0.0751, -0.0205,  0.0534, -0.0670,  0.0157,\n",
       "           0.0950, -0.0464, -0.0426, -0.1010,  0.0330,  0.0130,  0.1004,\n",
       "          -0.0155, -0.0787,  0.0499, -0.0615,  0.0734,  0.0471,  0.0794,\n",
       "           0.0090, -0.0760, -0.0646,  0.0213,  0.0357,  0.0896, -0.0415,\n",
       "          -0.0787,  0.0982, -0.0882,  0.0953,  0.0187,  0.0297, -0.0168,\n",
       "          -0.0851, -0.0943,  0.0424, -0.0249, -0.0700,  0.0172,  0.0160,\n",
       "          -0.0229, -0.0250, -0.0489,  0.0537,  0.0922,  0.0121, -0.1066,\n",
       "           0.0397, -0.1094,  0.0843,  0.0046,  0.0695,  0.0039, -0.0589,\n",
       "          -0.0571,  0.0426, -0.0290, -0.0175,  0.0123, -0.0663,  0.0026,\n",
       "          -0.0587, -0.0491, -0.1033, -0.0262,  0.1082, -0.0643, -0.0297,\n",
       "           0.0315,  0.0626],\n",
       "         [-0.0149, -0.0846,  0.0267, -0.0322,  0.0937, -0.0967,  0.0637,\n",
       "           0.0636, -0.0260,  0.0292, -0.0545, -0.0477, -0.0956, -0.0924,\n",
       "           0.0168, -0.0127, -0.0063, -0.0357, -0.0579,  0.0990, -0.0289,\n",
       "          -0.0729, -0.0288, -0.1050,  0.0266,  0.0616,  0.0739, -0.0917,\n",
       "          -0.0824,  0.0266,  0.0730,  0.0978,  0.0796,  0.1109, -0.1091,\n",
       "           0.1072,  0.0872, -0.0799, -0.0942, -0.0200,  0.0926,  0.0978,\n",
       "           0.0568, -0.0547,  0.0780,  0.0520, -0.0149,  0.0331, -0.0079,\n",
       "          -0.0797,  0.0005, -0.0534,  0.0868, -0.0599,  0.0604,  0.0164,\n",
       "           0.0664, -0.0185, -0.0217, -0.0603, -0.0239, -0.0012, -0.0556,\n",
       "           0.0169, -0.0426, -0.0814,  0.0658,  0.0128,  0.0753,  0.0309,\n",
       "          -0.0058,  0.0738,  0.0810,  0.0435, -0.0290, -0.0299, -0.0844,\n",
       "           0.0856,  0.0122,  0.0370,  0.0129,  0.0163,  0.0418,  0.0885,\n",
       "          -0.0496,  0.0787, -0.0250, -0.1116,  0.0005,  0.0090,  0.0454,\n",
       "          -0.0209, -0.0021,  0.0460,  0.0045,  0.0863, -0.0529, -0.0870,\n",
       "          -0.0657, -0.0799],\n",
       "         [ 0.0419,  0.0055, -0.0037, -0.0578,  0.0350,  0.0640,  0.0305,\n",
       "          -0.0636,  0.0569,  0.0810, -0.0651, -0.0348,  0.0917,  0.0188,\n",
       "           0.0958, -0.0739,  0.0668, -0.0557,  0.0759,  0.1103, -0.0824,\n",
       "          -0.0058, -0.0142, -0.0034,  0.0616, -0.1016,  0.0256, -0.0840,\n",
       "          -0.0474,  0.1017, -0.0940, -0.0300,  0.0348, -0.0185, -0.0579,\n",
       "           0.0049, -0.0301, -0.0671,  0.0014, -0.0705,  0.0022, -0.0143,\n",
       "           0.0178, -0.0366, -0.0400, -0.0782, -0.0408, -0.0231, -0.0068,\n",
       "          -0.1112, -0.0963,  0.0960,  0.0682,  0.0587,  0.0261,  0.0635,\n",
       "          -0.0475,  0.0828,  0.0498,  0.0340,  0.0196,  0.0803,  0.0366,\n",
       "           0.0731, -0.0896,  0.0473, -0.0628, -0.0298, -0.0226, -0.0922,\n",
       "           0.0614, -0.0556, -0.0103, -0.0481,  0.0374, -0.0491, -0.0610,\n",
       "          -0.0642, -0.0523, -0.0463, -0.1049, -0.0479,  0.0723,  0.0163,\n",
       "          -0.0774,  0.0780,  0.0504,  0.0278,  0.0208, -0.0488, -0.0639,\n",
       "          -0.0197, -0.0620,  0.0488,  0.0553,  0.0151, -0.0615,  0.0712,\n",
       "           0.0555,  0.0998],\n",
       "         [ 0.0419, -0.1026,  0.0350,  0.0704,  0.0797, -0.0651, -0.0051,\n",
       "          -0.1109,  0.0148, -0.0791, -0.0377, -0.0097, -0.0833, -0.0066,\n",
       "           0.0353,  0.0403, -0.0090,  0.0111, -0.0674, -0.0798,  0.0257,\n",
       "           0.0396,  0.0771, -0.0266, -0.0728, -0.0114, -0.0268, -0.0500,\n",
       "          -0.0898, -0.0775, -0.0927,  0.0309,  0.0697, -0.0322, -0.0190,\n",
       "           0.1100,  0.0478,  0.0720,  0.0247, -0.0150, -0.0377,  0.0543,\n",
       "          -0.0806,  0.0567,  0.0025, -0.0633, -0.0909, -0.0161,  0.0438,\n",
       "           0.0159, -0.0781,  0.0172, -0.0353, -0.0834,  0.0223, -0.0221,\n",
       "          -0.0292,  0.0872,  0.0756,  0.1015, -0.0544,  0.0452,  0.0913,\n",
       "          -0.0270,  0.0470, -0.0841,  0.0709,  0.0564, -0.0343,  0.0630,\n",
       "           0.0378,  0.0869, -0.0232, -0.0467, -0.0933,  0.0430, -0.0358,\n",
       "           0.0244,  0.0091,  0.1119, -0.0328, -0.0412,  0.0170, -0.0183,\n",
       "          -0.0398, -0.0351, -0.0488, -0.0595,  0.0066,  0.0172,  0.0095,\n",
       "          -0.0519,  0.0776, -0.0606,  0.0914, -0.0059, -0.0534, -0.0699,\n",
       "          -0.0691, -0.0293],\n",
       "         [ 0.0284, -0.0709,  0.0419, -0.0296, -0.0335,  0.0482, -0.0036,\n",
       "           0.0764, -0.0435,  0.0653, -0.0617,  0.0434, -0.0868, -0.0013,\n",
       "          -0.0673,  0.0313,  0.0024, -0.0933, -0.0963, -0.0680,  0.0190,\n",
       "           0.0887,  0.0581,  0.0597,  0.1073, -0.0995,  0.1009, -0.0555,\n",
       "          -0.0597,  0.0943,  0.0652, -0.0302,  0.0758,  0.0381,  0.0278,\n",
       "          -0.0538,  0.0943, -0.0631, -0.0206,  0.0953,  0.0131,  0.0312,\n",
       "           0.0484, -0.0851, -0.0740, -0.0048, -0.0131, -0.0978, -0.0487,\n",
       "           0.0600, -0.0147,  0.0144, -0.0037,  0.0383, -0.1023, -0.1026,\n",
       "          -0.0658, -0.0350, -0.0056, -0.0380,  0.0140, -0.0486, -0.0035,\n",
       "           0.1081, -0.1091,  0.0714, -0.1000,  0.0233,  0.0284, -0.0593,\n",
       "           0.0523,  0.0309, -0.0870, -0.0412, -0.0485, -0.0150, -0.0461,\n",
       "          -0.0248,  0.0454, -0.0727,  0.0934, -0.0555,  0.0888,  0.0810,\n",
       "           0.0619, -0.0596,  0.0984, -0.1018, -0.0718,  0.0204, -0.0267,\n",
       "           0.0936,  0.0222, -0.0469, -0.0130, -0.0086,  0.0118, -0.0344,\n",
       "          -0.0931, -0.0384],\n",
       "         [ 0.0496,  0.0889,  0.0434,  0.0023,  0.0929,  0.0273,  0.0602,\n",
       "          -0.0776, -0.0413, -0.0480, -0.0810,  0.0796, -0.0653,  0.0925,\n",
       "           0.0902,  0.0696, -0.0967,  0.0793, -0.0192, -0.0841, -0.0322,\n",
       "           0.0364,  0.0162,  0.0595, -0.0682,  0.0749, -0.1065, -0.1016,\n",
       "          -0.0096,  0.0164, -0.0154,  0.0619, -0.0808, -0.0512,  0.0420,\n",
       "          -0.0260,  0.0837, -0.0019, -0.0834,  0.0387, -0.1037,  0.0853,\n",
       "          -0.0953, -0.0460, -0.0045, -0.0242, -0.0830,  0.0937,  0.0640,\n",
       "           0.0129,  0.0778, -0.0652,  0.0935,  0.0696,  0.0953,  0.0685,\n",
       "           0.0259, -0.0955,  0.0317, -0.0923,  0.1082, -0.0636, -0.1022,\n",
       "           0.0374,  0.0122,  0.0438, -0.0716,  0.1088,  0.1096,  0.0168,\n",
       "           0.0095, -0.0416,  0.0599, -0.1071,  0.0940, -0.0765,  0.0114,\n",
       "          -0.0436,  0.0982, -0.0749,  0.0182,  0.0768,  0.0273,  0.0657,\n",
       "           0.0944, -0.0776,  0.0206,  0.0085,  0.0492,  0.0570, -0.0830,\n",
       "          -0.0682,  0.0254,  0.0070, -0.0917, -0.0303,  0.0666, -0.0522,\n",
       "           0.0721, -0.0357],\n",
       "         [ 0.0295, -0.0680,  0.1026,  0.0782,  0.0472, -0.0104,  0.0753,\n",
       "           0.0808,  0.0423,  0.0935, -0.0064,  0.0837, -0.0773,  0.0101,\n",
       "          -0.0714, -0.0291, -0.0814, -0.0366, -0.0779, -0.0495,  0.0291,\n",
       "          -0.0280,  0.0714,  0.0752,  0.0180,  0.0465,  0.0335, -0.0722,\n",
       "           0.0150,  0.0619,  0.0307,  0.0034,  0.0561,  0.0187,  0.0271,\n",
       "          -0.0323,  0.0664, -0.0152, -0.0737, -0.0533,  0.0948, -0.0279,\n",
       "           0.0841, -0.0771, -0.0133, -0.0010, -0.0785, -0.0203, -0.0467,\n",
       "          -0.0686, -0.0459, -0.0232,  0.0021, -0.0212, -0.0449,  0.0505,\n",
       "           0.0613,  0.0743,  0.0783,  0.0775, -0.0382, -0.0733,  0.1009,\n",
       "          -0.0076, -0.0496, -0.0031, -0.0898, -0.0619, -0.1022,  0.0456,\n",
       "           0.0135, -0.0087, -0.0373, -0.0503,  0.0823, -0.0543, -0.0139,\n",
       "           0.0545, -0.0394,  0.0560, -0.0867, -0.0793, -0.0812, -0.0037,\n",
       "           0.0484, -0.0628,  0.0750, -0.0448,  0.0391,  0.0081,  0.0551,\n",
       "           0.0688, -0.0878, -0.0114, -0.0498,  0.1122, -0.0011, -0.0618,\n",
       "          -0.1050,  0.0255],\n",
       "         [ 0.0879, -0.0125, -0.0993, -0.0559, -0.0781,  0.0629, -0.1151,\n",
       "           0.0963, -0.0578,  0.0669, -0.0743, -0.0575,  0.0796, -0.0329,\n",
       "           0.0361,  0.0029,  0.0844, -0.0075,  0.0025, -0.0656,  0.0315,\n",
       "           0.0531, -0.1008,  0.0413, -0.0413,  0.1071, -0.0628,  0.0665,\n",
       "           0.0137, -0.0705,  0.0912, -0.1083, -0.0762, -0.0074,  0.0102,\n",
       "           0.0743, -0.0036, -0.0656,  0.0907, -0.0986, -0.0673, -0.1171,\n",
       "          -0.0032,  0.0084,  0.0901,  0.0366,  0.1016, -0.0651,  0.1084,\n",
       "           0.0968,  0.0496,  0.0590,  0.0330, -0.0993,  0.1041, -0.0946,\n",
       "          -0.0334, -0.1000,  0.0987, -0.1093, -0.0975,  0.0288,  0.1149,\n",
       "           0.0385,  0.0912,  0.1025, -0.0187, -0.0233,  0.0968,  0.0447,\n",
       "          -0.0681, -0.0919, -0.0912, -0.1176,  0.0406, -0.0537,  0.0152,\n",
       "          -0.0404, -0.0670,  0.0120,  0.1099, -0.0886,  0.0465, -0.0357,\n",
       "           0.0493, -0.0356, -0.0946, -0.0035, -0.0693, -0.0916,  0.0533,\n",
       "          -0.0282,  0.0894,  0.0741, -0.0416, -0.0819, -0.0853,  0.1037,\n",
       "           0.0760,  0.0876]]), Parameter containing:\n",
       " tensor([-0.0701, -0.1133,  0.0234, -0.0693,  0.0491, -0.0870, -0.0691,\n",
       "          0.0995,  0.0553,  0.0634]), Parameter containing:\n",
       " tensor([[ 0.0501, -0.0236,  0.0079, -0.0393,  0.0301, -0.0857, -0.0439,\n",
       "           0.0655,  0.0504,  0.0143, -0.0859, -0.0589, -0.0735,  0.0141,\n",
       "           0.0282,  0.0603, -0.0843,  0.0451,  0.0004, -0.0642,  0.0439,\n",
       "           0.0167,  0.0817, -0.0177, -0.0364, -0.0391, -0.0786, -0.0115,\n",
       "          -0.0797,  0.0078,  0.0400,  0.0249, -0.0862, -0.0059, -0.0821,\n",
       "          -0.0300,  0.0564, -0.0801, -0.1025, -0.0262, -0.0315,  0.0048,\n",
       "          -0.0479, -0.0536,  0.0650, -0.1096, -0.0140, -0.0608, -0.1030,\n",
       "          -0.0675, -0.0826,  0.0207,  0.0719,  0.0639, -0.0389,  0.0216,\n",
       "          -0.0183, -0.0281, -0.1026, -0.0040,  0.0947, -0.0382, -0.0690,\n",
       "           0.0062,  0.0118, -0.0526, -0.1026, -0.0572, -0.0123, -0.0851,\n",
       "           0.0205,  0.0316, -0.0619,  0.0167,  0.0401, -0.0456,  0.0130,\n",
       "           0.0902, -0.0159, -0.1069, -0.1025, -0.0281,  0.0516, -0.0575,\n",
       "          -0.0624, -0.0118,  0.0300, -0.1158,  0.0094,  0.0642,  0.0335,\n",
       "           0.0814,  0.0303,  0.0032, -0.0435, -0.0176,  0.0368,  0.0594,\n",
       "          -0.0159,  0.0353],\n",
       "         [ 0.0455,  0.0068,  0.0225,  0.0470, -0.0220, -0.0958, -0.0522,\n",
       "           0.0756,  0.0264, -0.0631,  0.0802, -0.1114, -0.0342,  0.0023,\n",
       "           0.0841,  0.0444,  0.0371, -0.0164,  0.0926, -0.0914, -0.0135,\n",
       "           0.0522,  0.0154,  0.0661, -0.0414,  0.0579,  0.0137, -0.0366,\n",
       "           0.0727,  0.0075, -0.0432, -0.0619, -0.1090,  0.0638,  0.0426,\n",
       "           0.0151,  0.0496, -0.0776, -0.1040, -0.0731, -0.0064,  0.0759,\n",
       "          -0.0066, -0.0068, -0.0090,  0.0302, -0.0465,  0.0685, -0.0280,\n",
       "          -0.0519, -0.0447,  0.0832, -0.0250,  0.0817, -0.0354,  0.0960,\n",
       "           0.0436,  0.0375, -0.0007, -0.0201,  0.0463, -0.0091,  0.0547,\n",
       "           0.0908, -0.0654,  0.0723, -0.0624, -0.0782,  0.0074,  0.0856,\n",
       "           0.0125, -0.0572, -0.0978,  0.1048, -0.0643,  0.0044,  0.0783,\n",
       "          -0.0382, -0.0237,  0.0481,  0.0926, -0.0968, -0.0253, -0.0398,\n",
       "          -0.0279, -0.0848,  0.0797, -0.0537,  0.0211,  0.0352,  0.0734,\n",
       "           0.0347, -0.0938,  0.0455,  0.0830, -0.0537, -0.1147,  0.0353,\n",
       "           0.0439, -0.0390],\n",
       "         [ 0.0186, -0.1015, -0.1089, -0.0502,  0.0982, -0.0749,  0.0719,\n",
       "          -0.0157, -0.0317,  0.0458,  0.0934, -0.0963, -0.0863, -0.0790,\n",
       "          -0.0534, -0.0870,  0.0173,  0.0725,  0.0652, -0.0371,  0.0671,\n",
       "          -0.0059,  0.0664, -0.0893, -0.0004, -0.0671,  0.0502, -0.0604,\n",
       "           0.0464, -0.0555,  0.0283, -0.0017, -0.1288,  0.0297,  0.0066,\n",
       "          -0.1034,  0.0153, -0.0361,  0.0794,  0.0006, -0.0826,  0.0324,\n",
       "           0.0533, -0.0170, -0.0891, -0.0859, -0.1051,  0.0180, -0.1057,\n",
       "           0.0253, -0.0786, -0.0378,  0.0563,  0.0597,  0.0000,  0.0460,\n",
       "           0.0705, -0.0912, -0.1022,  0.0456,  0.0498,  0.0304,  0.0680,\n",
       "          -0.0351, -0.0383, -0.0961, -0.0613,  0.0429, -0.0147,  0.0560,\n",
       "          -0.0485, -0.0937,  0.0499, -0.0491, -0.0231, -0.0585, -0.0423,\n",
       "           0.0201,  0.0466, -0.0309, -0.1148,  0.0705,  0.0521, -0.0288,\n",
       "          -0.0036, -0.0322, -0.0718,  0.0058,  0.0591, -0.0246, -0.0295,\n",
       "           0.0759, -0.0183, -0.1009,  0.0058,  0.0809,  0.0143,  0.0062,\n",
       "          -0.0355, -0.0010],\n",
       "         [-0.0867, -0.0779, -0.0379, -0.0878,  0.0214,  0.0401,  0.0808,\n",
       "           0.0428,  0.0605,  0.0746, -0.1012, -0.0786, -0.0632, -0.0523,\n",
       "          -0.0857, -0.0643,  0.0677,  0.0476,  0.0031,  0.0934,  0.0709,\n",
       "           0.0694,  0.0492,  0.0521, -0.0146, -0.0911, -0.0362,  0.0358,\n",
       "          -0.0064,  0.0488, -0.0871, -0.0411,  0.0535,  0.0197, -0.0039,\n",
       "          -0.0544,  0.1131, -0.0271,  0.0056, -0.0010, -0.1002,  0.0511,\n",
       "           0.0352, -0.0486, -0.0116,  0.0715, -0.0425, -0.0934, -0.1078,\n",
       "          -0.0851, -0.0871, -0.0155,  0.0574, -0.1002,  0.0123, -0.1049,\n",
       "           0.0447,  0.0064,  0.0505, -0.0109, -0.0001,  0.0171,  0.0599,\n",
       "          -0.0249,  0.0501, -0.0234, -0.0588,  0.0306, -0.0690,  0.0013,\n",
       "          -0.0920,  0.0424, -0.0878,  0.0028, -0.0375, -0.0734,  0.0274,\n",
       "           0.0103, -0.1050, -0.0719, -0.0085, -0.0968,  0.0332, -0.0525,\n",
       "           0.0711,  0.0140, -0.0385, -0.0346, -0.0476,  0.0043, -0.1030,\n",
       "          -0.0011, -0.0998, -0.0160, -0.0378,  0.0271, -0.0724,  0.0077,\n",
       "          -0.0816,  0.0083],\n",
       "         [-0.0055,  0.0672, -0.0131, -0.0748,  0.0472, -0.0089, -0.0644,\n",
       "          -0.1116, -0.0945,  0.0395, -0.0299,  0.0242,  0.0788,  0.0410,\n",
       "          -0.0830, -0.0379,  0.0675, -0.0940, -0.0051, -0.0373, -0.1000,\n",
       "           0.0870,  0.0722, -0.0065,  0.0808, -0.0804,  0.0842, -0.0879,\n",
       "           0.0305, -0.0010, -0.0818,  0.0040, -0.0495, -0.0886,  0.0904,\n",
       "           0.0547,  0.0665, -0.0280,  0.0768, -0.0768, -0.0583, -0.0633,\n",
       "          -0.0623, -0.0524, -0.0837, -0.0075,  0.0669, -0.0126, -0.0365,\n",
       "          -0.0201,  0.0055, -0.0118,  0.0554, -0.0387,  0.0396, -0.0644,\n",
       "          -0.0937, -0.1117, -0.0822,  0.0643,  0.0247, -0.0741,  0.0166,\n",
       "          -0.0409,  0.0666, -0.0842,  0.0524,  0.0067,  0.0735, -0.0770,\n",
       "          -0.1092, -0.1020, -0.0846, -0.0457,  0.0402,  0.0089,  0.0767,\n",
       "          -0.0304,  0.0514,  0.0266, -0.0692,  0.0248, -0.0411, -0.0413,\n",
       "          -0.0882, -0.0948, -0.0902, -0.0349,  0.0338,  0.0582,  0.0587,\n",
       "          -0.0863, -0.0209,  0.0663,  0.0651, -0.0379, -0.0777, -0.0565,\n",
       "          -0.0157, -0.0794],\n",
       "         [ 0.0317, -0.0883, -0.0474, -0.0523,  0.0226,  0.0895, -0.0658,\n",
       "          -0.0903, -0.0740, -0.1015,  0.0619, -0.0101,  0.0743, -0.0768,\n",
       "          -0.0455, -0.0870, -0.0784,  0.0574, -0.0698,  0.0814, -0.0402,\n",
       "           0.0074, -0.1029,  0.0858, -0.0961,  0.0100, -0.0248, -0.0513,\n",
       "          -0.0476, -0.0781, -0.0557, -0.0685, -0.0102,  0.0670,  0.0446,\n",
       "          -0.0797, -0.0964, -0.0348,  0.0666, -0.0087, -0.0370,  0.0344,\n",
       "           0.0099,  0.0472, -0.1013, -0.0840,  0.0543, -0.0231, -0.0816,\n",
       "          -0.0336, -0.0209,  0.0745,  0.0014,  0.0660,  0.0373, -0.0655,\n",
       "          -0.0646, -0.0847, -0.0934,  0.0187,  0.0753, -0.0949,  0.0895,\n",
       "          -0.0939,  0.0383, -0.1015, -0.0962, -0.0067,  0.0783, -0.0049,\n",
       "          -0.1112,  0.0501,  0.0349, -0.0421, -0.0681, -0.0040, -0.0923,\n",
       "          -0.0724, -0.0226,  0.0473, -0.0474, -0.1088, -0.0071, -0.0370,\n",
       "          -0.0093,  0.0101, -0.0156, -0.0627,  0.0082, -0.1079, -0.0258,\n",
       "          -0.0677, -0.0089, -0.0314, -0.0105,  0.0859, -0.0058,  0.0735,\n",
       "          -0.0966, -0.0831]]), Parameter containing:\n",
       " tensor([-0.0167,  0.0531,  0.0162, -0.1142, -0.0515, -0.0188]), Parameter containing:\n",
       " tensor([[-0.0263, -0.0919,  0.0071, -0.0791, -0.0024,  0.0220,  0.0750,\n",
       "          -0.0792, -0.0238, -0.0233, -0.0412, -0.0377, -0.0150,  0.0443,\n",
       "          -0.0046, -0.0855, -0.1028, -0.0108,  0.0123,  0.0599,  0.0490,\n",
       "           0.0355, -0.0262,  0.0807,  0.0376, -0.0918,  0.0142,  0.0239,\n",
       "          -0.0568, -0.0009, -0.0439,  0.0147, -0.1090, -0.0906,  0.0057,\n",
       "           0.0934,  0.0623,  0.0580,  0.0869,  0.0130,  0.0176,  0.0422,\n",
       "           0.0195, -0.0745,  0.0872,  0.0595, -0.0261, -0.0156,  0.0484,\n",
       "          -0.0782, -0.0875, -0.0254,  0.0790,  0.0514,  0.0684, -0.0600,\n",
       "          -0.0273, -0.0671, -0.0207, -0.0512, -0.0424, -0.0149, -0.0850,\n",
       "           0.0454, -0.0624, -0.0168,  0.0230,  0.0316, -0.0653, -0.0157,\n",
       "          -0.0364, -0.0558, -0.0663, -0.0285, -0.0742, -0.1043, -0.0058,\n",
       "           0.0424, -0.0698, -0.0639,  0.0249, -0.0964, -0.0198, -0.0630,\n",
       "           0.0120,  0.0627, -0.0187, -0.1022, -0.0830, -0.0618,  0.0325,\n",
       "          -0.0440,  0.0132, -0.0413,  0.0248, -0.0294,  0.0289,  0.0686,\n",
       "          -0.0591,  0.0209],\n",
       "         [-0.0970, -0.0938,  0.0745, -0.0596, -0.0457, -0.0558, -0.0542,\n",
       "          -0.0717,  0.0086,  0.0577, -0.0051, -0.0930,  0.0740, -0.0409,\n",
       "           0.0185, -0.0746, -0.0602, -0.0340, -0.0750, -0.0517,  0.0730,\n",
       "          -0.0120, -0.0311,  0.0645,  0.0357, -0.0365,  0.0504, -0.0554,\n",
       "          -0.0575,  0.0615,  0.0687, -0.0025, -0.0807, -0.0251, -0.0943,\n",
       "           0.0629, -0.0184, -0.0742,  0.0919,  0.0856, -0.0661, -0.1012,\n",
       "          -0.0144, -0.0740, -0.0448, -0.0364,  0.0321,  0.0795, -0.0209,\n",
       "          -0.0660, -0.1432, -0.0231, -0.0581,  0.0331, -0.0463,  0.0622,\n",
       "          -0.0368, -0.0126,  0.0682,  0.0413, -0.0974,  0.0188,  0.0203,\n",
       "           0.0508, -0.0731, -0.0236,  0.0760, -0.1018, -0.0236,  0.0216,\n",
       "          -0.0741,  0.0773, -0.0609,  0.0722, -0.0699, -0.0610,  0.0200,\n",
       "          -0.0027, -0.0940,  0.0414,  0.0244,  0.0427, -0.0195,  0.0132,\n",
       "          -0.0024,  0.0156,  0.0343, -0.0338, -0.0038, -0.0016, -0.1075,\n",
       "           0.0674, -0.0299, -0.0566, -0.0604, -0.0728, -0.0262, -0.0443,\n",
       "          -0.0766,  0.0274],\n",
       "         [-0.0746,  0.0872,  0.0686, -0.0094, -0.0023,  0.0475,  0.0302,\n",
       "          -0.0047,  0.0437, -0.0829, -0.0306, -0.0752,  0.0579, -0.0131,\n",
       "          -0.0032, -0.0652, -0.0294, -0.0572,  0.0238,  0.0054, -0.0009,\n",
       "          -0.0661, -0.0473,  0.0253,  0.0722, -0.0981,  0.0801,  0.0558,\n",
       "           0.0484,  0.0919, -0.0359,  0.0396,  0.0189, -0.0143, -0.0718,\n",
       "          -0.0793, -0.0564, -0.0737,  0.0069,  0.0025, -0.0630,  0.0795,\n",
       "           0.0218, -0.0688, -0.0700,  0.0357,  0.0207, -0.0873,  0.0039,\n",
       "          -0.0801,  0.0603,  0.0053, -0.0514,  0.0311, -0.0611, -0.0070,\n",
       "           0.0132, -0.0010, -0.0097, -0.0703, -0.0260, -0.0598,  0.0617,\n",
       "           0.0690,  0.0726, -0.0393, -0.0230, -0.1027,  0.0215, -0.0715,\n",
       "           0.0812,  0.0746,  0.0233, -0.0243,  0.0339, -0.0288,  0.0918,\n",
       "           0.0504,  0.0473, -0.0770, -0.0899,  0.0841, -0.0456, -0.0980,\n",
       "           0.0724, -0.0756, -0.0405,  0.0693, -0.0834, -0.0810, -0.0375,\n",
       "           0.0018,  0.0673,  0.0342,  0.0432,  0.0326, -0.0626, -0.0639,\n",
       "          -0.0408, -0.0408],\n",
       "         [ 0.0855,  0.0393, -0.0002,  0.0556,  0.0172,  0.0580,  0.0462,\n",
       "           0.0603, -0.0274, -0.0358,  0.0715, -0.0599, -0.0795,  0.0488,\n",
       "           0.0486,  0.0677,  0.0680, -0.0054,  0.0696, -0.0524, -0.0212,\n",
       "           0.0756, -0.0603,  0.0839,  0.0518, -0.0565,  0.0585, -0.1113,\n",
       "          -0.0680, -0.1330,  0.0772,  0.0507, -0.0372, -0.0768,  0.0570,\n",
       "           0.0936,  0.0762, -0.0992,  0.0469, -0.0734, -0.0767,  0.0435,\n",
       "           0.0482, -0.0560,  0.0852, -0.0914, -0.1055,  0.0635, -0.0067,\n",
       "          -0.0755,  0.0604, -0.0385,  0.0106, -0.0315, -0.0550,  0.0215,\n",
       "          -0.0342, -0.0097, -0.0948,  0.0350, -0.0407, -0.0566,  0.0753,\n",
       "           0.0652,  0.0059,  0.0139,  0.0005,  0.0678,  0.0372,  0.0240,\n",
       "          -0.0175, -0.0273,  0.0540,  0.0418, -0.1131,  0.0689, -0.0367,\n",
       "           0.0676, -0.0692, -0.0518,  0.0060, -0.0048, -0.0741, -0.0091,\n",
       "           0.0225,  0.0155,  0.0703, -0.0572,  0.0420, -0.0171,  0.0786,\n",
       "          -0.0347,  0.0179, -0.0771, -0.0990, -0.0239, -0.0252, -0.1069,\n",
       "           0.0369,  0.0154],\n",
       "         [ 0.0297,  0.0584, -0.0816,  0.0612, -0.0523, -0.0256,  0.0511,\n",
       "           0.0058, -0.0999, -0.0272, -0.0755, -0.0535, -0.0613, -0.0633,\n",
       "           0.0500, -0.0919, -0.0336,  0.0644, -0.0308,  0.0064,  0.0217,\n",
       "           0.0639, -0.0310, -0.0719,  0.0621, -0.0292, -0.0825, -0.0566,\n",
       "          -0.0668, -0.1247, -0.0056,  0.0123,  0.0386,  0.0364, -0.0297,\n",
       "          -0.0352,  0.0597, -0.1030,  0.0819,  0.0496, -0.0279, -0.0539,\n",
       "           0.0144, -0.0838,  0.0699, -0.1180,  0.0899, -0.0612,  0.0809,\n",
       "          -0.0043,  0.0314,  0.0195,  0.0678, -0.0824, -0.0967,  0.0724,\n",
       "           0.0205, -0.1160, -0.1029, -0.0116,  0.0468, -0.0129, -0.0427,\n",
       "          -0.0679,  0.0305,  0.0506, -0.0779, -0.0955, -0.0940, -0.0275,\n",
       "          -0.0507, -0.0372, -0.0965, -0.0862,  0.0013,  0.0810,  0.0020,\n",
       "          -0.0896, -0.0184,  0.0048, -0.0258,  0.0456, -0.0326, -0.0197,\n",
       "           0.0307,  0.0767, -0.0110, -0.0861, -0.0778,  0.0307, -0.0987,\n",
       "          -0.0416, -0.0532, -0.0386, -0.0627,  0.0442, -0.0065,  0.0825,\n",
       "           0.0377,  0.0078],\n",
       "         [ 0.0240,  0.0186, -0.0091,  0.0531, -0.0263, -0.0026, -0.1042,\n",
       "          -0.1089,  0.0791, -0.0855,  0.0215, -0.0942,  0.0437, -0.0889,\n",
       "          -0.0845,  0.0136, -0.0428,  0.0479, -0.0479,  0.0063,  0.0083,\n",
       "           0.0560,  0.0472,  0.0510,  0.0415, -0.0958, -0.0679, -0.0272,\n",
       "           0.0039, -0.0154, -0.0406, -0.0115, -0.0338, -0.0132,  0.0741,\n",
       "           0.0344,  0.0633, -0.0825,  0.0803,  0.0823,  0.0806,  0.0139,\n",
       "          -0.0790, -0.0909, -0.1010,  0.0061,  0.0735, -0.0830,  0.0076,\n",
       "           0.0647, -0.0905,  0.0478, -0.0067, -0.0564, -0.0439, -0.0174,\n",
       "           0.0303, -0.0631, -0.0375,  0.0226,  0.0434,  0.0164,  0.0932,\n",
       "           0.0508,  0.0191,  0.0081,  0.0238, -0.0313,  0.0508, -0.0906,\n",
       "          -0.0778,  0.0388,  0.0634,  0.0732, -0.0273,  0.0394,  0.0321,\n",
       "           0.0418, -0.0975,  0.0271, -0.0912,  0.0414, -0.0467, -0.0782,\n",
       "           0.0895,  0.0032,  0.0326,  0.0321,  0.0939, -0.0597, -0.0212,\n",
       "           0.0275, -0.0591,  0.0048,  0.0569, -0.0339, -0.0757, -0.1080,\n",
       "          -0.0663, -0.0156]]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [ 4.9973, -6.3671,  0.8270, -4.4579,  4.3164, -7.6138]), Parameter containing:\n",
       " tensor([[ 0.0324,  0.0568,  0.0489, -0.0833, -0.0866,  0.0002, -0.0874,\n",
       "          -0.1223,  0.0443,  0.0123, -0.0268,  0.0628, -0.0155, -0.0448,\n",
       "           0.0069,  0.0009, -0.0087, -0.0058, -0.1049, -0.0100,  0.0456,\n",
       "          -0.0682,  0.0572, -0.0936,  0.0010, -0.0097,  0.0790, -0.0128,\n",
       "          -0.0291, -0.0773,  0.0309, -0.0479,  0.0386,  0.0684, -0.0069,\n",
       "           0.0370,  0.0534, -0.0516, -0.0136, -0.0011,  0.0606, -0.0439,\n",
       "           0.0326, -0.0264,  0.0506, -0.0946, -0.0838,  0.0316, -0.0875,\n",
       "           0.0624,  0.0307, -0.0943,  0.0384,  0.0752, -0.0658, -0.0876,\n",
       "           0.0660, -0.1214, -0.0732, -0.0459, -0.0236, -0.0301, -0.0345,\n",
       "          -0.0702,  0.0411,  0.0490,  0.0407, -0.0848,  0.0636,  0.0629,\n",
       "          -0.0133, -0.0477,  0.0729, -0.0690, -0.0610, -0.0959,  0.0872,\n",
       "          -0.0324,  0.0313,  0.0001, -0.0609,  0.0608, -0.1194, -0.1175,\n",
       "          -0.0064, -0.1146,  0.0363, -0.0104, -0.1046,  0.0347, -0.0220,\n",
       "          -0.1126, -0.1201, -0.0520,  0.0670,  0.0023, -0.1151,  0.0510,\n",
       "          -0.0967, -0.0923],\n",
       "         [ 0.0016,  0.0192,  0.0122, -0.0966, -0.0226,  0.0562,  0.0867,\n",
       "          -0.1067,  0.0757, -0.0337,  0.0236,  0.0674,  0.0142, -0.0823,\n",
       "           0.0002,  0.0368,  0.0977,  0.0331, -0.0267,  0.0785, -0.0053,\n",
       "           0.0782, -0.0394, -0.0238, -0.0134, -0.0823,  0.0740, -0.0350,\n",
       "          -0.0085,  0.0352,  0.0217,  0.0779,  0.0044, -0.0704, -0.0495,\n",
       "           0.0850, -0.0328, -0.0604, -0.0496, -0.0623,  0.0065,  0.0362,\n",
       "           0.0327,  0.0401,  0.0646, -0.0710, -0.0560,  0.0351, -0.0100,\n",
       "          -0.0952,  0.0471, -0.0242, -0.0738, -0.0102, -0.0703, -0.0450,\n",
       "          -0.0256,  0.0769, -0.0895, -0.0933,  0.0044, -0.0473, -0.0518,\n",
       "          -0.0531, -0.0993, -0.0494,  0.0569, -0.0987,  0.0501, -0.0391,\n",
       "           0.0773, -0.0595, -0.0653, -0.0728, -0.0444,  0.0621,  0.0872,\n",
       "          -0.0721, -0.0947, -0.0892, -0.0880, -0.0676, -0.0215,  0.0184,\n",
       "          -0.0798,  0.0150, -0.0313, -0.0734,  0.0917, -0.1023, -0.1100,\n",
       "          -0.0774,  0.0013,  0.0584, -0.0736, -0.0033, -0.0622,  0.0064,\n",
       "          -0.1129,  0.0303],\n",
       "         [-0.0062,  0.0488, -0.0094,  0.0169,  0.0150, -0.0368, -0.0312,\n",
       "           0.0094, -0.0355,  0.0149,  0.0854,  0.0379, -0.0391,  0.0406,\n",
       "          -0.0408,  0.0573,  0.0158,  0.0279, -0.0124,  0.0385,  0.0325,\n",
       "          -0.0531,  0.0752, -0.0055,  0.0427, -0.0807,  0.0408,  0.0566,\n",
       "          -0.0621, -0.0841, -0.0341,  0.0725,  0.0260,  0.0527, -0.0479,\n",
       "          -0.0854, -0.0775,  0.0163, -0.1041, -0.0387,  0.0015, -0.0569,\n",
       "           0.0106, -0.0637,  0.0770, -0.0503, -0.0584, -0.1145,  0.0645,\n",
       "          -0.0844, -0.0055,  0.0236,  0.0665, -0.1035,  0.0167, -0.0310,\n",
       "          -0.0567, -0.0959, -0.0333, -0.0012,  0.0296,  0.0096,  0.0652,\n",
       "          -0.0442, -0.0101, -0.0201,  0.0588,  0.0396, -0.0233,  0.0512,\n",
       "          -0.0922,  0.0630, -0.0750, -0.1237,  0.0331,  0.0733, -0.0142,\n",
       "           0.0717, -0.0588, -0.0714, -0.0107, -0.0513, -0.0973, -0.0956,\n",
       "          -0.0164, -0.0190, -0.1222,  0.0506, -0.1019, -0.0370,  0.0653,\n",
       "           0.0283, -0.1117,  0.0239, -0.0154,  0.0280, -0.0369, -0.0910,\n",
       "           0.0411,  0.0139],\n",
       "         [-0.0081, -0.0587,  0.0638,  0.0745,  0.0900,  0.0631, -0.0827,\n",
       "          -0.0531, -0.0886, -0.0524,  0.0883, -0.0041,  0.0840, -0.0702,\n",
       "          -0.0622, -0.0731, -0.0204, -0.0636,  0.0231, -0.0891,  0.0669,\n",
       "           0.0198, -0.1082, -0.0040, -0.0474, -0.1098,  0.0441, -0.0141,\n",
       "           0.0648, -0.0917, -0.0376, -0.0762,  0.0419, -0.1112,  0.0175,\n",
       "          -0.0372, -0.0791,  0.0666,  0.0518, -0.0818, -0.0313, -0.0806,\n",
       "           0.0611, -0.0991, -0.0738, -0.0751, -0.0426, -0.0444, -0.0450,\n",
       "           0.0060,  0.0965, -0.1005, -0.0085,  0.0555, -0.0967, -0.0966,\n",
       "           0.0257, -0.0466, -0.0324,  0.0532,  0.0615,  0.0054, -0.0470,\n",
       "           0.0838,  0.0108, -0.0583, -0.0336, -0.1045, -0.0969,  0.0136,\n",
       "          -0.1056, -0.1331,  0.0511, -0.0560, -0.0783,  0.0846,  0.0147,\n",
       "           0.0404,  0.0346, -0.0189, -0.0205,  0.0210, -0.0020, -0.0140,\n",
       "          -0.0876, -0.0702, -0.0145, -0.0353,  0.0476, -0.1017, -0.0887,\n",
       "          -0.0155, -0.1073,  0.0065,  0.0739, -0.0846,  0.0633,  0.0861,\n",
       "          -0.0212,  0.0379],\n",
       "         [-0.0420,  0.0294, -0.0832,  0.0375, -0.0152,  0.0757, -0.0885,\n",
       "          -0.0248,  0.0559, -0.0818, -0.0268, -0.0946, -0.0011, -0.0478,\n",
       "          -0.0830, -0.1100, -0.0989,  0.0454, -0.0039,  0.0872, -0.0522,\n",
       "          -0.0977,  0.0142, -0.0749,  0.0932, -0.0659, -0.0246,  0.0965,\n",
       "          -0.0478, -0.0300,  0.0117, -0.0866,  0.0118,  0.0829, -0.0121,\n",
       "          -0.0058, -0.0915,  0.0818, -0.0550,  0.0685,  0.0636, -0.0939,\n",
       "          -0.1105, -0.0468,  0.0167, -0.0275, -0.0268, -0.0830,  0.0330,\n",
       "          -0.0889,  0.0690, -0.0465, -0.0642, -0.0545,  0.0334,  0.0957,\n",
       "           0.0006,  0.0413,  0.0251, -0.0525, -0.0730, -0.0324,  0.0277,\n",
       "          -0.0043, -0.0833,  0.0849, -0.0644,  0.0040,  0.0529,  0.0359,\n",
       "          -0.0873, -0.0619, -0.0877, -0.0453, -0.0351, -0.0712,  0.0742,\n",
       "           0.0930,  0.1134, -0.0614,  0.1094, -0.0086,  0.0458,  0.0238,\n",
       "          -0.0331,  0.0419, -0.0436,  0.0203,  0.0324,  0.0414,  0.0456,\n",
       "          -0.0758,  0.0450,  0.0140,  0.0689,  0.0749, -0.0454,  0.0661,\n",
       "          -0.0558, -0.0925],\n",
       "         [-0.0646,  0.0519,  0.0589, -0.0230, -0.0576, -0.0779, -0.0019,\n",
       "          -0.0520, -0.0503,  0.0676,  0.0397, -0.0473,  0.0333, -0.0846,\n",
       "          -0.0488, -0.0753, -0.0869, -0.0759, -0.0394,  0.0086, -0.0122,\n",
       "          -0.0139,  0.0100,  0.0482, -0.0951, -0.0433,  0.0423,  0.0562,\n",
       "           0.0568, -0.0634,  0.0527,  0.0280, -0.0715, -0.1095, -0.0140,\n",
       "           0.0572,  0.0949,  0.0338, -0.0614,  0.0442, -0.1247, -0.0140,\n",
       "           0.0524,  0.0148, -0.0433,  0.0472, -0.0024,  0.0062,  0.0372,\n",
       "          -0.0577, -0.0152,  0.0854, -0.0563,  0.0039, -0.0703, -0.0932,\n",
       "          -0.0534, -0.0147,  0.0238, -0.0427,  0.0008,  0.0664,  0.0130,\n",
       "          -0.0728,  0.0671, -0.1161, -0.1136, -0.0029,  0.0331,  0.0751,\n",
       "           0.0681, -0.0744, -0.1011, -0.0903,  0.0498, -0.0607, -0.0603,\n",
       "          -0.1055, -0.0095, -0.1064,  0.0412,  0.0226, -0.0148,  0.0232,\n",
       "           0.0079, -0.0361,  0.0548, -0.0785,  0.0852,  0.0045, -0.0873,\n",
       "          -0.0460,  0.0415, -0.0577, -0.0918,  0.0382, -0.0633, -0.0307,\n",
       "           0.0206,  0.0785]]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [-4.3421, -1.4506, -4.1617, -0.4088,  2.0111, -4.4571])]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Agents[1].parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhYiI6KQoRERE52/Ox9kp8BBs4AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113d137f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_list = np.arange(len(r_list[0]))*ep_record\n",
    "plt.plot(epoch_list,r_list[0], label='Agent 1')\n",
    "plt.plot(epoch_list,r_list[1], label='Agent 2')\n",
    "plt.ylabel('Average reward in epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "#plt.savefig('N_ep='+str(N_ep)+'_seed='+str(num_seed)+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 0.1678]), tensor([ 0.9922]), tensor([ 0.9922]), tensor(1.00000e-02 *\n",
       "        [ 8.8948]), tensor([ 0.9766]), tensor(1.00000e-02 *\n",
       "        [ 8.1084]), tensor([ 0.9766]), tensor(1.00000e-02 *\n",
       "        [ 3.7251]), tensor([ 0.9766]), tensor([ 0.9453])]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p_prop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-227-647eb2d6637e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp_prop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'p_prop' is not defined"
     ]
    }
   ],
   "source": [
    "p_prop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
